{"docstore/data":{"80aae853-d665-47af-8c76-7fec51f5cadd":{"indexId":"80aae853-d665-47af-8c76-7fec51f5cadd","nodesDict":{"bb165f69-2985-461f-9436-17030a70c482":{"id_":"bb165f69-2985-461f-9436-17030a70c482","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"VZyeTgqj0pKtVrwFzpT6MthsC3rIBk3csJixLxZgyQQ="},"NEXT":{"nodeId":"bb0808cc-a0fb-45d4-8d9e-e2109f3c3fc2","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"5r1D1wWP+QNKkKZjt0xbeEjcdR7NYIs6jl7dY6tuerI="}},"text":"electronics\r\nArticle\r\nA Comparative Analysis of Object Detection Metrics with a\r\nCompanion Open-Source Toolkit\r\nRafael Padilla\u0003, Wesley L. Passos, Thadeu L. B. Dias, Sergio L. Nettoand Eduardo A. B. da Silva\r\n\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\r\n\u0001\u0001\u0002\u0003\u0004\u0005\u0006\u0007\r\nCitation:Padilla, R.; Passos, W.L.;\r\nDias, T.L.B.; Netto, S.L., da Silva,\r\nE.A.B. A Comparative Analysis of\r\nObject Detection Metrics with a\r\nCompanion Open-Source Toolkit. Electronics2021,10,  279. https://doi.org/10.3390/\r\nelectronics10030279\r\nAcademic Editor: Tomasz Trzcinski\r\nReceived: 25 December 2020\r\nAccepted: 20 January 2021\r\nPublished: 25 January 2021\r\nPublisher’s  Note:MDPI  stays  neu-\r\ntral with regard to jurisdictional clai-\r\nms in published maps and institutio-\r\nnal affiliations. Copyright:© 2021 by the authors. Li-\r\ncensee   MDPI,   Basel,    Switzerland. This article is an open access article\r\ndistributed under the terms and con-\r\nditions of the Creative Commons At-\r\ntribution  (CC  BY)   license  (https://\r\ncreativecommons.org/licenses/by/\r\n4.0/). Electrical Engineering Program/Alberto Luiz Coimbra Institute for Post-Graduation and Research in Engineering\r\n(PEE/COPPE), PO Box 68504, Rio de Janeiro 21941-972, RJ, Brazil; wesley.passos@smt.ufrj.br (W.L.P.);\r\nthadeu.dias@smt.ufrj.br (T.L.B.D.); sergioln@smt.ufrj.br (S.L.N.); eduardo@smt.ufrj.br (E.A.B.d.S. )\r\n*Correspondence: rafael.padilla@smt.ufrj.br\r\nAbstract:Recent outstanding results of supervised object detection in competitions and challenges\r\nare often associated with specific metrics and datasets. The evaluation of such methods applied in\r\ndifferent contexts have increased the demand for annotated datasets.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ADFqeGAPjGZfxS/p0HrOc/GnynWZv1x8VaItsddwwVs="},"bb0808cc-a0fb-45d4-8d9e-e2109f3c3fc2":{"id_":"bb0808cc-a0fb-45d4-8d9e-e2109f3c3fc2","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"VZyeTgqj0pKtVrwFzpT6MthsC3rIBk3csJixLxZgyQQ="},"PREVIOUS":{"nodeId":"bb165f69-2985-461f-9436-17030a70c482","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ADFqeGAPjGZfxS/p0HrOc/GnynWZv1x8VaItsddwwVs="},"NEXT":{"nodeId":"46ca9052-0dba-4c51-8bf8-c37e0be14838","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"s9qcjGY048bvl358ifZjVfF4jD4w8SOmE4F9p+jUuTo="}},"text":"The evaluation of such methods applied in\r\ndifferent contexts have increased the demand for annotated datasets. Annotation tools represent the\r\nlocation and size of objects in distinct formats, leading to a lack of consensus on the representation. Such a scenario often complicates the comparison of object detection methods. This work alleviates\r\nthis problem along the following lines: (i) It provides an overview of the most relevant evaluation\r\nmethods used in object detection competitions, highlighting their peculiarities, differences, and ad-\r\nvantages; (ii) it examines the most used annotation formats, showing how different implementations\r\nmay influence the assessment results; and (iii) it provides a novel open-source toolkit supporting dif-\r\nferent annotation formats and 15 performance metrics, making it easy for researchers to evaluate the\r\nperformance of their detection algorithms in most known datasets. In addition, this work proposes a\r\nnew metric, also included in the toolkit, for evaluating object detection in videos that is based on the\r\nspatio-temporal overlap between the ground-truth and detected bounding boxes. Keywords:object-detection metrics;  precision;  recall;  evaluation;  automatic assessment;  bound-\r\ning boxes\r\n1. Introduction\r\nThe human visual system can effectively distinguish objects in different environments\r\nand  contexts,  even  under  a  variety  of  constraints  such  as  low  illumination  [1],  color\r\ndifferences [2], and occlusions [3,4]. In addition, objects are key to the understanding of\r\na scene’s context, which lends paramount importance to the estimation of their precise\r\nlocation and classification. This has led computer vision researchers to explore automatic\r\nobject detection for decades [5], reaching impressive results particularly in the last few\r\nyears [6–9]. Object detection algorithms attempt to locate general occurrences of one or more\r\npredefined classes of objects. In a system designed to detect pedestrians, for instance, an\r\nalgorithm tries to locate all pedestrians that appear within an image or a video [3,10,11]. In the identification task, however, an algorithm tries to recognize a specific instance of\r\na given class of objects. In the pedestrian example, an identification algorithm wants to\r\ndetermine the identity of each pedestrian previously detected. Initially, real-time object detection applications were limited to only one object type [12]\r\nat a time, mostly due to hardware limitations.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5r1D1wWP+QNKkKZjt0xbeEjcdR7NYIs6jl7dY6tuerI="},"46ca9052-0dba-4c51-8bf8-c37e0be14838":{"id_":"46ca9052-0dba-4c51-8bf8-c37e0be14838","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"VZyeTgqj0pKtVrwFzpT6MthsC3rIBk3csJixLxZgyQQ="},"PREVIOUS":{"nodeId":"bb0808cc-a0fb-45d4-8d9e-e2109f3c3fc2","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"5r1D1wWP+QNKkKZjt0xbeEjcdR7NYIs6jl7dY6tuerI="}},"text":"Later on, advancements in object detec-\r\ntion techniques led to their increasing adoption in areas that included the manufacturing\r\nindustry with optical inspections [13], video surveillance [14], forensics [15,16], medical\r\nimage analysis [17–19], autonomous vehicles [20], and traffic monitoring [21]. In the last\r\ndecade, the use of deep neural networks (DNNs) has completely changed the landscape\r\nof the computer vision field [22]. DNNs have allowed for drastic improvements in image\r\nElectronics2021,10, 279. https://doi.org/10.3390/electronics10030279                                                           https://www.mdpi.com/journal/electronics","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"s9qcjGY048bvl358ifZjVfF4jD4w8SOmE4F9p+jUuTo="},"2f7be790-8b16-4549-a9d6-56f04c18978c":{"id_":"2f7be790-8b16-4549-a9d6-56f04c18978c","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"go1S/5+bWJliU1m1hUPJx+ybbrEz+QgeIaXZo00nTgo="},"NEXT":{"nodeId":"ed09c550-bea8-47ef-ad33-96114996ebb5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"FOKI5gBcINkIbEiZgtBRER+Z7EuUmG2VFD+ExtI3J0M="}},"text":"Electronics2021,10, 2792 of 28\r\nclassification, image segmentation, anomaly detection, optical character recognition (OCR),\r\naction recognition, image generation, and object detection [5]. The field of object detection has yielded significant improvements in both efficiency\r\nand accuracy. To validate such improvements, new techniques must be assessed against\r\ncurrent state-of-the-art approaches, preferably over widely available datasets. However,\r\nbenchmark datasets and evaluation metrics differ from work to work, often making their\r\ncomparative assessment confusing and misleading. We identified two main reasons for\r\nsuch confusion in comparative assessments:\r\n•There are often differences in bounding box representation formats among differ-\r\nent detectors. Boxes could be represented, for instance by their upper-left corner\r\ncoordinates (x, y) and their absolute dimensions (width, height) in pixels, or by their\r\nrelative coordinates (xrel,yrel) and dimensions(widthrel,heightrel), with the values\r\nnormalized by the image size, among others;\r\n•Each performance assessment tool implements a set of different metrics, requiring\r\nspecific formats for the ground-truth and detected bounding boxes. Even though many tools have been developed to convert the annotated boxes from\r\none format to another,  the quality assessment of the final detections still lacks a tool\r\ncompatible with different bounding box formats and multiple metrics. Our previous\r\nwork [23] contributed to the research community in this direction, by presenting a tool\r\nwhich reads ground-truth and detected bounding boxes in a closed format and evaluates\r\nthe detections using the average precision (AP) and mean average precision (mAP) metrics,\r\nas required in the PASCAL Challenge [24]. In this work that contribution is significantly\r\nexpanded by incorporating 13 other metrics, as well as by supporting additional annotation\r\nformats into the developed open-source toolbox. The new evaluation tool is available at\r\nhttps://github.com/rafaelpadilla/review_object_detection_metrics. We believe that our\r\nwork significantly simplifies the task of evaluating object detection algorithms. This work intends to explain in detail the computation of the most popular metrics\r\nused as benchmarks by the research community, particularly in online challenges and com-\r\npetitions, providing their mathematical foundations and a practical example to illustrate\r\ntheir applicability. In order to do so, after a brief contextualization of the object-detection\r\nfield in Section 2, the most common annotation formats and assessment metrics are ex-\r\namined in Sections 3 and 4, respectively.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0o81+5HhUiMoV8i3W8C0khbhKqesXUPiYKczaXQV3vk="},"ed09c550-bea8-47ef-ad33-96114996ebb5":{"id_":"ed09c550-bea8-47ef-ad33-96114996ebb5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"go1S/5+bWJliU1m1hUPJx+ybbrEz+QgeIaXZo00nTgo="},"PREVIOUS":{"nodeId":"2f7be790-8b16-4549-a9d6-56f04c18978c","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"0o81+5HhUiMoV8i3W8C0khbhKqesXUPiYKczaXQV3vk="}},"text":"A numerical example is provided in Section 5\r\nillustrating the previous concepts from a practical perspective. Popular metrics are further\r\naddressed in Section 6. In Section 7 object detection in videos is discussed from an inte-\r\ngrated spatio-temporal point of view, and a new metric for videos is provided. Section 8\r\npresents an open-source and freely distributed toolkit that implements all discussed con-\r\ncepts in a unified and validated way, as verified in Section 9. Finally, Section 10 concludes\r\nthe paper by summarizing its main technical contributions. 2. An Overview of Selected Works on Object Detection\r\nBack in the mid-50s and 60s the first attempts to recognize simple patterns in images\r\nwere published [25,26]. These works identified primitive shapes and convex polygons\r\nbased on contours. In the mid-80s, more complex shapes started gaining meaning, such as\r\nin [27], which described an automated process to construct a three-dimensional geometric\r\ndescription of an airplane. To describe more complex objects, instead of characterizing them by their shapes,\r\nautomated feature extraction methods were developed. Different methods attempted to\r\nfind important feature points that when combined could describe objects broadly. Robust\r\nfeature points are represented by distinctive pixels, whose neighborhood describe the same\r\nobject irrespective of changes in pose, rotation, and illumination. The Harris detector [28]\r\nfinds such points in the object corners based on local intensity changes. A local search\r\nalgorithm using gradients was devised in [29] to solve the image registration problem,\r\nwhich later was expanded to a tracking algorithm [30] for identifying important points\r\nin videos.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FOKI5gBcINkIbEiZgtBRER+Z7EuUmG2VFD+ExtI3J0M="},"75204d22-c0f2-4cd7-bec6-1a66930353ab":{"id_":"75204d22-c0f2-4cd7-bec6-1a66930353ab","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"N0bFxJGurRq/cDJ0h0BrAhUa/ZD5n0/IDqhPkfif3cw="},"NEXT":{"nodeId":"eac4d057-bc64-4a84-82d8-1665e69f7fe7","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"3lQbfU3Z/8meuEtK0Zge9Snu25TBopdqPRzdlxsbJ7E="}},"text":"Electronics2021,10, 2793 of 28\r\nMore robust methods were able to identify characteristic pixel points and represent\r\nthem as feature vectors. The so-called scale invariant feature transform (SIFT) [31], for\r\ninstance, applied the difference of Gaussians in several scales coupled with histograms\r\nof gradients, yielding characteristic points with features that are robust to scale changes\r\nand rotation. Another popular feature detector and descriptor, the speed up robust fea-\r\ntures (SURF) [32], was claimed to be faster and more robust than SIFT, and uses a blob\r\ndetector based on the Hessian matrix for interest point detection and wavelet responses for\r\nfeature representations. Feature-point representation methods alone are not able to perform object detection,\r\nbut can help in extracting a group of keypoints that are used to represent them. In [33],\r\nthe SIFT keypoints and features are used to detect humans in images, and in [34] SIFT\r\nwas combined with color histograms to classify regions of interest across frames to track\r\nobjects in videos. Another powerful feature extractor widely applied for object detection\r\nis the histogram of oriented gradients (HOG) [35], which is computed for several image\r\nsmall cells. The histograms of each cell are combined to form the object descriptor, which,\r\nassociated to a classifier, can perform the object detection task [35,36]. The Viola–Jones object detection framework was described in the path-breaking work\r\nof [12]. It could detect a single class object at a rate of 15 frames per second. The proposed\r\nalgorithm employed a cascade of weak classifiers to process image patches of different sizes,\r\nbeing able to associate bounding boxes to the target object. The Viola–Jones method was\r\nfirst applied to face detection and required extensive training to automatically select a group\r\nof Haar-features to represent the target object, thus detecting one class of objects at a time. This framework has been extended to detect other object classes such as pedestrians [10,11]\r\nand cars [37]. More recently, with the growth and popularization of deep learning in computer\r\nvision problems [6,7,38–40], object detection algorithms have started to develop from a\r\nnew perspective [41,42]. The traditional feature extraction [31,32,35] phase is performed by\r\nconvolutional neural networks (CNNs), which are dominating computer vision research\r\nin many fields.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/u8f9xg+KO1j4PhvWBtsykvHV3mFyFUYPGHw9cBpV70="},"eac4d057-bc64-4a84-82d8-1665e69f7fe7":{"id_":"eac4d057-bc64-4a84-82d8-1665e69f7fe7","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"N0bFxJGurRq/cDJ0h0BrAhUa/ZD5n0/IDqhPkfif3cw="},"PREVIOUS":{"nodeId":"75204d22-c0f2-4cd7-bec6-1a66930353ab","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"/u8f9xg+KO1j4PhvWBtsykvHV3mFyFUYPGHw9cBpV70="}},"text":"Due to their spatial invariance, convolutions perform feature extraction\r\nspatially and can be combined into layers to produce the desired feature maps. The network\r\nend is usually composed of fully connected (FC) layers that can perform classification\r\nand regression tasks. The output is then compared to a desired result and the network\r\nparameters are adjusted to minimize a given loss function. The advantage of using DNNs\r\nin object detection tasks is the fact that their architectures can extract features and predict\r\nbounding boxes in the same pipeline, allowing efficient end-to-end training. The more\r\nlayers a network has, the more complex features it is able to extract, but the more parameters\r\nit needs to learn, demanding more computer processing power and data. When it is not feasible to acquire more real data, data augmentation techniques are\r\nused to generate artificial but realistic data. Color and geometric operations and changes\r\ninside the target object area are the main actions performed by data augmentation methods\r\nfor object detection tasks [43]. The work in [44] applied generative adversarial networks\r\n(GANs) to increase by 10 times the amount of medical chest images to detect patients with\r\nCOVID-19. In [45], the number of images was increased by applying filters in astronomy\r\nimages so as to improve the performance of galaxy detectors. The CNN-based object detectors may be cataloged as single-shot or region-based\r\ndetectors, also known as one- or two-stage detectors, respectively. The single-shot detectors\r\nwork by splitting the images into a grid of cells. For each cell, they make bounding-box\r\nguesses of different scales and aspect ratios. This type of detector prioritizes speed rather\r\nthan accuracy, aiming to predict both bounding box and class simultaneously. Overfeat [46]\r\nwas one of the first single-shot detectors, followed by the single shot multiBox detector\r\n(SSD) [47], and all versions of you only look once (YOLO)[9,48–51]. The region-based\r\ndetectors perform the detection in two steps. First, they generate a sparse set of region\r\nproposals in the image where the objects are supposed to be. The second stage classifies\r\neach object proposal and refines its estimated position. The region-based convolutional","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"3lQbfU3Z/8meuEtK0Zge9Snu25TBopdqPRzdlxsbJ7E="},"497e054e-5f2c-48f1-bb05-77df921f4b3d":{"id_":"497e054e-5f2c-48f1-bb05-77df921f4b3d","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"+5pPeviVIVzQ4KmxQ8gU+wNFtTaV5r++ByrAG2MXHAg="},"NEXT":{"nodeId":"4f1d5c98-66eb-4465-bb38-fc89d6f9335d","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"FQevLnUR52cbpK1W9/j/K7xm1KQah5r9F32pp7MPfHw="}},"text":"Electronics2021,10, 2794 of 28\r\nneural network (R-CNN) [52] was a pioneer employing CNNs in this last stage, achieving\r\nsignificant gains in accuracy. Later works such as Fast R-CNN [53], Faster R-CNN [8], and\r\nregion-based fully convolutional networks (R-FCN) [54] suggest changes in R-CNN to\r\nimprove its speed. The aforementioned detectors have some heuristic and hand-crafted\r\nsteps such as region feature extraction or non-maximum suppression to remove duplicate\r\ndetections. In this context, graph neural networks (GNNs) are employed to compute\r\nregion of interest features in a more efficient way and process the objects simultaneously\r\nby modeling them according to their appearance feature and geometry [55,56]. Hybrid solutions combining different approaches have been proposed lately and\r\nhave proved to be more robust in various object-detection applications. The work in [57]\r\nproposes a hybrid solution involving a genetic algorithm and CNNs to classify small\r\nobjects (structures) presented in microscopy images. Feature descriptors coupled with a\r\ncuckoo search algorithm were applied by the authors of [58] to detect vessels in a marine\r\nenvironment using synthetic aperture radar (SAR) images. This approach was compared\r\nto genetic algorithms and neural network models individually, improving precision to\r\nnearly 96.2%. Vision-based autonomous vehicles can also benefit from hybrid models as\r\nshown in [59], where a system integrating different approaches was developed to detect\r\nand identify pedestrians and to predict their movements. In the context of detecting objects\r\nusing depth information, the work in [60] proposes a hybrid attention neural network that\r\nincorporates depth and high-level RGB features to produce an attention map to remove\r\nbackground information. Other works aim to detect the most important region of interest and segment relevant\r\nobjects using salient object detectors. The work in [61] proposes a pipeline to separate\r\nan input image into a pair of images using content-preserving transforms. Then, each\r\nresulting image is passed by an interweaved convolutional neural network, which extracts\r\ncomplementary information of the image pairs and fuses them into the final salient map. As medical images are acquired with special equipment, they form a very specific\r\ntype of image [17]. To detect lesions, organs, and other structures of interest can be crucial\r\nfor a precise diagnostic.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"A+EvN7eYKq0rUmoBhWib0pjLwcS0531llxYz+5fg7BQ="},"4f1d5c98-66eb-4465-bb38-fc89d6f9335d":{"id_":"4f1d5c98-66eb-4465-bb38-fc89d6f9335d","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"+5pPeviVIVzQ4KmxQ8gU+wNFtTaV5r++ByrAG2MXHAg="},"PREVIOUS":{"nodeId":"497e054e-5f2c-48f1-bb05-77df921f4b3d","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"A+EvN7eYKq0rUmoBhWib0pjLwcS0531llxYz+5fg7BQ="}},"text":"To detect lesions, organs, and other structures of interest can be crucial\r\nfor a precise diagnostic. However, most object detection systems are designed for general\r\napplications and usually do not perform well in medical images without adaptations [62]. Detecting anomalies such as glaucoma, breast, and lung lesions, for instance, have been\r\nexplored from the medical object-detection perspective in [63,64]. In the medical field,\r\ntraining and testing data usually have significant differences due to data scarcity and\r\nprivacy. In order to address this issue, a domain adaptation framework, referred to as\r\nclustering CNNs (CLU-CNNs) [65], has been proposed to improve the generalization\r\ncapability without specific domain training. With new object detection methods being constantly released, it is highly desirable that\r\na consensual evaluation procedure is established. To do so, the most common bounding\r\nbox formats used by public datasets and competitions are revised in the next section. 3. Bounding Box Formats\r\nGiven the large, ever growing number of object detectors based on supervised meth-\r\nods in different areas, specific datasets have been built to train these systems. Popular\r\ncompetitions such as the common objects in context (COCO) [66], PASCAL visual object\r\nclasses (VOC) [67], and Open Images Dataset [68] offer annotated datasets so that partici-\r\npants can train and evaluate their models before submission. Apart from using available\r\ndatasets, collecting data for a object detection task can be quite challenging, as labeling\r\nimages and videos is often a manual and quite demanding exercise. The medical field pro-\r\nvides a good example of this fact: Developing a database of X-ray, electroencephalography\r\n(EEG), magnetoencephalography (MEG), or electrocardiography (ECG) images involves\r\nnot only high costs for capturing such signals,  but also requires expert knowledge to\r\ninterpret and annotate them.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FQevLnUR52cbpK1W9/j/K7xm1KQah5r9F32pp7MPfHw="},"85aa0046-bf45-4076-8174-1f6cf01ba3ff":{"id_":"85aa0046-bf45-4076-8174-1f6cf01ba3ff","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Iin83v63Q0JZB6dSKvn9hlxFwoC/Js5jRGM5NAqaOZ0="},"NEXT":{"nodeId":"d69bf3e3-5708-4782-adc6-a7175c6f35ce","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ZsqkmsjSXCd0vN8wc5ldt5Y2zGAM+NgjSM3BvzxiJqw="}},"text":"Electronics2021,10, 2795 of 28\r\nTo ease the object annotation process in images and videos, many tools have been\r\ndeveloped to annotate different datasets. Such tools basically offer the same features, such\r\nas bounding-box annotations and polygon-like silhouettes, as shown in Figure 1. (a)\r\n(b)\r\nFigure 1.Two different types of annotated images from OpenImage [68]:  (a) Bounding box annotations; (b) Silhouette\r\nannotations (in yellowish-green), also referred to as segmentation and pixel-level annotations. A vast amount of annotation tools are freely available. Table 1 lists the most popular\r\nones with their respective bounding box output formats. Table 1.Popular free annotation tools and their supported output formats. Annotation Tool                                               Annotation Types                                               Output Formats\r\nLabelMe [69]                                       Bounding boxes and polygonsLabelMe, but provides conversion toCOCO and PASCAL VOC\r\nLabelIMG [70]                                                   Bounding boxes                                         PASCAL VOC and YOLO\r\nMicrosoft VoTT [71]                                 Bounding boxes and polygons\r\nPASCAL VOC, TFRecords, specific CSV,\r\nAzure Custom Vision Service, Microsoft\r\nCognitive Toolkit (CNTK), VoTT\r\nComputer Vision Annotation Tool\r\n(CVAT) [72]Bounding boxes and polygons\r\nCOCO, CVAT, LabelMe, PASCAL VOC,\r\nTFRecord, YOLO, and others\r\nVGG Image Annotation Tool (VIA) [73]                Bounding boxes and polygons                    COCO and specific CSV and JSON\r\nSome datasets introduced new formats to represent their annotations,  which are\r\nusually named after the datasets themselves. The PASCAL VOC dataset [67] established\r\nthe PASCAL VOC XML format and the COCO dataset [66] represents their annotations in\r\nthe COCO format, embodied in a JSON file. Annotation tools also brought further formats. For example, CVAT [72], a popular annotation tool, outputs bounding boxes in multiple\r\nformats, including its own specific XML-based one, named a CVAT format. The most\r\npopular bounding box formats shown in Table 1 are described in more detail. Note that\r\nwhenever we refer to absolute coordinates, we mean coordinates that are expressed on the\r\nimage coordinate frame, as opposed to coordinates that are normalized by the image width\r\nor image height.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oDZeM2FnAWnJh+oCSlwPEHD0a/AR4rMCrXhlhVb9B1E="},"d69bf3e3-5708-4782-adc6-a7175c6f35ce":{"id_":"d69bf3e3-5708-4782-adc6-a7175c6f35ce","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Iin83v63Q0JZB6dSKvn9hlxFwoC/Js5jRGM5NAqaOZ0="},"PREVIOUS":{"nodeId":"85aa0046-bf45-4076-8174-1f6cf01ba3ff","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"oDZeM2FnAWnJh+oCSlwPEHD0a/AR4rMCrXhlhVb9B1E="}},"text":"1.PASCAL VOC: It consists of one XML file for each image containing none, one or\r\nmultiple bounding boxes. The upper-left and bottom-right pixel coordinates are\r\nabsolute. Each bounding box also contains a tag representing the class of the object. Extra information about the labeled object can be provided, such as whether, the\r\nobject extends beyond the bounding box or it is partially occluded. The annotations\r\nin the ImageNet [74] and PASCAL VOC [67] datasets are provided using the PASCAL\r\nVOC format;","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZsqkmsjSXCd0vN8wc5ldt5Y2zGAM+NgjSM3BvzxiJqw="},"48f12d8e-80c5-44d9-8075-823bbdf24fc6":{"id_":"48f12d8e-80c5-44d9-8075-823bbdf24fc6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"8DstPTqoVanwWuhG1I2p42sGy6NlY0hV2pRvI3P47H0="},"NEXT":{"nodeId":"87cd7020-13d7-490b-ba71-34038ed9804a","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"svsYqj/wyDM9u+YwezGkIuhkVO6BDfk4wt8GfkfnaDw="}},"text":"Electronics2021,10, 2796 of 28\r\n2.COCO: It is represented by a single JSON file containing all bounding boxes of a\r\ngiven dataset. The classes of the objects are listed separately in thecategoriestag and\r\nidentified by anid. The image file corresponding to an annotation is also indicated in\r\na separate element (images) that contains its file name and is referenced by anid. The\r\nbounding boxes and their object classes are listed in a different element (annotations),\r\nwith their top-left(x,y)coordinates being absolute, and with explicit values of width\r\nand height;\r\n3.LabelMe: The bounding-box annotations in this format are inserted in a single JSON\r\nfile for each image, containing a list of boxes represented by their absolute upper-left\r\nand bottom-right coordinates. Besides the class of the object, this format also contains\r\nthe image data encoded in base64 type, thus making the LabelMe format to consume\r\nmore storage space than others;\r\n4.YOLO: One TXT file per image is used in this representation. Each line of the file\r\ncontains the class id and the bounding box coordinates. An extra file is needed to\r\nmap the class id to the class name. The bounding box coordinates are not absolute,\r\nbeing represented by the format\u0000xcenterimage width,ycenterimage height,box widthimage width,heightimage height\u0001. The\r\nadvantage of representing the boxes in this format is that, if the image dimensions are\r\nscaled, the bounding box coordinates do not change, and thus the annotation file does\r\nnot have to be altered. This type of format is the one preferred by those who annotate\r\nimages in one resolution and need to scale their dimensions to fulfill the input shape\r\nrequirement of a specific CNN. The YOLO object detector needs bounding boxes in\r\nthis format to execute training;\r\n5.VoTT: This representation of the bounding boxes coordinates and object class is made\r\nin a JSON file (one file per image) and the coordinates are expressed as the width,\r\nheight and upper-left(x,y)pixel position in absolute coordinates. The Visual Object\r\nTagging Tool (VoTT) produces annotations in this format;\r\n6.CVAT: It consists of a unique XML file with all bounding boxes in the dataset repre-\r\nsented by the upper-left and bottom-right pixel absolute coordinates.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"opTpXKBcH4hTE7TRhV9M4iEAr4UeCkP+wzcDJO+PnOg="},"87cd7020-13d7-490b-ba71-34038ed9804a":{"id_":"87cd7020-13d7-490b-ba71-34038ed9804a","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"8DstPTqoVanwWuhG1I2p42sGy6NlY0hV2pRvI3P47H0="},"PREVIOUS":{"nodeId":"48f12d8e-80c5-44d9-8075-823bbdf24fc6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"opTpXKBcH4hTE7TRhV9M4iEAr4UeCkP+wzcDJO+PnOg="}},"text":"This format has\r\nbeen created with the CVAT annotation tool;\r\n7.TFRecord:This is a serialized representation of the whole dataset containing all images\r\nand annotations in a single file. This format is recognized by theTensorflow library [75];\r\n8.Tensorflow Object Detection: This is a CSV file containing all labeled bounding boxes\r\nof the dataset. The bounding box format is represented by the upper-left and bottom-\r\nright pixel absolute coordinates. This is also a widely used format employed by the\r\nTensorflow library;\r\n9.Open Images Dataset: This format is associated with the Open Images Dataset [68] to\r\nannotate its ground-truth bounding boxes. All annotations are written in a unique\r\nCSV file listing the name of the images and labels, as well as upper-left and bottom-\r\nright absolute coordinates of the bounding boxes. Extra information about the labeled\r\nobject is conveyed by other tags such as,  for example,IsOcclude,IsGroupOf,  and\r\nIsTruncated. As each dataset is annotated using a specific format, works tend to employ the evalua-\r\ntion tools provided along with the datasets to assess their performance. Therefore, their\r\nresults are dependent on the specific metric implementation associated with the used\r\ndataset. For example, the PASCAL VOC dataset employs the PASCAL VOC annotation\r\nformat, which provides a MATLAB code implementing the metrics AP and mAP (inter-\r\nsection over union (IOU)=.50). This tends to inhibit the use of other metrics to report\r\nresults obtained for this particular dataset. Table 2 lists popular object detection methods\r\nalong with the datasets and the 14 different metrics used to report their results, namely:\r\nAP@[.5:.05:.95], AP@.50, AP@.75, APS, APM, APL, AR1, AR10, AR100, ARS, ARM, ARL, mAP\r\n(IOU=.50), and AP. As the evaluation metrics are directly associated with a given annotation format,\r\nalmost all works report their results only for the metrics implemented for the benchmarking\r\ndataset. For example, mAP (IOU=.50) is reported when the PASCAL VOC dataset is used,","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"svsYqj/wyDM9u+YwezGkIuhkVO6BDfk4wt8GfkfnaDw="},"c5b3dac3-e14f-483b-bb17-9415a01c9cfa":{"id_":"c5b3dac3-e14f-483b-bb17-9415a01c9cfa","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"aGhgWdjk/qhnYd8XpGXpun6d/RFmV7bSoD0g1WX6nH8="},"NEXT":{"nodeId":"986277e7-ff07-42bd-982f-765581bbb55c","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Keb0UFwPsG81GSOl4ZznPDcOyYIM/WyaRki2bu6IAgI="}},"text":"Electronics2021,10, 2797 of 28\r\nwhile AP@[.5:.05:.95] is applied to report results on the COCO dataset. If a work uses the\r\nCOCO dataset to train a model and wants to evaluate their results with the PASCAL VOC\r\ntool, it will be necessary to convert the ground-truth COCO JSON format to the PASCAL\r\nVOC XML format. This scenario discourages the use of such cross-dataset assessments,\r\nwhich have become quite rare in the object detection literature. Table 2.Popular object detection methods along with the datasets and metrics used to report their results.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Wt9qEXOsKxKWO1Jof9R4mIDTrMQ6Dt3Kdl+uXg6XZCQ="},"986277e7-ff07-42bd-982f-765581bbb55c":{"id_":"986277e7-ff07-42bd-982f-765581bbb55c","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"aGhgWdjk/qhnYd8XpGXpun6d/RFmV7bSoD0g1WX6nH8="},"PREVIOUS":{"nodeId":"c5b3dac3-e14f-483b-bb17-9415a01c9cfa","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Wt9qEXOsKxKWO1Jof9R4mIDTrMQ6Dt3Kdl+uXg6XZCQ="},"NEXT":{"nodeId":"2b407770-11d4-414b-a1ca-f97d63c9b026","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"b4hKNO+uwwzYPUQsMJDh3Ja70K7luGgAsyxWBqYy/HE="}},"text":"Table 2.Popular object detection methods along with the datasets and metrics used to report their results. Method                             Benchmark Dataset                                                                          Metrics\r\nCornerNet [76]                                  COCO                                                 AP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL\r\nEfficientDet [77]                                 COCO                                                                AP@[.5:.05:.95]; AP@.50; AP@.75\r\nFast R-CNN [53]            PASCAL VOC 2007, 2010, 2012                                                      AP; mAP (IOU=.50)\r\nFaster R-CNN [8]PASCAL VOC 2007, 2012                                                           AP; mAP (IOU=.50)\r\nFaster R-CNN [8]COCO                                                                        AP@[.5:.05:.95]; AP@.50\r\nR-CNN [52]                PASCAL VOC 2007, 2010, 2012                                                      AP; mAP (IOU=.50)\r\nRFB Net [78]                         PASCAL VOC 2007                                                                    mAP (IOU=.50)\r\nRFB Net [78]                                    COCO                                                 AP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL\r\nRefineDet [79]                   PASCAL VOC 2007, 2012                                                               mAP (IOU=.50)\r\nRefineDet [79]                                   COCO                                                 AP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL\r\nRetinaNet [80]                                   COCO                                                 AP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL\r\nR-FCN [54]                     PASCAL VOC 2007, 2012                                                               mAP (IOU=.50)\r\nR-FCN [54]                                      COCO                                                          AP@[.5:.05:.95];AP@.50; APS; APM; APL\r\nSSD [47]                        PASCAL VOC 2007, 2012                                                               mAP (IOU=.50)\r\nSSD [47]                                        COCOAP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL; AR1; AR10; AR100;AR\r\nS; ARM; ARLSSD [47]                                     ImageNet                                                                            mAP (IOU=.50)\r\nYolo v1 [48]PASCAL VOC 2007, 2012; Picasso;People-ArtAP; mAP (IOU=.50)\r\nYolo v2 [49]                     PASCAL VOC 2007, 2012                                                           AP; mAP (IOU=.50)\r\nYolo v2 [49]                                     COCOAP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL; AR1; AR10; AR100;AR\r\nS; ARM; ARL\r\nYolo v3 [50]                                     COCOAP@[.5:.05:.95]; AP@.50; AP@.75; APS; APM; APL; AR1; AR10; AR100;AR\r\nS; ARM; ARLYolo v4 [51]                                     COCO                                                 AP@[.5:.05:.95]; AP@.50; AP@.75; AP\r\nS; APM; APLYolo v5 [9]                                      COCO                                                                        AP@[.5:.05:.95]; AP@.50\r\nAn example of confusions that may arise in such a scenario is given by the fact that\r\nsome works affirm that the metrics AP@.50 and mAP (IOU=.50) are the same [54], which\r\nmay not always be true.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Keb0UFwPsG81GSOl4ZznPDcOyYIM/WyaRki2bu6IAgI="},"2b407770-11d4-414b-a1ca-f97d63c9b026":{"id_":"2b407770-11d4-414b-a1ca-f97d63c9b026","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"aGhgWdjk/qhnYd8XpGXpun6d/RFmV7bSoD0g1WX6nH8="},"PREVIOUS":{"nodeId":"986277e7-ff07-42bd-982f-765581bbb55c","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Keb0UFwPsG81GSOl4ZznPDcOyYIM/WyaRki2bu6IAgI="}},"text":"The origins of such misunderstandings are the differences in how\r\neach tool computes the corresponding metrics. The next section deals with this problem\r\nby detailing the implementations of the several object detection metrics and pointing out\r\ntheir differences. 4. Performance Metrics\r\nChallenges and online competitions have pushed forward the frontier of the object\r\ndetection field, improving results for specific datasets in every new edition. To validate\r\nthe submitted results, each competition applies a specific metric to rank the submitted\r\ndetections. These assessment criteria have also been used by the research community\r\nto report and compare object detection methods using different datasets as illustrated in\r\nTable 2. Among the popular metrics to report the results, this section will cover those\r\nused by the most popular competitions, namely Open Images RVC [81], COCO Detection\r\nChallenge [82],VOC  Challenge  [24],  Datalab  Cup  [83],  Google  AI  Open  Images  chal-\r\nlenge [84], Lyft 3D Object Detection for Autonomous Vehicles [85], and City Intelligence\r\nHackathon [86]. Object detectors aim to predict the location of objects of a given class in an\r\nimage or video with a high confidence. They do so by placing bounding boxes to identify\r\nthe positions of the objects. Therefore, a detection is represented by a set of three attributes:\r\nThe object class, the corresponding bounding box, and the confidence score, usually given","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"b4hKNO+uwwzYPUQsMJDh3Ja70K7luGgAsyxWBqYy/HE="},"6e66f860-1e11-4b38-be09-4fda96a62be3":{"id_":"6e66f860-1e11-4b38-be09-4fda96a62be3","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"pVBp3yjXdkAx8g15SojBOFAl1olJZwB48oyU9jo0eDw="},"NEXT":{"nodeId":"414ad66e-8cd3-435c-8a1c-d8a2175eedd6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"5ZSUR3w21YcK3j9WYjSJf4B/OAS/uWgtpVsegvFCHxI="}},"text":"Electronics2021,10, 2798 of 28\r\nby a value between 0 and 1 showing how confident the detector is about that prediction. The assessment is done based on:\r\n•A set of ground-truth bounding boxes representing the rectangular areas of an image\r\ncontaining objects of the class to be detected, and\r\n•a set of detections predicted by a model, each one consisting of a bounding box, a\r\nclass, and a confidence value. Detection evaluation metrics are used to quantify the performance of detection algo-\r\nrithms in different areas and fields [87,88]. In the case of object detection, the employed\r\nevaluation metrics measure how close the detected bounding boxes are to the ground-\r\ntruth bounding boxes. This measurement is done independently for each object class, by\r\nassessing the amount of overlap of the predicted and ground-truth areas. Consider a target object to be detected represented by a ground-truth bounding box\r\nBgtand the detected area represented by a predicted bounding boxBp. Without taking into\r\naccount a confidence level, a perfect match is considered when the area and location of the\r\npredicted and ground-truth boxes are the same. These two conditions are assessed by the\r\nintersection over union (IOU), a measurement based on the Jaccard Index, a coefficient of\r\nsimilarity for two sets of data [89]. In the object detection scope, the IOU is equal to the area\r\nof the overlap (intersection) between the predicted bounding boxBpand the ground-truth\r\nbounding boxBgtdivided by the area of their union, that is:\r\nJ(Bp,Bgt) =IOU=area(Bp\\Bgt)area(B\r\np[Bgt)\r\n,                                           (1)\r\nas illustrated in Figure 2. Figure 2.Illustration of the intersection over union (IOU). A perfect match occurs whenIOU=1and, if both bounding boxes do not intercept\r\neach other,IOU=0. The closer to1the IOU gets, the better the detection is considered. As\r\nobject detectors also perform the classification of each bounding box, only ground-truth\r\nand detected boxes of the same class are comparable through the IOU. By setting an IOU threshold, a metric can be more or less restrictive on considering\r\ndetections as correct or incorrect.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QPy8kgktJ03YVsELngU/SgH1LLtvDFB0vqaT6R1VCZA="},"414ad66e-8cd3-435c-8a1c-d8a2175eedd6":{"id_":"414ad66e-8cd3-435c-8a1c-d8a2175eedd6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"pVBp3yjXdkAx8g15SojBOFAl1olJZwB48oyU9jo0eDw="},"PREVIOUS":{"nodeId":"6e66f860-1e11-4b38-be09-4fda96a62be3","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"QPy8kgktJ03YVsELngU/SgH1LLtvDFB0vqaT6R1VCZA="}},"text":"An IOU threshold closer to1is more restrictive as it\r\nrequires almost-perfect detections, while an IOU threshold closer to, but different than0\r\nis more flexible, considering as detections even small overlaps betweenBpandBgt. IOU\r\nvalues are usually expressed in percentages, and the most used threshold values are50%\r\nand75%. In Sections 4.1, 4.2, and 4.4 the IOU is used to define the metrics that are most\r\nrelevant to object detection. 4.1. Precision and Recall\r\nLet us consider a detector that assumes that every possible rectangular region of the\r\nimage contains a target object (this would be done by placing bounding boxes of all possible\r\nsizes centered in every image pixel). If there is one object to be detected, the detector would\r\ncorrectly find it by one of the many predicted bounding boxes. That is not an efficient\r\nway to detect objects, as many wrong predictions are made as well. Conversely, a detector\r\nwhich never generates any bounding box, will never have a miss-detection. These extreme","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5ZSUR3w21YcK3j9WYjSJf4B/OAS/uWgtpVsegvFCHxI="},"b218c7e8-d22d-44eb-8f41-dd0e1827c040":{"id_":"b218c7e8-d22d-44eb-8f41-dd0e1827c040","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"TxNqVv2J0tLNwdw9xRpYU1HCm7NaPtRmRCbxZpryXzc="},"NEXT":{"nodeId":"8ee532dc-e743-46f9-baf4-6a4abeda2142","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"bZO3pIvh2uGMV0uPSJ3WCwZzW9Fx6VPE9t6oS4Aq8lY="}},"text":"Electronics2021,10, 2799 of 28\r\nexamples highlight two important concepts, referred as precision and recall, are further\r\nexplained below. Precision is the ability of a model to identify only relevant objects. It is the percentage\r\nof correct positive predictions. Recall is the ability of a model to find all relevant cases (all\r\nground-truth bounding boxes). It is the percentage of correct positive predictions among all\r\ngiven ground truths. To calculate the precision and recall values, each detected bounding\r\nbox must first be classified as:\r\n•       True positive (TP): A correct detection of a ground-truth bounding box;\r\n•False positive (FP): An incorrect detection of a non-existing object or a misplaced\r\ndetection of an existing object;\r\n•       False negative (FN): An undetected ground-truth bounding box. Assuming there is a dataset withGground-truths and a model that outputsNdetec-\r\ntions, of whichSare correct (S\u0014G), the concepts of precision and recall can be formally\r\nexpressed as:\r\nPr=\r\nSÂ\r\nn=1\r\nTPn\r\nSÂ\r\nn=1\r\nTPn+N\u0000SÂ\r\nn=1\r\nFPn\r\n=\r\nSÂ\r\nn=1\r\nTPn\r\nall detections,                                        (2)\r\nRc=\r\nSÂ\r\nn=1\r\nTPn\r\nSÂ\r\nn=1\r\nTPn+G\u0000SÂ\r\nn=1\r\nFNn\r\n=\r\nSÂ\r\nn=1\r\nTPn\r\nall ground truths. (3)\r\n4.2. Average Precision\r\nAs discussed above, the output of an object detector is characterized by a bounding\r\nbox, a class, and a confidence interval. The confidence level can be taken into account\r\nin the precision and recall calculations by considering as positive detections only those\r\nwhose confidence is larger than a confidence thresholdt. The detections whose confi-\r\ndence level is smaller thantare considered as negatives.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Xno6EWOqkqlHQkjipQldRnffC4fxtBiM4IQ6v2aLhyY="},"8ee532dc-e743-46f9-baf4-6a4abeda2142":{"id_":"8ee532dc-e743-46f9-baf4-6a4abeda2142","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"TxNqVv2J0tLNwdw9xRpYU1HCm7NaPtRmRCbxZpryXzc="},"PREVIOUS":{"nodeId":"b218c7e8-d22d-44eb-8f41-dd0e1827c040","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Xno6EWOqkqlHQkjipQldRnffC4fxtBiM4IQ6v2aLhyY="}},"text":"The detections whose confi-\r\ndence level is smaller thantare considered as negatives. By doing so, one may rewrite\r\nEquations (2) and (3) to consider this dependence on the confidence thresholdtas:\r\nPr(t) =\r\nSÂ\r\nn=1\r\nTPn(t)\r\nSÂ\r\nn=1\r\nTPn(t) +N\u0000SÂ\r\nn=1\r\nFPn(t)\r\n=\r\nSÂ\r\nn=1\r\nTPn(t)\r\nall detections(t),                         (4)\r\nRc(t) =\r\nSÂ\r\nn=1\r\nTPn(t)\r\nSÂ\r\nn=1\r\nTPn(t) +G\u0000SÂ\r\nn=1\r\nFNn(t)\r\n=\r\nSÂ\r\nn=1\r\nTPn(t)\r\nall ground truths. (5)\r\nBoth TP(t)and FP(t)are decreasing functions oft, as a largertreduces the number\r\nof positive detections. Conversely, FN(t)is an increasing function oft, since less positive\r\ndetections imply a larger number of negative detections. In addition,ÂTP(t) +ÂFN(t)\r\ndoes not depend ontand is a constant equal to the number of all the ground truths. Therefore, from Equation (5), the recallRc(t)is a decreasing function oft. On the other\r\nhand, nothing can be said a priori about the precisionPr(t), since both the numerator\r\nand denominator of Equation (4) are decreasing functions oft, and indeed the graph of","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"bZO3pIvh2uGMV0uPSJ3WCwZzW9Fx6VPE9t6oS4Aq8lY="},"c8946979-4b3a-458d-bcde-c00be67f38d5":{"id_":"c8946979-4b3a-458d-bcde-c00be67f38d5","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"m8u2UhdewRqA1VOxusq8CR+XVIupYWyiVkDN06eFB8c="},"NEXT":{"nodeId":"983daa78-077e-4119-8fa0-792c5a382fb9","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"KgzqzHdI6q9SUIYB6Oa7SQsrZWS5h2ygayjqdvqqnBw="}},"text":"Electronics2021,10, 27910 of 28\r\nPr(t)\u0002Rc(t)tends to exhibit a zig-zag behavior in practical cases, as later illustrated\r\nin Section 5. In practice, a good object detector should find all ground-truth objects (FN=0\u0011\r\nhigh recall), while identifying only relevant objects (FP=0\u0011high precision). Therefore,\r\na particular object detector can be considered good if, when the confidence threshold\r\ndecreases, its precision remains high as its recall increases. Hence, a large area under\r\nthe curve (AUC) tends to indicate both high precision and high recall. Unfortunately, in\r\npractical cases, the precision\u0002recall plot is often not monotonic, being zigzag-like instead,\r\nwhich poses challenges to an accurate measurement of its AUC. The average precision (AP) is a metric based on the area under aPr\u0002Rccurve that has\r\nbeen pre-processed to eliminate the zig-zag behavior. It summarizes this precision-recall\r\ntrade-off dictated by confidence levels of the predicted bounding boxes. To compute the AP, one starts by ordering theKdifferent confidence valuest(k)\r\noutput by the object detector as:\r\nt(k),k=1, 2, . . . ,Ksuch thatt(i)>t(j)fori>j. (6)\r\nSince theRcvalues also have a one-to-one, monotonic correspondence witht, which\r\nhas a one-to-one, monotonic, correspondence with the indexk, then thePr\u0002Rccurve\r\nis not continuous but sampled at the discrete pointsRc(t(k)), leading to the set of pairs\r\n(Pr(t(k),Rc(t(k))indexed byk. Now one defines an ordered set of reference recall valuesRr(n),\r\nRr(n),n=1, 2, . . . ,Nsuch thatRr(m)<Rr(n)form>n. (7)\r\nThe AP is computed using the two ordered sets in Equations (6) and (7). But before\r\ncomputing AP, the precision\u0002recall pairs have to be interpolated such that the resulting\r\nprecision\u0002recall curve is monotonic.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ypL6Myjg/sKXyX6mwcxb8uLcOj4xPsADLfqHan6Gq8s="},"983daa78-077e-4119-8fa0-792c5a382fb9":{"id_":"983daa78-077e-4119-8fa0-792c5a382fb9","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"m8u2UhdewRqA1VOxusq8CR+XVIupYWyiVkDN06eFB8c="},"PREVIOUS":{"nodeId":"c8946979-4b3a-458d-bcde-c00be67f38d5","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ypL6Myjg/sKXyX6mwcxb8uLcOj4xPsADLfqHan6Gq8s="}},"text":"The resulting interpolated curve is defined by a\r\ncontinuous functionPrinterp(R), whereRis a real value contained in the interval[0, 1],\r\ndefined as:\r\nPrinterp(R) =max\r\nkjRc(t(k))\u0015R\r\nfPr(t(k))g,                                          (8)\r\nwheret(k)is defined in Equation (6) andRc(t(k))is the recall value for the confidence\r\nt(k), computed according to Equation (5). The precision value interpolated at recallR\r\ncorresponds to the maximum precisionPrinterp(k)whose corresponding recall value is\r\ngreater than or equal toR. Note that an interpolation using a polynomial fitting would\r\nnot be convenient in this case, since a polynomial interpolation cannot guarantee that the\r\nresulting interpolated curve is monotonic. Now one is ready to compute AP by samplingPrinterp(R)at theNreference recall\r\nvaluesRrdefined in Equation (7). The AP is the area under thePr\u0002Rccurve calculated by\r\na Riemann integral ofPrinterp(R)using theKrecall values from the setRr(k)inEquation (7)\r\nas sampling points, that is,\r\nAP=KÂ\r\nk=0\r\n(Rr(k)\u0000Rr(k+1))Printerp(Rr(k)),                                   (9)\r\nwherePrinterp(R)is defined in Equation (8) andRr(k)is given by Equation (12), with\r\nRr(0) =1 andRr(K+1) =0. There are basically two approaches to compute this Riemann integral: TheN-point\r\ninterpolation and the all-point interpolation, as detailed below.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KgzqzHdI6q9SUIYB6Oa7SQsrZWS5h2ygayjqdvqqnBw="},"2d2da21d-6eed-457d-9bbf-7f289a83527c":{"id_":"2d2da21d-6eed-457d-9bbf-7f289a83527c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"6B64uFgJzcOpsXeEXQgpqHTZHFgI7cvixUR/HnFZltk="},"NEXT":{"nodeId":"cf7127f3-3e87-4044-b019-2d228acc2b36","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"QFL3r4WlJDm/skRs3+1v2e8Wh4SZ+eJVuHyaZX4Ptng="}},"text":"Electronics2021,10, 27911 of 28\r\n4.2.1.N-Point Interpolation\r\nIn theN-point interpolation, the set of reference recall valuesRr(n)for the compu-\r\ntation of the Riemann integral in Equation (9) are equally spaced in the interval[0, 1],\r\nthat is,\r\nRr(n) =N\u0000nN\u00001,n=1, 2, . . . ,N. (10)\r\nand thus the expression for AP becomes:\r\nAP=1NNÂ\r\nn=1\r\nPrinterp(Rr(n)). (11)\r\nActually theN-point interpolation as defined by Equation (11) computes an AP value\r\nwhich is equal to the value computed by the Riemann integral in Equation (9) multiplied\r\nbyN\u00001N. Popular applications of this interpolation method useN=101as in thecompetition [82]\r\nandN=11as initially adopted by the competition [24], which was later changed to the\r\nall-point interpolation method. 4.2.2. All-Point Interpolation\r\nFor the computation of AP using the so-called all-point interpolation, here referred\r\nto asAPall, as the set valuesRr(n)used to compute the Riemann integral in Equation (9)\r\ncorresponds exactly to the set of recall values computed considering allKconfidence levels\r\nt(k)in Equation (6), with the confidencest(0) =0andt(K+1) =1added so that the\r\npointsRr(0) =1 andRr(K+1) =0 are considered in Equation (9). More precisely,\r\nRr(0) =1,\r\nRr(k) =Rc(t(k)),k=1, 2, . . . ,K,                                     (12)\r\nRr(K+1) =0. whereRc(t(k))is given by Equation (5) withRc(t(0)) =1 andRc(t(K+1)) =0. Using this definition ofRr(k)in Equation (12),APallis computed usingEquation (9). In the all-point interpolation, instead of using the precision observed at only a few points,\r\nthe AP is obtained by interpolating the precision at each recall level. The Pascal Chal-\r\nlenge [24] adopts the all-point interpolation method to compute the average precision.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"MoMFT9S8ZsyUIAeUWrR92I7+q/HmDX7pgv0WBB6lW+k="},"cf7127f3-3e87-4044-b019-2d228acc2b36":{"id_":"cf7127f3-3e87-4044-b019-2d228acc2b36","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"6B64uFgJzcOpsXeEXQgpqHTZHFgI7cvixUR/HnFZltk="},"PREVIOUS":{"nodeId":"2d2da21d-6eed-457d-9bbf-7f289a83527c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"MoMFT9S8ZsyUIAeUWrR92I7+q/HmDX7pgv0WBB6lW+k="}},"text":"4.3. Mean Average Precision\r\nRegardless of the interpolation method, AP is obtained individually for each class. In\r\nlarge datasets with many classes, it is useful to have a unique metric that is able to represent\r\nthe exactness of the detections among all classes. For such cases, the mean average precision\r\n(mAP) is computed, which is simply the average AP over all classes [8,47], that is,\r\nmAP=1CCÂ\r\ni=1\r\nAPi,                                                        (13)\r\nwhereAPiis the AP value for thei-th class andCis the total number of classes being\r\nevaluated. 4.4. Average Recall\r\nThe average recall (AR) [90] is another evaluation metric used to measure the assertive-\r\nness of object detectors for a given class. Unlike the average precision, the confidences\r\nof the estimated detections are not taken into account in AR computation. This turns all\r\ndetections into positive ones, which is equivalent to setting the confidence threshold as\r\nt=0 in Equations (4) and (5).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QFL3r4WlJDm/skRs3+1v2e8Wh4SZ+eJVuHyaZX4Ptng="},"ace5f143-f7b0-4942-a3b4-e4ca3f10e6ac":{"id_":"ace5f143-f7b0-4942-a3b4-e4ca3f10e6ac","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"iskYQQgCfZMjeo1uyLmT8WJqUaIw0d34fySP7vnm+yc="},"NEXT":{"nodeId":"c74c0a50-9c15-4059-95b8-ec8c796634a0","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"HCf9WWnWs7gfPKptyUsml+eNS4IQEOGLS37d8vARpYk="}},"text":"Electronics2021,10, 27912 of 28\r\nThe AR metric makes an evaluation at a large range of IOU thresholds, by taking into\r\naccount all recall values obtained for IOU thresholds in the interval[0.5, 1]. An IOU of\r\n0.5 can be interpreted as a rough localization of an object and is the least acceptable IOU\r\nby most of the metrics, and an IOU equal to 1 is equivalent to the perfect location of the\r\ndetected object. Therefore, by averaging recall values in the interval[0.5, 1], the model is\r\nevaluated on the condition of the object location being considerably accurate. Letobe the IOU overlap between a ground truth and a detected bounding box as\r\ncomputed by Equation (1), andRcIOU(o)a function that retrieves the recall for a given IOU\r\no. The AR is defined as twice the area under theRcIOU(o)\u0002ocurve for the IOU interval\r\n[0.5, 1], that is,\r\nAR=2\r\nZ1\r\n0.5\r\nRcIOU(o)do. (14)\r\nThe authors in [90] also give a straightforward equation for the computation of the\r\nabove integral from the discrete sample set, as twice the average of the excess IOU for all\r\nthe ground-truths, that is,\r\nAR=2GGÂ\r\ni=1\r\nmax(IOUi\u00000.5, 0),                                            (15)\r\nwhereIOUidenotes the best IOU obtained for a given ground truthiandGis the total\r\nnumber of ground-truths. Interestingly, COCO also reports the AR, although its definition does not match exactly\r\nthat in Equation(15). Instead, what is reported as the COCO AR is the average of the\r\nmaximum obtained recall across several IOU thresholds. To do so one first defines a set of\r\nOIOU thresholds:\r\nt(o),o=1, 2, . . . ,O.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"I/MilErBbjFRBiTRbjA6MqxWweb0QzfpP+qtVimOoDI="},"c74c0a50-9c15-4059-95b8-ec8c796634a0":{"id_":"c74c0a50-9c15-4059-95b8-ec8c796634a0","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"iskYQQgCfZMjeo1uyLmT8WJqUaIw0d34fySP7vnm+yc="},"PREVIOUS":{"nodeId":"ace5f143-f7b0-4942-a3b4-e4ca3f10e6ac","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"I/MilErBbjFRBiTRbjA6MqxWweb0QzfpP+qtVimOoDI="}},"text":". . ,O. (16)\r\nThen, lettingPrt(o)(t(k)),Rct(o)(t(k))be the precision\u0002recall points for a confidence\r\nt(k), given the IOU thresholdt(o), the COCO AR is computed as:\r\nAR=1OOÂ\r\no=1\r\nmax\r\nkjPrt(o)(t(k))>0\r\nfRct(o)(t(k))g,                                     (17)\r\nthat is, the average of the largest recall values such that the precision is greater than zero\r\nfor each IOU threshold, andt(k)as defined in Equation(6). Effectively, this yields a coarse\r\napproximation of the original integral in Equation (14), provided that the IOU threshold\r\nsett(o)covers an adequate range of overlaps. 4.5. Mean Average Recall\r\nAs  the  AR  is  calculated  individually  for  each  class,  similarly  to  what  is  done  to\r\ncompute mAP, a unique AR value can be obtained considering the mean AR among all\r\nclasses, that is:\r\nmAR=1CCÂ\r\ni=1\r\nARi. (18)\r\nIn the sequel, a practical example illustrates the differences reflected in the final result\r\ndepending on the chosen method. 5. A Numerical Example\r\nConsidering the set of 12 images in Figure 3, each image, except (a), (g), and (j), has at\r\nleast one target object of the classcat, whose ground-truth locations are delimited by the\r\ngreen rectangles. There is a total of 12 target objects limited by the green boxes. Images\r\n(b), (e), and (f) have each two ground-truth samples of the target class. An object detector\r\npredicted 12 objects represented by the red rectangles (labeled with letters ‘A’ to ‘L’) with\r\ntheir associated confidence levels being represented as percentages also shown close to","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HCf9WWnWs7gfPKptyUsml+eNS4IQEOGLS37d8vARpYk="},"7d2ddc5a-0d02-444c-88b9-4c2d99ccfe25":{"id_":"7d2ddc5a-0d02-444c-88b9-4c2d99ccfe25","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"7tT06et02m7yAViKIL5KW8Hn8PqzI/FPY4EEbzwwA6c="},"NEXT":{"nodeId":"ba4b4f48-d3f4-418a-9aab-fd99007a4c42","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"DQ7xsffzkFMJjCmcmlXDOsyl8oKHA9SNwLAd5nv2fwo="}},"text":"Electronics2021,10, 27913 of 28\r\nthe corresponding boxes. From the above, images (a), (g), and (j) are expected to have no\r\ndetection, and images (b), (e), and (f) are expected to have two detections each. All things considered, to evaluate the precision and recall of the 12 detections it is\r\nnecessary to establish an IOU thresholdt, which will classify each detection as TP or FP. In\r\nthis example, let us first consider as TP the detections with IOU>50%, that ist=0.5. As stated before, AP is a metric that integrates precision and recall in different confi-\r\ndence values. Thus, it is necessary to count the amount of TP and FP classifications given\r\nthe different confidence levels. Table 3 presents each detection from our example sorted\r\nby their confidence levels. In this table, columnsÂTP(t)andÂFP(t)are the accumulated\r\nTPs and FPs, respectively, whose corresponding confidence levels are larger than or equal\r\nto the confidencetspecified in the second column of the table. Precision (Pr(t)) and recall\r\n(Rc(t)) values are calculated based on Equations(4)and(5), respectively. In this example\r\na detection is considered as a TP only if its IOU is larger than 50%, and in this case the\r\ncolumn ‘IOU>0.5? ’ is marked as ‘Yes’, otherwise it is marked as ‘No’ and is considered\r\nan FP. In this example, all detections overlap some ground-truth withIOU>0.5, except\r\ndetection ‘J’, which is not overlapping any ground-truth, so there is no IOU to be computed\r\nin this case. (a)(b)                                            (c)                                       (d)\r\n(e)(f)                                       (g)                                     (h)\r\n(i)(j)                                                    (k)                                              (l)\r\nFigure 3.Samples of 12 images from the PASCAL VOC 2012 dataset [67] with ground-truth objects of the classcatin\r\ngreen boxes, and the detections performed by [9] in red boxes along with their respected confidence levels. In samples\r\n(b–d,f,h,i,k,l) the amount of the ground-truth and detected objects is the same.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"T5kLudtYCnVdciHnNf2u+6I6uRGkcGcE/hDr9Kdq9Qc="},"ba4b4f48-d3f4-418a-9aab-fd99007a4c42":{"id_":"ba4b4f48-d3f4-418a-9aab-fd99007a4c42","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"7tT06et02m7yAViKIL5KW8Hn8PqzI/FPY4EEbzwwA6c="},"PREVIOUS":{"nodeId":"7d2ddc5a-0d02-444c-88b9-4c2d99ccfe25","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"T5kLudtYCnVdciHnNf2u+6I6uRGkcGcE/hDr9Kdq9Qc="}},"text":"In samples of images (a,g), no ground-truth\r\nobject should be detected but one false detection occurred in image (j). In sample (e) there are two target objects to be\r\ndetected, but the detector missed one of them. Some detectors can output one detection overlapping multiple ground truths, as seen\r\nin the image from Figure 3b with detections ‘A’ and ‘B’. As detection ‘A’ has a higher\r\nconfidence than ‘B’ (89%>82%), ‘A’ has the preference over ‘B’ to match the ground-truth,\r\nso ‘A’ is associated with the ground truth which gives the highest IOU. Figure 4c,d show","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DQ7xsffzkFMJjCmcmlXDOsyl8oKHA9SNwLAd5nv2fwo="},"2e50274c-b6a2-419e-bf9d-d0b38053c4b7":{"id_":"2e50274c-b6a2-419e-bf9d-d0b38053c4b7","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"MtsTVYWIr7Q7qQxqsiySYOcV6rTXl1YkkYpkDyCOIZc="},"NEXT":{"nodeId":"3ffcf0c6-29f1-45dd-a669-45d6280ba945","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"SfcRZGYVGsDfgarvWgXyhRMDBEP6+CXgtvmkya2P2/I="}},"text":"Electronics2021,10, 27914 of 28\r\nthe two possible associations that ‘A’ can have, ending up with the first one, which presents\r\na higher IOU. Detection ‘B’ is left with the remaining ground truth in Figure 4f. Another\r\nsimilar situation where one detection could be associated with more than one ground truth\r\nis faced by detection ‘E’ in Figure 3e. The application of the same rule results in matching\r\ndetection ‘E’ with the ground truth whose IOU is the highest, represented by the fairer cat,\r\nat the bottom of the image. Table 3.Precision and recall values for detections in Figure 3, that contain a total of 12 ground truths, considering an IOU\r\nthresholdt=0.5.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"W0eEO0VlyrGmcxXWz7/eJaLSqCmzElinfPAhUNtH83w="},"3ffcf0c6-29f1-45dd-a669-45d6280ba945":{"id_":"3ffcf0c6-29f1-45dd-a669-45d6280ba945","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"MtsTVYWIr7Q7qQxqsiySYOcV6rTXl1YkkYpkDyCOIZc="},"PREVIOUS":{"nodeId":"2e50274c-b6a2-419e-bf9d-d0b38053c4b7","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"W0eEO0VlyrGmcxXWz7/eJaLSqCmzElinfPAhUNtH83w="},"NEXT":{"nodeId":"5772766e-90ee-4a92-9f98-119c96721a6a","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"WM2PTj8AJ1PtJfbGLIzhl8xDdX6ukZ6SobO6y+HTwfs="}},"text":"Bounding Box       Confidence(t)IOU             IOU>0.5?ÂTP(t)ÂFP(t)Pr(t)Rc(t)\r\nD                           99%                    0.91                     Yes                         1                          0                     1.0000                  0.0833\r\nK                           98%                    0.70                     Yes                         2                          0                     1.0000                  0.1667\r\nC                           95%                    0.86                     Yes                         3                          0                     1.0000                  0.2500\r\nH                           95%                    0.72                     Yes                         4                          0                     1.0000                  0.3333\r\nL                           94%                    0.91                     Yes                         5                          0                     1.0000                  0.4167\r\nI                            92%                    0.86                     Yes                         6                          0                     1.0000                  0.5000\r\nA                           89%                    0.92                     Yes                         7                          0                     1.0000                  0.5833\r\nF                            86%                    0.87                     Yes                         8                          0                     1.0000                  0.6667\r\nJ                            85%                       -                        No                         8                          1                     0.8889                  0.6667\r\nB                           82%                    0.84                     Yes                         9                          1                     0.9000                  0.7500\r\nE                           81%                    0.74                     Yes                        10                         1                     0.9091                  0.8333\r\nG                           76%                    0.76                     Yes                        11                         1                     0.9167                  0.9167\r\n(a)                                                 (b)\r\n(c)(d)                                            (e)                                            (f)\r\nFigure 4.Particular cases showing detected bounding boxes overlapping multiple ground truths. (a) Original image with\r\npredicted (red) and ground-truth (green) bounding boxes. (b) Bounding boxes only. (c,d) Possible overlaps of the first\r\nground truth. (c) Detection ‘A’ overlapping the first ground truth with IOU=.92. (d) Detection ‘A’ overlapping the second\r\nground truth with IOU=.20.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"SfcRZGYVGsDfgarvWgXyhRMDBEP6+CXgtvmkya2P2/I="},"5772766e-90ee-4a92-9f98-119c96721a6a":{"id_":"5772766e-90ee-4a92-9f98-119c96721a6a","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"MtsTVYWIr7Q7qQxqsiySYOcV6rTXl1YkkYpkDyCOIZc="},"PREVIOUS":{"nodeId":"3ffcf0c6-29f1-45dd-a669-45d6280ba945","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"SfcRZGYVGsDfgarvWgXyhRMDBEP6+CXgtvmkya2P2/I="}},"text":"(d) Detection ‘A’ overlapping the second\r\nground truth with IOU=.20. (e,f) Possible overlaps of the second ground truth. (e) Detection ‘B’ overlapping the first ground\r\ntruth with IOU=.19. (f) Detection ‘B’ overlapping the second ground truth with IOU=.84. By choosing a more restrictive IOU threshold, different precisionPr(t)and recall\r\nRc(t)values can be obtained. Table 4 computes the precision and recall values with a\r\nmore strict IOU threshold oft=0.75. By that, it is noticeable the occurrence of more\r\nFP detections and less TP detections, thus reducing both the precisionPr(t)and recall\r\nRc(t)values. Graphical representations of thePr(t)\u0002Rc(t)values presented in Tables 3 and 4 can\r\nbe seen in Figure 5. By comparing both curves, one may note that for this example:","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WM2PTj8AJ1PtJfbGLIzhl8xDdX6ukZ6SobO6y+HTwfs="},"f57ee23b-e6ba-4ce4-bda0-392961099515":{"id_":"f57ee23b-e6ba-4ce4-bda0-392961099515","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"RsQIol14XLAtxjru/GxslXGVdoGUeoPtwJOj7Wu/I3Q="},"NEXT":{"nodeId":"00e305fe-13dc-4215-b564-05519cec51a2","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"S1gvTm+e1Ypqc0o2PC3a5ZCjejEFfN3evYTAHjQPGdA="}},"text":"Electronics2021,10, 27915 of 28\r\n•With a less restrictive IOU threshold (t=0.5), higher recall values can be obtained\r\nwith the highest precision. In other words, the detector can retrieve about66.5%of\r\nthe total ground truths without any miss detection. •Usingt=0.75, the detector is more sensitive to different confidence valuest. This is\r\nexplained by the more accentuated monotonic behavior for this IOU threshold. •Regardless the IOU threshold applied, this detector can never retrieve100%of the\r\nground truths (Pr(t) =1) for any confidence valuet. This is due to the fact that the\r\nalgorithm failed to output any bounding box for one of the ground truthsin Figure 3e. Table 4.Precision and recall values for detections in Figure 3, that contain a total of 12 ground truths, considering an IOU\r\nthresholdt=0.75.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5sdcDUhnD8HVXKAdw4Ea6toauPR3vn+SyLFZ51e9ct0="},"00e305fe-13dc-4215-b564-05519cec51a2":{"id_":"00e305fe-13dc-4215-b564-05519cec51a2","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"RsQIol14XLAtxjru/GxslXGVdoGUeoPtwJOj7Wu/I3Q="},"PREVIOUS":{"nodeId":"f57ee23b-e6ba-4ce4-bda0-392961099515","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"5sdcDUhnD8HVXKAdw4Ea6toauPR3vn+SyLFZ51e9ct0="},"NEXT":{"nodeId":"a618bd84-652d-49ed-85cc-ade20f17c95f","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"I6kPiEmxmlmW8uqCdx2O5DbZ4QhI/L23Dw8EVnuoKNQ="}},"text":"Bounding Box         Confidence (t)             IOU            IOU>0.75?ÂTP(t)ÂFP(t)Pr(t)Rc(t)\r\nD                              99%                        0.91                     Yes                         1                          0                   1.0000               0.0833\r\nK                              98%                        0.70                     No                         1                          1                   0.5000               0.0833\r\nC                               95%                        0.86                     Yes                         2                          1                   0.6667               0.1667\r\nH                              95%                        0.72                     No                         2                          2                   0.5000               0.1667\r\nL                               94%                        0.91                     Yes                         3                          2                   0.6000               0.2500\r\nI                               92%                        0.86                     Yes                         4                          2                   0.6667               0.3333\r\nA                              89%                        0.92                     Yes                         5                          2                   0.7143               0.4167\r\nF                               86%                        0.87                     Yes                         6                          2                   0.7500               0.5000\r\nJ                               85%                          -                        No                         6                          3                   0.6667               0.5000\r\nB                               82%                        0.84                     Yes                         7                          3                   0.7000               0.5833\r\nE                               81%                        0.74                     No                         7                          4                   0.6364               0.5833\r\nG                              76%                        0.76                     Yes                         8                          4                   0.6667               0.6667\r\n\u0003\u0002\u0003\u0003\u0002\u0005\u0003\u0002\u0006\u0003\u0002\b\u0003\u0002\t\u0004\u0002\u0003\r\n\u0018\u0012\u0011\u0010\u0014\u0014\r\n\u0003\u0002\u0003\r\n\u0003\u0002\u0005\r\n\u0003\u0002\u0006\r\n\u0003\u0002\b\r\n\u0003\u0002\t\r\n\u0004\u0002\u0003\r\n\u0017\u0018\u0012\u0011\u0013\u0019\u0013\u0016\u0015\r\n\u000e\u0018\u0012\u0011\u0013\u0019\u0013\u0016\u0015\u0000\u001b\u0000\u0018\u0012\u0011\u0010\u0014\u0014\u0000\u0017\u0016\u0013\u0015\u001a\u0019\r\n\u000b\u0014\u0010\u0019\u0019\n\u0000\u0011\u0010\u001a\u0001\u0000\f\r\u000f\n\u0000\u0003\u0002\u0007\u0003\r\n\u0017\u0018\u0012\u0011\u0013\u0019\u0013\u0016\u0015\u0000\u001b\u0000\u0018\u0012\u0011\u0010\u0014\u0014\u0000\u0017\u0016\u0013\u0015\u001a\u0019\r\n\u0003\u0002\u0003\u0003\u0002\u0005\u0003\u0002\u0006\u0003\u0002\b\u0003\u0002\n\u0004\u0002\u0003\r\n\u0019\u0013\u0012\u0011\u0015\u0015\r\n\u0003\u0002\u0003\r\n\u0003\u0002\u0005\r\n\u0003\u0002\u0006\r\n\u0003\u0002\b\r\n\u0003\u0002\n\r\n\u0004\u0002\u0003\r\n\u0018\u0019\u0013\u0012\u0014\u001a\u0014\u0017\u0016\r\n\u000f\u0019\u0013\u0012\u0014\u001a\u0014\u0017\u0016\u0000\u001c\u0000\u0019\u0013\u0012\u0011\u0015\u0015\u0000\u0018\u0017\u0014\u0016\u001b\u001a\r\n\f\u0015\u0011\u001a\u001a\u000b\u0000\u0012\u0011\u001b\u0001\u0000\r\u000e\u0010\u000b\u0000\u0003\u0002\t\u0007\r\n\u0018\u0019\u0013\u0012\u0014\u001a\u0014\u0017\u0016\u0000\u001c\u0000\u0019\u0013\u0012\u0011\u0015\u0015\u0000\u0018\u0017\u0014\u0016\u001b\u001a\r\n(a)(b)\r\nFigure 5.Precision\u0002Recall points with values calculated for:  (a) Results provided in Table 3.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"S1gvTm+e1Ypqc0o2PC3a5ZCjejEFfN3evYTAHjQPGdA="},"a618bd84-652d-49ed-85cc-ade20f17c95f":{"id_":"a618bd84-652d-49ed-85cc-ade20f17c95f","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"RsQIol14XLAtxjru/GxslXGVdoGUeoPtwJOj7Wu/I3Q="},"PREVIOUS":{"nodeId":"00e305fe-13dc-4215-b564-05519cec51a2","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"S1gvTm+e1Ypqc0o2PC3a5ZCjejEFfN3evYTAHjQPGdA="}},"text":"(b) Results provided\r\nin Table 4. Note  that  Figure  5  suggests  that  an  IOU  threshold  oft=0.5is  less  affected by\r\ndifferent confidence levels. The graph for the lowest IOU threshold (t=0.5) shows that\r\nwhen confidence levelstare high, the precisionPr(t)does not vary, being equal to the\r\nmaximum (1.0) for most of confidence valuest. However, in order to detect more objects\r\n(increasing the recallRc(t)), it is necessary to set a lower confidence thresholdt, which\r\nreduces the precision at most by12%. On the other hand, considering the highest IOU\r\nthreshold (t=0.75), the detector can retrieve half of the target objects (recall =0.5) with a\r\nprecision of 0.75.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"I6kPiEmxmlmW8uqCdx2O5DbZ4QhI/L23Dw8EVnuoKNQ="},"f8f866bf-b0a1-48e1-937d-b7031f53e387":{"id_":"f8f866bf-b0a1-48e1-937d-b7031f53e387","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Mnyoy+FpRawcfoYYzVkO7q/j45njP387DroHLDF0fGo="},"NEXT":{"nodeId":"d1536560-6689-4ed8-bb30-b56050f67df5","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"s/kQne76UYsitW73qvk/u3uWAjsm/irD8dmymZdBw6U="}},"text":"Electronics2021,10, 27916 of 28\r\nAs previously explained, different methods can be applied to estimate the average pre-\r\ncision, that is, the area under the precision\u0002recall curve. To obtain AP using theN-point\r\ninterpolation in Equation(11)withN=11points, the area under thePr\u0002Rccurve is com-\r\nputed as the average of the interpolated precisionPrinterp(R)(Equation (9))samples consid-\r\nering the sampling recall pointsRatRr(n)in the setf0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0g\r\n(Equation (10)). On the other hand, to obtain AP using the all-point interpolation approach,\r\nthe area under thePr\u0002Rccurve is computed by the Riemann integral inEquation (9),\r\nsampling  the  recall  pointsRatRr(n)coincident  with  theRc(t)values  given  by  the\r\nlast column of Table 3 or ofTable 4 (Equation (12)). The results can be seen inFigure 6. When an IOU thresholdt=0.5was applied, the 11-point interpolation method obtained\r\nAP=88.64%while the all-point interpolation method resulted in a slightly higher AP,\r\nreachingAP=89.58%. Similarly, for an IOU thresholdt=0.75, the 11-point interpolation\r\nmethod obtained AP=49.24% and the all-point interpolation obtained AP=50.97%. \u0005\u0004\u0005\u0005\u0004\u0006\u0005\u0004\u0007\u0005\u0004\b\u0005\u0004\t\u0005\u0004\n\u0005\u0004\u000b\u0005\u0004\f\u0005\u0004\r\u0005\u0004\u000e\u0006\u0004\u0005\r\n\u001f\u0019\u0017\u0016\u001b\u001b\r\n\u0005\u0004\u0005\r\n\u0005\u0004\u0007\r\n\u0005\u0004\t\r\n\u0005\u0004\u000b\r\n\u0005\u0004\r\r\n\u0006\u0004\u0005\r\n\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\r\n\u0012\u001c!\u0019\u001f\u001e\u001d\u001b\u0016!\u0019\u0018\u0000\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\u0000\"\u0000\u001f\u0019\u0017\u0016\u001b\u001b\u0000\u001e\u001d\u001a\u001c!","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"W+nzTSEXmOyUH+tBw1q+9hdTb1tFeIHjmW0roKlRWbo="},"d1536560-6689-4ed8-bb30-b56050f67df5":{"id_":"d1536560-6689-4ed8-bb30-b56050f67df5","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"Mnyoy+FpRawcfoYYzVkO7q/j45njP387DroHLDF0fGo="},"PREVIOUS":{"nodeId":"f8f866bf-b0a1-48e1-937d-b7031f53e387","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"W+nzTSEXmOyUH+tBw1q+9hdTb1tFeIHjmW0roKlRWbo="}},"text":"\u0011\u001b\u0016  \u000f\u0000\u0017\u0016!\u0002\u0000\u0010\u0014\u000f\u0000\r\r\u0004\u000b\t\u0001\u0002\u0000\u0012\u0013\u0015\u000f\u0000\u0005\u0004\n\u0005\r\n\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\u0000\"\u0000\u001f\u0019\u0017\u0016\u001b\u001b\u0000\u001e\u001d\u001a\u001c! \u0006\u0006\u0003\u001e\u001d\u001a\u001c!\u0000\u001a\u001c!\u0019\u001f\u001e\u001d\u001b\u0016!\u0019\u0018\u0000\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\r\n(a)(b)\r\n\u0005\u0004\u0005\u0005\u0004\u0006\u0005\u0004\u0007\u0005\u0004\b\u0005\u0004\t\u0005\u0004\n\u0005\u0004\u000b\u0005\u0004\f\u0005\u0004\r\u0005\u0004\u000e\u0006\u0004\u0005\r\n\u001f\u0019\u0017\u0016\u001b\u001b\r\n\u0005\u0004\u0005\r\n\u0005\u0004\u0007\r\n\u0005\u0004\t\r\n\u0005\u0004\u000b\r\n\u0005\u0004\r\r\n\u0006\u0004\u0005\r\n\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\r\n\u0012\u001c!\u0019\u001f\u001e\u001d\u001b\u0016!\u0019\u0018\u0000\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\u0000\"\u0000\u001f\u0019\u0017\u0016\u001b\u001b\u0000\u001e\u001d\u001a\u001c! \u0011\u001b\u0016  \u000f\u0000\u0017\u0016!\u0002\u0000\u0010\u0014\u000f\u0000\t\u000e\u0004\u0007\t\u0001\u0002\u0000\u0012\u0013\u0015\u000f\u0000\u0005\u0004\f\n\r\n\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\u0000\"\u0000\u001f\u0019\u0017\u0016\u001b\u001b\u0000\u001e\u001d\u001a\u001c! \u0006\u0006\u0003\u001e\u001d\u001a\u001c!\u0000\u001a\u001c!\u0019\u001f\u001e\u001d\u001b\u0016!\u0019\u0018\u0000\u001e\u001f\u0019\u0017\u001a \u001a\u001d\u001c\r\n(c)(d)\r\nFigure 6.Results for different approaches for computing the AP metric. (a) 11-point interpolation with IOU threshold\r\nt=0.5. (b) All-point interpolation with IOU thresholdt=0.5. (c) 11-point interpolation with IOU thresholdt=0.75. (d) All-point interpolation with IOU thresholdt=0.75. When a lower IOU thresholdtwas considered (t=0.5as opposed tot=0.75), the\r\nAP was considerably increased in both interpolation approaches. This is caused by the\r\nincrease in the TP detections, due to a lower IOU threshold.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"s/kQne76UYsitW73qvk/u3uWAjsm/irD8dmymZdBw6U="},"3b9ba718-faf9-4cd1-8cdc-69aa7c42f2bb":{"id_":"3b9ba718-faf9-4cd1-8cdc-69aa7c42f2bb","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"q7NOzWJaJ1+CJml34wVcQAtn1xhN3PvH27nRDQlrgWA="},"NEXT":{"nodeId":"93f10b47-49f9-479c-8aad-5baa802ad2b8","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"9vi1Fll4ZyKjOCBJyvPp1RgNQsXda0VcL0uMXwmH6Xw="}},"text":"Electronics2021,10, 27917 of 28\r\nIf focus is shifted towards how well localized the detections are, irrespective of their\r\nconfidence values, it is sensible to consult the AR metrics (Equations (14)–(18)). Computing\r\ntwice the average excess IOU for the samples in this practical example as in Equation(15),\r\nyieldsAR=60%, while computing the average max recall across the standard COCO IOU\r\nthresholds, that ist2f0.50, 0.55,. . ., 0.95g, as in Equation(17), yieldsAR=66%. As the\r\nlatter computation effectively does a coarser quantization of the IOU space, the two AR\r\nfigures differ slightly. The next section enlists and briefly describes which variations of the\r\nmetrics based on AP and AR are more frequently employed in the literature. In most cases\r\nthey are the result of combinations of different IOU thresholds and interpolation methods. 6. Most Employed Metrics Based on AP and AR\r\nAs previously presented,  there are different ways to evaluate the area under the\r\nprecision\u0002recall and recall\u0002IOU curves. Nonetheless, besides such combinations of\r\ndifferent IOU thresholds and interpolation points, that are other variations that result in\r\ndifferent metric values. Some methods limit the evaluation by object scales and detections\r\nper image. This section overviews the distinctions behind all the metrics shown in Table 2. 6.1. AP with IOU Threshold t=0.5\r\nThis AP metric is widely used to evaluate detections in the PASCAL VOC dataset [67]. Its official implementation is in MATLAB and it is available in the PASCAL VOC toolkit. It measures the AP of each class individually by computing the area under the precision\r\n\u0002recall curve interpolating all points as presented in Equation(9). In order to classify\r\ndetections as TP or FP the IOU threshold is set tot=0.5. 6.2. mAP with IOU Threshold t=0.5\r\nThis metric is also used by the PASCAL VOC dataset and is also available in their\r\nMATLAB toolkit. It is calculated as the AP with IOUt=0.5, but the result obtained by\r\neach class is averaged as given in Equation (13). 6.3.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"AOEO9bhCcFpnv6Z2O7PDQYgZMN3tRQodNNE7AEXiwjc="},"93f10b47-49f9-479c-8aad-5baa802ad2b8":{"id_":"93f10b47-49f9-479c-8aad-5baa802ad2b8","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"q7NOzWJaJ1+CJml34wVcQAtn1xhN3PvH27nRDQlrgWA="},"PREVIOUS":{"nodeId":"3b9ba718-faf9-4cd1-8cdc-69aa7c42f2bb","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"AOEO9bhCcFpnv6Z2O7PDQYgZMN3tRQodNNE7AEXiwjc="}},"text":"6.3. AP@.5 and AP@.75\r\nThese two metrics evaluate the precision\u0002recall curve differently than the PASCAL\r\nVOC metrics. In this method, the interpolation is performed inN=101recall points, as\r\ngiven in Equation(11). Then, the computed results for each class are summed up and\r\ndivided by the number of classes, as in Equation (13). The only difference between AP@.5 and AP@.75 regards the applied IOU thresholds. AP@.5 usest=0.5whereas AP@.75 appliest=0.75. These metrics are commonly used\r\nto report detections performed in the COCO dataset and are officially available in their\r\nofficial evaluation tool. 6.4. AP@[.5:.05:.95]\r\nThis metric expands the AP@.5 and AP@.75 metrics by computing the AP@ with\r\n10 different IOU thresholds (t= [0.5, 0.55, ..., 0.95]) and taking the average among all\r\ncomputed results. 6.5. APS, APM, and APL\r\nThese three metrics, also referred to as AP Across Scales, apply the AP@[.5,.05:.95]\r\nfrom Section 6.1 taking into consideration the area of the ground-truth object:\r\n•      APSonly evaluates small ground-truth objects (area < 322pixels);\r\n•      APMonly evaluates medium-sized ground-truth objects (322< area < 962pixels);\r\n•      APLonly evaluates large ground-truth objects (area > 962). When evaluating objects of a given size, objects of the other sizes (both ground-truth\r\nand predicted) are not considered in the evaluation. This metric is also part of the COCO\r\nevaluation dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9vi1Fll4ZyKjOCBJyvPp1RgNQsXda0VcL0uMXwmH6Xw="},"118f6ae7-6c86-4a03-946f-8c9b7dd441cc":{"id_":"118f6ae7-6c86-4a03-946f-8c9b7dd441cc","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"1pXIRrzmCrAYzZCPPD+xrNenxR719xZH1FEJEoV9dpU="},"NEXT":{"nodeId":"d1ed118d-0c75-4283-8a8a-585c8c7cafee","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"eqBrXTLeQT1J7xvZZJE0SmEuQWSxxpecLQhMo5D64ps="}},"text":"Electronics2021,10, 27918 of 28\r\n6.6. AR1, AR10, and AR100\r\nThese AR variations apply Equation(14)limiting the number of detections per image,\r\nthat is, they calculate the AR given a fixed amount of detections per image, averaged\r\nover all classes and IOUs. The IOUs used to measure the recall values are the same as\r\nin AP@[.5,.05:.95]. AR1considers up to one detection per image, while AR10and AR100consider at most\r\n10 and 100 objects per image, respectively. 6.7. ARS, ARMand ARL\r\nSimilarly to the AR variations with limited number of detections per image, these\r\nmetrics evaluate detections considering the same areas as the AP across scales. As the\r\nmetrics based on AR are implemented in the COCO official evaluation tool,  they are\r\nregularly reported with the COCO dataset. 6.8. F1-Score\r\nThe F1-score is defined as the harmonic mean of the precision (Pr) and recall (Rc) of a\r\ngiven detector, that is:\r\nF1=2Pr.RcPr+Rc=TPTP+FN+FP\r\n2\r\n. (19)\r\nThe F1-score is limited to the interval [0, 1], being 0 if precision or recall (or both) are\r\n0, and 1 when both precision and recall are 1. As the F1-score does not take into account different confidence values, it is only used\r\nto compare object detectors in a fixed confidence threshold levelt. 6.9. Other Metrics\r\nOther less popular metrics have also been proposed to evaluate object detections. They are mainly designed to be applied with particular datasets. The Open Images Object\r\nDetection Metric, for example, is similar to mAP (IOU=.50), being specifically designed to\r\nconsider special ground-truth annotations of the Open Images dataset [68]. This dataset\r\ngroups into a single annotation five or more objects of the same class that somehow are\r\noccluding each other, such asa group of flowersora group of people. This metric simply\r\nignores a detection if it overlaps a ground-truth box tagged asgroup of, whose area of\r\nintersection between the detection and ground-truth boxes divided by the area of the\r\ndetection is greater than 0.5.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"8f/UHXbX+RqeB8Esh7QA/488bMJnWHCDHg+GZxD/YYE="},"d1ed118d-0c75-4283-8a8a-585c8c7cafee":{"id_":"d1ed118d-0c75-4283-8a8a-585c8c7cafee","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"1pXIRrzmCrAYzZCPPD+xrNenxR719xZH1FEJEoV9dpU="},"PREVIOUS":{"nodeId":"118f6ae7-6c86-4a03-946f-8c9b7dd441cc","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"8f/UHXbX+RqeB8Esh7QA/488bMJnWHCDHg+GZxD/YYE="}},"text":"This way, it does not penalize detections matching a group of\r\nvery close ground-truth objects. The localization recall-precision (LRP) error, a new metric suggested in [91], intends\r\nto consider the accuracy of the detected bounding box localization and equitably evaluate\r\nsituations where the AP is unable to distinguish very different precision\u0002recall curves. 6.10. Comparisons among Metrics\r\nIn  practice,  the  COCO’s  AP@[.5:.05:.95]  and  PASCAL  mAP  metrics  are  the  most\r\npopular ones used as benchmarks. However, as COCO’s AP@[.5:.05:.95] is affected by\r\ndifferent IOUs, it is not possible to evaluate the effectiveness of the detector with a more\r\nor less restrictive IOU with this metric. For a more strict evaluation with respect to the\r\nlikeness of the ground truth and detection bounding boxes, the AP@.75 metric should be\r\napplied. In datasets where the objects appear to have relatively different sizes, AP metrics\r\nconcerning their areas should be employed. By that, the assertiveness of objects with\r\nsimilar relative sizes can be compared. As shown in this work, the interpolation methods\r\napplied by the AP metrics try to remove the non-monotonic behavior of thePr(t)\u0002Rc(t)\r\ncurve before calculating its AUC. In anN-point interpolation, a greaterNleads to a better\r\nAUC approximation. Therefore, the 101-point interpolation approach used by COCO’s AP\r\nmetrics provides a better AUC approximation than the 11-point interpolation approach. On the other hand, PASCAL VOC uses the all-point interpolation, which is an even better\r\napproximation of the AUC. In cases where the detector is expected to detect at least a","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"eqBrXTLeQT1J7xvZZJE0SmEuQWSxxpecLQhMo5D64ps="},"ca10acdb-e423-49ed-b3ca-40f1e24fdefe":{"id_":"ca10acdb-e423-49ed-b3ca-40f1e24fdefe","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"goRiAFzQnucVU0vFCbP4jGSQSTrO+62k6BnNTkZ+QyM="},"NEXT":{"nodeId":"cedd0416-c27b-42eb-9a72-50e9b1799a5f","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"NDXzZKO6GPRtW10q31fuGAHnnHx3EFJbm5psTw/upt0="}},"text":"Electronics2021,10, 27919 of 28\r\ncertain amount of objects in a given image (e.g., detecting one bird in a flock of birds should\r\nbe sufficient), AR metrics regarding detections or sizes are more appropriate. 7. Evaluating Object Detection in Videos\r\nMany works in the literature use the mAP as a metric to evaluate the performance of\r\nobject detection models in video sequences [92–98]. In this case, the frames are evaluated\r\nindependently, ignoring the spatio-temporal characteristics of the objects presented in\r\nthe scene. The authors of [92] categorized the ground-truth objects according to their\r\nmotion speed. This is done by measuring the average IOU score of the current frame\r\nand the nearby\u000610frames. In this context, in addition to mAP, they also reported the\r\nmAP over the slow, medium, and fast groups, denoted as mAP(slow), mAP(medium), and\r\nmAP(fast), respectively. In some applications, the latency in the identification of the objects of interest plays\r\na crucial role in how well the overall system will perform. Detection delay, defined as\r\nthe number of frames between the first occurrence of an object in the scene and its first\r\ndetection, then becomes an important measurement for time-critical systems. The authors\r\nin [99] claim that AP is not sufficient to quantify the temporal behavior of detectors, and\r\npropose a complementary metric, the average delay (AD), averaging the mean detection\r\ndelay over multiple false positive ratio thresholds, and over different object sizes, yielding\r\na metric that fits well for systems that rely on timely detections. While the cost of detection\r\nlatency is significant for a somewhat niche set of tasks, the inclusion of time information\r\nfor video detection metrics can be useful to assess system behaviors that would otherwise\r\nbe elusive when only the standard, frame level AP metric is used. 7.1. Spatio-Temporal Tube Average Precision\r\nAs discussed above, the aforementioned metrics are all used on an image or frame\r\nlevel. However, when dealing with videos, one may be interested in evaluating the model\r\nperformance at the whole video level. In this work, we propose an extension of the AP\r\nmetric to evaluate video object detection models that we refer to as spatio-temporal tube\r\nAP (STT-AP).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5IM2lx2tVj4yY2XRD4Vud9GkXwKBtMqqejBPxgkiV9g="},"cedd0416-c27b-42eb-9a72-50e9b1799a5f":{"id_":"cedd0416-c27b-42eb-9a72-50e9b1799a5f","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"goRiAFzQnucVU0vFCbP4jGSQSTrO+62k6BnNTkZ+QyM="},"PREVIOUS":{"nodeId":"ca10acdb-e423-49ed-b3ca-40f1e24fdefe","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"5IM2lx2tVj4yY2XRD4Vud9GkXwKBtMqqejBPxgkiV9g="}},"text":"As in AP, a threshold over the IOU is also used to determine whether the\r\ndetections are correct or not. However, instead of using two types of overlaps (spatial and\r\ntemporal), we extend the standard IOU definition to consider the spatio-temporal tubes\r\ngenerated by the detection and of the ground truth. This metric, that integrates spatial and\r\ntemporal localizations, is concise, yet expressive. Instead of considering each detection of the same object independently along the\r\nframes, the spatial bounding boxes of the same object are concatenated along the temporal\r\ndimension, forming a spatio-temporal tube, which is the video analogous to an image\r\nbounding box. A spatio-temporal tubeToof an objectois the spatio-temporal region\r\ndefined as the concatenation of the bounding boxes of this object from each frame of a\r\nvideo, that is:\r\nTo=\u0002Bo,qBo,q+1\u0001\u0001\u0001Bo,q+Q\u00001\u0003,                                              (20)\r\nwhereBo,kis the bounding box of the objectoin framekof the video that is constituted of\r\nQframes indexed byk=q,q+1, . . . ,q+Q\u00001. Using spatio-temporal tubes, the concept of IOU used in object detection in images\r\n(see Section 4) can be naturally extended to videos. Considering a ground-truth spatio-\r\ntemporal tubeTgtand a predicted spatio-temporal tubeTp, the spatio-temporal tube IOU\r\n(STT-IOU) measures the ratio of the overlapping to the union of the “discrete volume”\r\nbetweenTgtandTp, such that:\r\nSTT-IOU=volume(Tp\\Tgt)volume(T\r\np[Tgt)\r\n=\r\nÂ\r\nk\r\narea of overlap in framek\r\nÂ\r\nk\r\narea of union in framek,                 (21)\r\nas illustrated in Figure 7.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NDXzZKO6GPRtW10q31fuGAHnnHx3EFJbm5psTw/upt0="},"f9f01f49-b55f-4b05-8df7-5335fe8e2099":{"id_":"f9f01f49-b55f-4b05-8df7-5335fe8e2099","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"H5tcEPMNvmfSmX1n9DRAvN5arKFOSE/8yfzKl6Zyo/E="},"NEXT":{"nodeId":"aa3a2014-f4fc-4072-b1fe-fd5b6a9d8ce2","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"/gto5tzy9wt+roiHRSTS6SiC3b0aXbDxaIgsQACJpb8="}},"text":"Electronics2021,10, 27920 of 28\r\nvolume of overlap\r\nvolume of unionSTT-IOU =volume of union=\r\nFigure 7.Spatio-temporal tube IOU (STT-IOU). In this way, an object is considered a TP if the STT-IOU is equal or greater than a\r\nchosen threshold. As in the conventional AP, this metric may be as rigorous as desired. The closer to 1 it is, the more well-located the predicted tube must be to be considered a TP. Figure 8 illustrates four different STTs. The STTs in red are detected STTs and the STTs\r\nin green are ground-truth STTs. The STT in (1) constitutes an FP case, and the STT in (2) may\r\nbe a TP case (depending on the STT-IOU) , as it intersects the corresponding ground-truths\r\nSTT (3). Since there is no detection corresponding to the ground-truth STT (4), it is an FN. time\r\nframeq\r\nframeq+ 1\r\nframeq+l\r\nframeq+ (l+ 1)\r\nGround-truth STT\r\nDetected STT\r\nDetected bounding boxes\r\nGround-truth bounding boxes\r\n12\r\n34\r\nFigure 8.Illustration of STTs. The STT (1) constitutes a false positive (FP). The STT (2) may constitute true positive\r\n(TP) (depending on the STT-IOU) as the detection STT intersects the corresponding ground-truth STT (3). The STT (4)\r\nconstitutes a false negative (FN). Based on these definitions, the proposed STT-AP metric follows the AP: For each class,\r\neach ground-truth tube is associated with the predicted tube of the same class that presents\r\nthe highest STT-IOU scores (since it is higher than the threshold). The ground-truth tubes\r\nnot associated with a predicted tube are FNs and the predicted tubes not associated with a\r\nground-truth tube are FPs. Then, the spatio-temporal tube predictions are ranked according\r\nto the predicted confidence level (from the highest to the lowest), irrespective of correctness.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"s5gLwxhBbp2Oz3Tjva1t3+dpCR2XNlSI9FCsswSuGVw="},"aa3a2014-f4fc-4072-b1fe-fd5b6a9d8ce2":{"id_":"aa3a2014-f4fc-4072-b1fe-fd5b6a9d8ce2","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"H5tcEPMNvmfSmX1n9DRAvN5arKFOSE/8yfzKl6Zyo/E="},"PREVIOUS":{"nodeId":"f9f01f49-b55f-4b05-8df7-5335fe8e2099","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"s5gLwxhBbp2Oz3Tjva1t3+dpCR2XNlSI9FCsswSuGVw="}},"text":"Since the STT-AP evaluates the detection of an object in the video as a whole, the confidence\r\nlevel assumed for a spatio-temporal tube is the average confidence of the bounding boxes\r\ncorresponding to each of its constituent frames. After that, the all-point interpolation","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/gto5tzy9wt+roiHRSTS6SiC3b0aXbDxaIgsQACJpb8="},"9fe722e5-53ab-4df2-aeb0-dac7291d51d2":{"id_":"9fe722e5-53ab-4df2-aeb0-dac7291d51d2","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"LDDcDZVW2bhKY0YMp7eTuZDWT7LKVEkKRnh3MXXhl4E="},"NEXT":{"nodeId":"61ed8a12-391c-4529-b87d-3d8867893ef9","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"aJKmWDqY5DEcYsV+7RrdciJeDDpiSk4bYHCpUqxCInM="}},"text":"Electronics2021,10, 27921 of 28\r\n(Section 4.2.2) is performed allowing one to compute the proposed STT-AP. This procedure\r\nmay be repeated and averaged for all classes in the database, yielding the so-called mean\r\nSTT-AP (mSTT-AP). From Equation(21)one can readily see that the computational cost\r\nper frame of the SST-AP is similar to the one of the frame-by-frame mAP. 8. An Open-Source Toolbox\r\nThis paper focuses on explaining and comparing the different metrics and formats\r\ncurrently used in object detection, detailing the specifications and pointing out the par-\r\nticularities of each metric variation. The existing tools provided by popular competi-\r\ntions[24,81–85]are not adequate to evaluate metrics using annotations in formats that are\r\ndifferent from their native ones. Thus, to complement the analysis of the metrics presented\r\nhere, the authors have developed and released an open-source toolkit as a reliable source\r\nof object detection metrics for the academic community and researchers. With more than3100stars and740forks, our previously available tool for object detec-\r\ntion assessment [100] has received positive feedback from the community and researchers. It has also been used as the official tool in competition [86], adopted in 3rd-party libraries\r\nsuch as [101], and parts of our code have been used by many other works such as in\r\nYoloV5 [9]. Besides the significant acceptance by the community, we have received many\r\nrequests to expand the tool in order to support new metrics and bounding box formats. Such demands motivated us to offer more evaluation metrics, to accept more bounding\r\nbox formats, and to present a novel metric for object detection in videos. This tool implements the same metrics used by the most popular competitions and\r\nobject-detection benchmark researches. This implementation does not require modifications\r\nof the detection model to match complicated input formats, avoiding conversions to XML,\r\nJSON, CSV, or other file types. It supports more than eight different kinds of annotation\r\nformats, including the ones presented in Table 1. To ensure the accuracy of the results,\r\nthe implementation strictly followed the metric definitions and the output results were\r\ncarefully validated against the ones of the official implementations.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GEXICEnyRSvxHorhwkozOwdHQSumwxDKmAiKMoxjUWQ="},"61ed8a12-391c-4529-b87d-3d8867893ef9":{"id_":"61ed8a12-391c-4529-b87d-3d8867893ef9","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"LDDcDZVW2bhKY0YMp7eTuZDWT7LKVEkKRnh3MXXhl4E="},"PREVIOUS":{"nodeId":"9fe722e5-53ab-4df2-aeb0-dac7291d51d2","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"GEXICEnyRSvxHorhwkozOwdHQSumwxDKmAiKMoxjUWQ="}},"text":"Developed in Phython and supporting 14 object detection metrics for images, this\r\nwork also incorporates the novel spatio-temporal metric described in Section 7.1 to evaluate\r\ndetected object in videos aggregating some of the concepts applied to evaluate detections\r\nin images. From a practical point of view, the tool can also be adapted and expanded to\r\nsupport new metrics and formats. The expanded project distributed with this paper can be\r\naccessed at https://github.com/rafaelpadilla/review_object_detection_metrics [102]. 9. Metrics Evaluation in a Practical Example\r\nIn this section, we use different object detection metrics to evaluate YOLOv5 model [9]. The chosen model was trained with the COCO dataset and was applied in the train-\r\ning/validation PASCAL VOC 2012 dataset [67]. Intentionally different datasets were used\r\nto train and evaluate the model to evidence the potential of our tool to deal with different\r\nground-truth and detected bounding-box formats. For this experiment, the annotations of\r\nthe ground-truth boxes are in PASCAL VOC format containing 20 classes of objects, while\r\nthe model was trained with COCO dataset and was able to detect objects in 80 classes,\r\npredicting detections in text files in the YOLO format. By using our tool, one can quickly obtain 14 different metrics without the necessity to\r\nconvert files to specific formats. As some classes of the ground-truth dataset are tagged\r\ndifferently by the detector (e.g., PASCAL VOC classtvmonitoris referred to astvin COCO\r\ndataset), the only required work is to provide a text file listing the names of the classes\r\nin the ground-truth format. This way the evaluation tool can recognize that the detected\r\nobjectairplaneshould be evaluated asaeroplane. A total of17,125of images from the train/val PASCAL VOC 2012 dataset containing\r\n40,138objects of 20 classes were evaluated by the YOLOV5 model to detect objects in 80 dif-\r\nferent classes. A total of74,752detections were detected by the model.Figure 9 compares\r\nthe distribution of ground-truth and detected objects per class. Due to the difference of","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"aJKmWDqY5DEcYsV+7RrdciJeDDpiSk4bYHCpUqxCInM="},"8c526887-d171-428d-85aa-893042bb2080":{"id_":"8c526887-d171-428d-85aa-893042bb2080","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"AJc8Wd+YQ0w3I56Nm2yZitthiMRYBX9xrHhB7BP9+XQ="}},"text":"Electronics2021,10, 27922 of 28\r\nclasses in the training and testing datasets, many predicted classes are not in the ground-\r\ntruth set, so detections of the extra classes are ignored by the metrics. 17,50015,00012,50010,000\r\n5,0002,500\r\n7,500\r\n(a) Class distribution in the ground-truth dataset. 30,00025,00020,00015,00010,000\r\n5,000\r\n(b) Class distribution of the detected objects. Figure 9.Class distributions: (a) Ground-truth bounding boxes. (b) Detected bounding boxes. The AP results for each class are presented in Table 5. The highest AP values over all\r\nclasses were obtained when the AUC was measured with the 11-point interpolation method\r\nand an IOU threshold oft=0.5, resulting inmAP=0.58. As expected for all cases, a more\r\nrigorous IOU threshold (t=0.75) resulted in a smaller AP. Comparing the individual AP\r\nresults among all classes, the most difficult object for all interpolation methods was the\r\npotted plant, having an AP not higher than0.37for an IOU threshold oft=0.5and an AP\r\nnot higher than 0.22 with an IOU threshold oft=0.75.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GFgJmMnphdVwPo7NYbsUvV89I2wbDPGXivmPaRqVeWc="},"a9d30358-d539-445c-8100-1a5fb93d8814":{"id_":"a9d30358-d539-445c-8100-1a5fb93d8814","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"19GW6RtjpsM3MRXiyl6N5/1U79StDFnU/jf7gU/RpgQ="},"NEXT":{"nodeId":"08420902-5e5c-400d-a73e-32a3fcb46e02","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"0MjwfL9W6pMBvbebp3KnD9FRwfYPuU1VW44h4Sseals="}},"text":"Electronics2021,10, 27923 of 28\r\nTable 5.AP results obtained with different interpolation methods and IOU thresholds.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"eifmPDaD7PeUBWv02XEknT8atgmZp+beCp0aeKUvPe0="},"08420902-5e5c-400d-a73e-32a3fcb46e02":{"id_":"08420902-5e5c-400d-a73e-32a3fcb46e02","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"19GW6RtjpsM3MRXiyl6N5/1U79StDFnU/jf7gU/RpgQ="},"PREVIOUS":{"nodeId":"a9d30358-d539-445c-8100-1a5fb93d8814","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"eifmPDaD7PeUBWv02XEknT8atgmZp+beCp0aeKUvPe0="},"NEXT":{"nodeId":"b3e4462c-54ed-4543-bbd4-048876b2e2c3","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"aIlnD6xeERZOYxeUe4hpM8mt9P2WjvtXAhe0Iedr8pk="}},"text":"ClassIOU Threshold = 0.5                                                              IOU Threshold = 0.75\r\n101-Point                 11-Point                 All-Point                101-Point                 11-Point                 All-Point\r\naeroplane                     0.760.790.77                          0.57                          0.58                          0.58\r\nbicycle                        0.41                          0.43                          0.41                          0.31                          0.33                          0.31\r\nbird                          0.66                          0.67                          0.66                          0.49                          0.48                          0.50\r\nboat                          0.47                          0.46                          0.47                          0.28                          0.29                          0.29\r\nbottle                         0.45                          0.47                          0.45                          0.32                          0.34                          0.33\r\nbus                           0.79                          0.78                          0.80                          0.74                          0.69                          0.74\r\ncar                           0.52                          0.53                          0.53                          0.39                          0.39                          0.39\r\ncat                           0.73                          0.74                          0.73                          0.54                          0.53                          0.54\r\nchair                         0.41                          0.40                          0.41                          0.30                          0.32                          0.30\r\ncow                          0.74                          0.69                          0.74                          0.59                          0.58                          0.60\r\ndiningtable                    0.44                          0.46                          0.44                          0.28                          0.31                          0.28\r\ndog                          0.66                          0.64                          0.65                          0.53                          0.53                          0.53\r\nhorse                         0.42                          0.43                          0.43                          0.35                          0.36                          0.35\r\nmotorbike                     0.51                          0.53                          0.51                          0.38                          0.39                          0.38\r\nperson                        0.67                          0.65                          0.68                          0.53                          0.54                          0.53\r\npottedplant                   0.37                          0.39                          0.37                          0.22                          0.25                          0.22\r\nsheep                         0.68                          0.68                          0.68                          0.56                          0.58                          0.57\r\nsofa                          0.44                          0.45                          0.44                          0.37                          0.38                          0.37\r\ntrain                          0.75                          0.77                          0.76                          0.65                          0.66                          0.65\r\ntvmonitor                     0.54                          0.55                          0.54                          0.43                          0.45                          0.43\r\naverage                       0.57                          0.58                          0.57                          0.44                          0.45                          0.44\r\nThe results obtained by the variations which apply AP and AR with different sizes\r\nand quantity of objects per image are summarized in Table 6.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0MjwfL9W6pMBvbebp3KnD9FRwfYPuU1VW44h4Sseals="},"b3e4462c-54ed-4543-bbd4-048876b2e2c3":{"id_":"b3e4462c-54ed-4543-bbd4-048876b2e2c3","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"19GW6RtjpsM3MRXiyl6N5/1U79StDFnU/jf7gU/RpgQ="},"PREVIOUS":{"nodeId":"08420902-5e5c-400d-a73e-32a3fcb46e02","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"0MjwfL9W6pMBvbebp3KnD9FRwfYPuU1VW44h4Sseals="}},"text":"Table 6.Values of AP and average recall (AR) variations for different object sizes and number of\r\ndetections per image. Metric                                                                            Result\r\nAPS0.13\r\nAPM0.33\r\nAPL0.46\r\nAR10.39\r\nAR100.53\r\nAR1000.53\r\nARS0.23\r\nARM0.47\r\nARL0.58\r\nEven if the same interpolation technique is applied, the results may vary depending\r\non the IOU threshold. Similarly, different interpolations with the same IOU threshold may\r\nalso lead to distinct results. The metrics considering objects in different scales are useful to compare the assertive-\r\nness of detections in datasets containing objects of different scales. In the COCO dataset,\r\nfor instance, roughly42%of the objects are considered small (area <322pixels),34%are\r\nconsidered medium (322< area <962pixels), and24%are considered large (area >962\r\npixels). This explains the vast amount of works using this dataset to report their results. 10. Conclusions\r\nThis work analyzed the formats of bounding boxes used to represent the objects in\r\npopular datasets, demonstrated the most common benchmark object detection metrics, and\r\nsuggested a new metric for videos, the spatio-temporal tube average precision (STT-AP),\r\nbased on the concepts used to evaluate object detection in images. The similarities and","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"aIlnD6xeERZOYxeUe4hpM8mt9P2WjvtXAhe0Iedr8pk="},"695c03eb-534e-464e-a189-271577eb1bb4":{"id_":"695c03eb-534e-464e-a189-271577eb1bb4","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"jCZkJukACM08jNOChUcBhrZSo5BHTdVqC6nbigB8Kxg="},"NEXT":{"nodeId":"390ba6a8-4f19-4a51-9ad2-b3994a8ad919","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"9pV4jKvngGgL+egvRoyRrxFSDROLvAN25JjFiPFK7UE="}},"text":"Electronics2021,10, 27924 of 28\r\ninconsistencies of each metric were examined, and our results revealed their dissimilarities\r\nby evaluating the predictions of a pre-trained object detector in a largely used dataset. A\r\ntoolkit implementing all described metrics in a way compatible to most data-annotation\r\nformats in use was presented and validated. Such results may facilitate direct and unified\r\ncomparisons among most algorithms being proposed in the field of object detection. For\r\nfuture work, we intend to perform a regular survey update through its companion website,\r\nincorporating newly proposed metrics and annotation formats, and extend it to the problem\r\nof object tracking. Author Contributions:Conceptualization, all authors; methodology, all authors; software, R.P.,\r\nW.L.P. and T.L.B.D.; validation, R.P., W.L.P. and T.L.B.D.; formal analysis, all authors; investigation, all\r\nauthors; writing—original draft preparation, R.P., W.L.P. and T.L.B.D.; writing—review and editing,\r\nS.L.N. and E.A.B.d.S.; visualization, R.P., W.L.P. and T.L.B.D.; supervision, S.L.N. and E.A.B.d.S.;\r\nfunding acquisition, S.L.N. and E.A.B.d.S. All authors have read and agreed to the published version\r\nof the manuscript. Funding:This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal\r\nde Nível Superior – Brasil (CAPES) – Finance Code 001, Conselho Nacional de Desenvolvimento\r\nCientífico e Tecnológico (CNPq) and Fundação de Amparo à Pesquisa do Estado do Rio de Janeiro\r\n(FAPERJ). Wesley L. Passos and Eduardo A. B. da Silva are also partially funded by the Google Latin\r\nAmerica Research Awards (LARA) 2020. Data Availability Statement:The images used in Section 5 are part of the PASCAL VOC dataset\r\nand can be downloaded at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QJ4M8gPn9CK5Sr6fMto96CnPcMMeN1K+dqqsbgCsf28="},"390ba6a8-4f19-4a51-9ad2-b3994a8ad919":{"id_":"390ba6a8-4f19-4a51-9ad2-b3994a8ad919","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"jCZkJukACM08jNOChUcBhrZSo5BHTdVqC6nbigB8Kxg="},"PREVIOUS":{"nodeId":"695c03eb-534e-464e-a189-271577eb1bb4","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"QJ4M8gPn9CK5Sr6fMto96CnPcMMeN1K+dqqsbgCsf28="},"NEXT":{"nodeId":"2659ef8d-8be8-425c-bbd2-dcf2d63c7a15","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"H1QZjna4E0AQU10+X3qmgndW1RouLk+f9MRfmMpLqDI="}},"text":"The\r\nmodel presented in Section 9 was trained with the COCO dataset, which can be downloaded at\r\nhttps://cocodataset.org, and tested with images from the PASCAL VOC dataset, which can be\r\ndownloaded at http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html. The tool presented\r\nin this work can be accessed at https://github.com/rafaelpadilla/review_object_detection_metrics. Acknowledgments:The authors gratefully acknowledge the support of NVIDIA Corporation with\r\nthe donation of the Titan X Pascal GPU used for this research. Conflicts of Interest:The authors declare no conflict of interest. References\r\n1.Cheng, F.C.;  Huang, S.C.;  Ruan, S.J. Illumination-Sensitive Background Modeling Approach for Accurate Moving Object\r\nDetection.IEEE Trans. Broadcast.2011,57, 794–801. [CrossRef]\r\n2.Khan, F.S.; Anwer, R.M.; Van De Weijer, J.; Bagdanov, A.D.; Vanrell, M.; Lopez, A.M. Color Attributes for Object Detection. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Providence, RI, USA, 16–21\r\nJune 2012; pp. 3306–3313. 3.Ouyang, W.;  Wang, X. A Discriminative Deep Model for Pedestrian Detection with Occlusion Handling. In Proceedings\r\nof the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Providence, RI, USA, 16–21 June 2012;\r\npp. 3258–3265. 4.Gao, T.; Packer, B.; Koller, D. A Segmentation-Aware Object Detection Model with Occlusion Handling. In Proceedings of\r\nthe 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Colorado Springs, CO, USA, 20–25 June 2011;\r\npp. 1361–1368. 5. Zou, Z.; Shi, Z.; Guo, Y.; Ye, J. Object Detection in 20 Years: A Survey.arXiv2019, arXiv:1905.05055. 6.Krizhevsky, A.; Sutskever, I.; Hinton, G.E.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9pV4jKvngGgL+egvRoyRrxFSDROLvAN25JjFiPFK7UE="},"2659ef8d-8be8-425c-bbd2-dcf2d63c7a15":{"id_":"2659ef8d-8be8-425c-bbd2-dcf2d63c7a15","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"jCZkJukACM08jNOChUcBhrZSo5BHTdVqC6nbigB8Kxg="},"PREVIOUS":{"nodeId":"390ba6a8-4f19-4a51-9ad2-b3994a8ad919","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"9pV4jKvngGgL+egvRoyRrxFSDROLvAN25JjFiPFK7UE="}},"text":"ImageNet Classification with Deep Convolutional Neural Networks. In Proceedings of\r\nthe 25th International Conference on Neural Information Processing Systems (NeurIPS), Lake Tahoe, NV, USA, 3–8 December\r\n2012; pp. 1097–1105. 7.Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. Gradient-Based Learning Applied to Document Recognition.Proc. IEEE1998,\r\n86, 2278–2324. [CrossRef]\r\n8.Ren, S.; He, K.; Girshick, R.; Sun, J. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.IEEE\r\nTrans. Pattern Anal. Mach. Intell.2017,39, 1137–1149. [CrossRef]\r\n9.Jocher, G.; Stoken, A.; Borovec, J.; NanoCode012, C.; Changyu, L.; Laughing, H. ultralytics/yolov5: v3.0. 2020. Available online:\r\nhttps://github.com/ultralytics/yolov5 (accessed on 20 December 2020). 10. Dollar, P.; Wojek, C.; Schiele, B.; Perona, P. Pedestrian Detection: An Evaluation of the State of the Art.IEEE Trans. Pattern Anal. Mach. Intell.2012,34, 743–761. [CrossRef]\r\n11.Ohn-Bar, E.; Trivedi, M.M. To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection. In Proceedings of the\r\n23rd International Conference on Pattern Recognition (ICPR), Cancun, Mexico, 4–8 December 2016; pp. 3350–3355.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"H1QZjna4E0AQU10+X3qmgndW1RouLk+f9MRfmMpLqDI="},"37e8e63f-7de4-44f6-a028-7ca9d298dba7":{"id_":"37e8e63f-7de4-44f6-a028-7ca9d298dba7","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"g2XiYlHKBPLi/pYzSA3Ta1HPhACPAnEvD3fBziB3Gwc="},"NEXT":{"nodeId":"7e9eb8de-ff85-419f-b71d-413cca7df45c","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"jGH18Np/xK/n4hRUBwHNca8jirFQvDMYvCxd/XmX/aI="}},"text":"Electronics2021,10, 27925 of 28\r\n12. Viola, P.; Jones, M. Robust Real-Time Object Detection.Int. J. Comput. Vis.2004,57, 137–154. [CrossRef]\r\n13.Hirano, Y.; Garcia, C.; Sukthankar, R.; Hoogs, A. Industry and Object Recognition: Applications, Applied Research and Challenges. Towar. Categ. Level Object Recognit.2006,4170, 49–64. 14. Wang, X. Intelligent Multi-Camera Video Surveillance: A Review.Pattern Recognit. Lett.2013,34, 3–19. [CrossRef]\r\n15.Franke, K.; Srihari, S.N. Computational Forensics: An Overview. In Proceedings of the International Workshop on Computational\r\nForensics (IWCF), Washington, DC, USA, 7–8 August 2008; pp. 1–10. 16.Baltieri, D.; Vezzani, R.; Cucchiara, R. 3DPes: 3D People Dataset for Surveillance and Forensics. In Proceedings of the 2011\r\nJoint ACM Workshop on Human Gesture and Behavior Understanding, Scottsdale, AZ, USA, 28 November–1 December 2011;\r\npp. 59–64. 17.Olabarriaga, S.D.; Smeulders, A.W. Interaction in the Segmentation of Medical Images:  A Survey.Med Image Anal.2001,\r\n5, 127–142. [CrossRef]\r\n18.Cootes, T.F.; Taylor, C.J. Statistical Models of Appearance for Medical Image Analysis and Computer Vision. In Proceedings of\r\nthe Medical Imaging 2001: Image Processing, San Diego, CA, USA, 3 July 2001; pp. 236–248. 19.Ganster, H.; Pinz, P.; Rohrer, R.; Wildling, E.; Binder, M.; Kittler, H. Automated Melanoma Recognition.IEEE Trans. Med Imaging\r\n2001,20, 233–239.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"BZTE3o5OIQesbEcsvrMzwg7S+FR98uXj4Zddp2hHo/w="},"7e9eb8de-ff85-419f-b71d-413cca7df45c":{"id_":"7e9eb8de-ff85-419f-b71d-413cca7df45c","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"g2XiYlHKBPLi/pYzSA3Ta1HPhACPAnEvD3fBziB3Gwc="},"PREVIOUS":{"nodeId":"37e8e63f-7de4-44f6-a028-7ca9d298dba7","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"BZTE3o5OIQesbEcsvrMzwg7S+FR98uXj4Zddp2hHo/w="},"NEXT":{"nodeId":"7a875d8d-2d1c-4d33-8d82-7817f50f700b","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"HrTLwgcRiYJfnY18Ts8cv3KdwdFxVmCrBHo18OSeL/M="}},"text":"Med Imaging\r\n2001,20, 233–239. [CrossRef]\r\n20.Janai, J.; Güney, F.; Behl, A.; Geiger, A.Computer Vision for Autonomous Vehicles:  Problems, Datasets and State of the Art; Now\r\nPublishers: Norwell, MA, USA; Delft, The Netherlands, 2020. 21.Buch, N.; Velastin, S.A.; Orwell, J. A Review of Computer Vision Techniques for the Analysis of Urban Traffic.IEEE Trans. Intell. Transp. Syst.2011,12, 920–939. [CrossRef]\r\n22.Zhao, Z.; Zheng, P.; Xu, S.; Wu, X. Object Detection With Deep Learning: A Review.IEEE Trans. Neural Networks Learn. Syst.2019,\r\n30, 3212–3232. [CrossRef]\r\n23.Padilla, R.; Netto, S.L.; da Silva, E.A.B. A Survey on Performance Metrics for Object-Detection Algorithms. In Proceedings of the\r\n27th International Conference on Systems, Signals and Image Processing (IWSSIP), Niteroi, Brazil, 1–3 July 2020; pp. 237–242. 24.Everingham, M.;  Eslami, S.M.A.;  Van Gool, L.;  Williams, C.K.I.;  Winn, J.;  Zisserman, A. The Pascal Visual Object Classes\r\nChallenge: A Retrospective.Int. J. Comput. Vis.2015,111, 98–136. [CrossRef]\r\n25.Attneave, F.; Arnoult, M.D. The Quantitative Study of Shape and Pattern Perception.Psychol. Bull.1956,53, 452–471. [CrossRef]\r\n[PubMed]\r\n26.Roberts, L.G. Machine Perception of Three-Dimensional Solids. PhD Thesis, Massachusetts Institute of Technology, Cambridge,\r\nMA, USA, 1963. 27.Lowe, D.G.; Binford, T.O. The Recovery of Three-Dimensional Structure from Image Curves.IEEE Trans. Pattern Anal. Mach. Intell.1985,7, 320–326.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jGH18Np/xK/n4hRUBwHNca8jirFQvDMYvCxd/XmX/aI="},"7a875d8d-2d1c-4d33-8d82-7817f50f700b":{"id_":"7a875d8d-2d1c-4d33-8d82-7817f50f700b","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"g2XiYlHKBPLi/pYzSA3Ta1HPhACPAnEvD3fBziB3Gwc="},"PREVIOUS":{"nodeId":"7e9eb8de-ff85-419f-b71d-413cca7df45c","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"jGH18Np/xK/n4hRUBwHNca8jirFQvDMYvCxd/XmX/aI="},"NEXT":{"nodeId":"10d35630-e735-4674-bd5d-15664ff4a4f3","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ua/pQRZS7G0Z7E8yGaymXI0c9z30yNmm9G5oVvziwNc="}},"text":"Mach. Intell.1985,7, 320–326. [CrossRef] [PubMed]\r\n28.Harris, C.G.; Stephens, M. A Combined Corner and Edge Detector. In Proceedings of the Alvey Vision Conference, Manchester,\r\nUK, 31 August–2 September 1988; pp. 23.1–23.6. 29.Lucas, B.D.; Kanade, T. An Iterative Image Registration Technique with an Application to Stereo Vision. In Proceedings of the 7th\r\nInternational Joint Conference on Artificial Intelligence (IJCAI), Vancouver, BC, Canada, 24–28 August 1981; pp. 674–679. 30.Shi, J.; Tomasi. Good Features to Track. In Proceedings of the 1994 IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR), Seattle, WA, USA, 21–23 June 1994; pp. 593–600. 31. Lowe, D.G. Distinctive Image Features from Scale-Invariant Keypoints.Int. J. Comput. Vis.2004,60, 91–110. [CrossRef]\r\n32.Bay, H.; Tuytelaars, T.; Van Gool, L. SURF: Speeded Up Robust Features. In Proceedings of the 9th European Conference on\r\nComputer Vision (ECCV), Graz, Austria, 7–13 May 2006; pp. 404–417. 33.Nguyen, T.; Park, E.A.; Han, J.; Park, D.C.; Min, S.Y. Object Detection Using Scale Invariant Feature Transform. InIn Genetic and\r\nEvolutionary Computing; Springer: Berlin/Heidelberg, Germany, 2014; pp. 65–72. 34.Zhou, H.; Yuan, Y.; Shi, C. Object Tracking Using SIFT Features and Mean Shift.Comput. Vis. Image Underst.2009,113, 345–352. [CrossRef]\r\n35.Dalal, N.; Triggs, B. Histograms of Oriented Gradients for Human Detection. In Proceedings of the 2005 IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR), San Diego, CA, USA, 20–25 June 2005; pp. 886–893.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HrTLwgcRiYJfnY18Ts8cv3KdwdFxVmCrBHo18OSeL/M="},"10d35630-e735-4674-bd5d-15664ff4a4f3":{"id_":"10d35630-e735-4674-bd5d-15664ff4a4f3","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"g2XiYlHKBPLi/pYzSA3Ta1HPhACPAnEvD3fBziB3Gwc="},"PREVIOUS":{"nodeId":"7a875d8d-2d1c-4d33-8d82-7817f50f700b","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"HrTLwgcRiYJfnY18Ts8cv3KdwdFxVmCrBHo18OSeL/M="}},"text":"886–893. 36.Mizuno, K.; Terachi, Y.; Takagi, K.; Izumi, S.; Kawaguchi, H.; Yoshimoto, M. Architectural Study of HOG Feature Extraction\r\nProcessor for Real-Time Object Detection. In Proceedings of the IEEE Workshop on Signal Processing Systems, Quebec City, QC,\r\nCanada, 17–19 October 2012; pp. 197–202. 37. Sun, Z.; Bebis, G.; Miller, R. On-Road Vehicle Detection: A Review.IEEE Trans. Pattern Anal. Mach. Intell.2006,28, 694–711. 38.Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going Deeper with\r\nConvolutions. In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA,\r\nUSA, 7–12 June 2015; pp. 1–9. 39.Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings of the 3rd\r\nInternational Conference on Learning Representations (ICLR), San Diego, CA, USA, 7–9 May 2015; pp. 1–14. 40.He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the 2016 IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27–30 June 2016; pp. 3350–3355. 41.Hinton, G.E.; Osindero, S.; Teh, Y.W. A Fast Learning Algorithm for Deep Belief Nets.Neural Comput.2006,18, 1527–1554. [CrossRef]","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ua/pQRZS7G0Z7E8yGaymXI0c9z30yNmm9G5oVvziwNc="},"f6a77c52-d461-4503-887d-34cc42841072":{"id_":"f6a77c52-d461-4503-887d-34cc42841072","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"YyNIPS6PwGzXBc1tvgoE0LBwz0FtsOmagGrCvZL6s+k="},"NEXT":{"nodeId":"a2252d5b-5f83-44f2-b2c3-fdf95e7f8d3d","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"2XilwafLFNCKPSzRd8BRqMyXiLnGGv331ezo0TZpJBA="}},"text":"Electronics2021,10, 27926 of 28\r\n42.Hinton, G.E.; Salakhutdinov, R.R. Reducing the Dimensionality of Data with Neural Networks.Science2006,313, 504–507. [CrossRef]\r\n43.Zoph, B.; Cubuk, E.D.; Ghiasi, G.; Lin, T.Y.; Shlens, J.; Le, Q.V. Learning Data Augmentation Strategies for Object Detection. In Proceedings of the 16th European Conference on Computer Vision (ECCV), Glasgow, UK, 23–28 August 2020; pp. 566–583. 44.Loey, M.; Manogaran, G.; Khalifa, N.E.M. A Deep Transfer Learning Model with Classical Data Augmentation and CGAN to\r\nDetect COVID-19 from Chest CT Radiography Digital Images.Neural Comput. Appl.2020, 1–13. [CrossRef] [PubMed]\r\n45.González, R.E.; Munoz, R.P.; Hernández, C.A. Galaxy Detection and Identification Using Deep Learning and Data Augmentation. Astron. Comput.2018,25, 103–109. [CrossRef]\r\n46.Sermanet, P.; Eigen, D.; Zhang, X.; Mathieu, M.; Fergus, R.; LeCun, Y. OverFeat:  Integrated Recognition, Localization and\r\nDetection using Convolutional Networks. In Proceedings of the 2nd International Conference on Learning Representations\r\n(ICLR), Banff, AB, Canada, 14–16 April 2014; pp. 1–16. 47.Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single Shot MultiBox Detector. In Proceedings of\r\nthe 14th European Conference on Computer Vision (ECCV), Amsterdam, The Netherlands, 11–14 October 2016; pp. 21–37. 48.Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"MdUxiiNfl6ERfcfE0hA0cDIwzH/VTIso7mTaa1ekexU="},"a2252d5b-5f83-44f2-b2c3-fdf95e7f8d3d":{"id_":"a2252d5b-5f83-44f2-b2c3-fdf95e7f8d3d","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"YyNIPS6PwGzXBc1tvgoE0LBwz0FtsOmagGrCvZL6s+k="},"PREVIOUS":{"nodeId":"f6a77c52-d461-4503-887d-34cc42841072","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"MdUxiiNfl6ERfcfE0hA0cDIwzH/VTIso7mTaa1ekexU="},"NEXT":{"nodeId":"0b4291ec-c915-41ed-a6d6-c16d355f02b8","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"VPheyy9O2eJHEZV0qSez0D54d+YhS2K729wSK2GO5LE="}},"text":"You Only Look Once: Unified, Real-Time Object Detection. In Proceedings of the\r\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27–30 June 2016; pp. 779–788. 49.Redmon, J.; Farhadi, A. YOLO9000: Better, Faster, Stronger. In Proceedings of the 2017 IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 6517–6525. 50. Redmon, J.; Farhadi, A. YOLOv3: An Incremental Improvement.arXiv2018, arXiv:1804.02767. 51.Bochkovskiy,  A.;  Wang,  C.Y.;  Liao,  H.Y.M. YOLOv4:   Optimal  Speed  and  Accuracy  of  Object  Detection.arXiv2020\r\narXiv:2004.10934. 52.Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, OH, USA, 23–28\r\nJune 2014; p. 587. 53.Girshick, R. Fast R-CNN. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile,\r\n7–13 December 2015; pp. 1440–1448. 54.Dai, J.; Li, Y.; He, K.; Sun, J. R-FCN: Object Detection Via Region-Based Fully Convolutional Networks. In Proceedings of the\r\n30th International Conference on Neural Information Processing Systems (NeurIPS), Barcelona, Spain, 5–10 December 2016;\r\npp. 379–387. 55.Gu, J.; Hu, H.; Wang, L.; Wei, Y.; Dai, J. Learning Region Features for Object Detection. In Proceedings of the 15th European\r\nConference on Computer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"2XilwafLFNCKPSzRd8BRqMyXiLnGGv331ezo0TZpJBA="},"0b4291ec-c915-41ed-a6d6-c16d355f02b8":{"id_":"0b4291ec-c915-41ed-a6d6-c16d355f02b8","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"YyNIPS6PwGzXBc1tvgoE0LBwz0FtsOmagGrCvZL6s+k="},"PREVIOUS":{"nodeId":"a2252d5b-5f83-44f2-b2c3-fdf95e7f8d3d","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"2XilwafLFNCKPSzRd8BRqMyXiLnGGv331ezo0TZpJBA="},"NEXT":{"nodeId":"b474b3fd-7ecf-4f15-a6e5-078c2e64b9d9","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"53rrNdJLFJkG5ld2yc/Q2N3VWCDRzTJIaAZnz1doCks="}},"text":"392–406. 56.Hu, H.; Gu, J.; Zhang, Z.; Dai, J.; Wei, Y. Relation Networks for Object Detection. In Proceedings of the 2018 IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18–22 June 2018; pp. 3588–3597. 57.Połap, D. An Adaptive Genetic Algorithm as a Supporting Mechanism for Microscopy Image Analysis in a Cascade of Convolution\r\nNeural Networks.Appl. Soft Comput. J.2020,97, 1–11. [CrossRef]\r\n58.S, I.T.J.; Sasikala, J.; Juliet, D.S. Optimized Vessel Detection in Marine Environment Using Hybrid Adaptive Cuckoo Search\r\nAlgorithm.Comput. Electr. Eng.2019,78, 482–492. [CrossRef]\r\n59.Li, Y.; Wang, H.; Dang, L.M.; Nguyen, T.N.; Han, D.; Lee, A.; Jang, I.; Moon, H. A Deep Learning-based Hybrid Framework for\r\nObject Detection and Recognition in Autonomous Driving.IEEE Access2020,8, 194228–194239. [CrossRef]\r\n60. Chen, Y.; Zhou, W. Hybrid-Attention Network for RGB-D Salient Object Detection.Appl. Sci.2020,10, 5806. [CrossRef]\r\n61.Zhang, P.; Liu, W.; Lei, Y.; Lu, H. Hyperfusion-Net: Hyper-Densely Reflective Feature Fusion for Salient Object Detection.Pattern\r\nRecognit.2019,93, 521–533. [CrossRef]\r\n62.Litjens, G.; Kooi, T.; Bejnordi, B.E.; Setio, A.A.A.; Ciompi, F.; Ghafoorian, M.; Van Der Laak, J.A.; Van Ginneken, B.; Sánchez, C.I. A Survey on Deep Learning in Medical Image Analysis.Med. Image Anal.2017,42, 60–88. [CrossRef]\r\n63.Cao, Z.; Duan, L.; Yang, G.; Yue, T.; Chen, Q.; Fu, H.; Xu, Y.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VPheyy9O2eJHEZV0qSez0D54d+YhS2K729wSK2GO5LE="},"b474b3fd-7ecf-4f15-a6e5-078c2e64b9d9":{"id_":"b474b3fd-7ecf-4f15-a6e5-078c2e64b9d9","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"YyNIPS6PwGzXBc1tvgoE0LBwz0FtsOmagGrCvZL6s+k="},"PREVIOUS":{"nodeId":"0b4291ec-c915-41ed-a6d6-c16d355f02b8","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"VPheyy9O2eJHEZV0qSez0D54d+YhS2K729wSK2GO5LE="}},"text":"Breast Tumor Detection in Ultrasound Images Using Deep Learning. InInternational Workshop on Patch-Based Techniques in Medical Imaging; Springer: Cham, Switzerland, 2017; pp. 121–128. 64.Jaeger, P.F.; Kohl, S.A.; Bickelhaupt, S.; Isensee, F.; Kuder, T.A.; Schlemmer, H.P.; Maier-Hein, K.H. Retina U-Net: Embarrassingly\r\nSimple Exploitation of Segmentation Supervision for Medical Object Detection. In Proceedings of Machine Learning for Health\r\nWorkshop (ML4H), Vancouver, BC, Canada, 13–14 December 2020; pp. 171–183. 65.Li, Z.; Dong, M.; Wen, S.; Hu, X.; Zhou, P.; Zeng, Z. CLU-CNNs: Object detection for Medical Images.Neurocomputing2019,\r\n350, 53–59. [CrossRef]\r\n66.Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollár, P.; Zitnick, C.L. Microsoft COCO: Common Objects in\r\nContext. In Proceedings of the 13th European Conference on Computer Vision (ECCV), Zurich, Switzerland, 6–12 September\r\n2014; pp. 740–755. 67.Everingham, M.; Van Gool, L.; Williams, C.K.I.; Winn, J.; Zisserman, A. The PASCAL Visual Object Classes Challenge 2012\r\n(VOC2012) Results. Available online:  http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html\r\n(accessed on 20 December 2020). 68.Kuznetsova, A.; Rom, H.; Alldrin, N.; Uijlings, J.; Krasin, I.; Pont-Tuset, J.; Kamali, S.; Popov, S.; Malloci, M.; Kolesnikov, A.; et al. The Open Images Dataset V4: Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale.Int. J. Comput. Vis.2020,128, 1956–1981. [CrossRef]","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"53rrNdJLFJkG5ld2yc/Q2N3VWCDRzTJIaAZnz1doCks="},"b4559018-f716-4022-83fb-7040c514d999":{"id_":"b4559018-f716-4022-83fb-7040c514d999","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ZLY+fFPyNhf97VL7md4xZV3n4azkfE2rgS0QRUlfwnk="},"NEXT":{"nodeId":"1dd9e963-85b4-46f2-9670-de006c51fa41","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"HfYdXgUVHGZpDhhf4mfkVkghCS5da7qHUEcbjX/lCHQ="}},"text":"Electronics2021,10, 27927 of 28\r\n69.Wada, K. labelme: Image Polygonal Annotation with Python. 2016. Available online: https://github.com/wkentaro/labelme\r\n(accessed on 20 December 2020). 70. Lin, T. LabelImg. 2015. Available online: https://github.com/tzutalin/labelImg (accessed on 20 December 2020). 71.Wada, K. VoTT: Visual Object Tagging Tool. Available online: https://github.com/Microsoft/VoTT (accessed on 20 December\r\n2020). 72.Sekachev, B.; Manovich, N.; Zhiltsov, M.; Zhavoronkov, A.; Kalinin, D. opencv/cvat v1.1.0. 2020. Available online:  http:\r\n//doi.org/10.5281/zenodo.4009388 (accessed on 20 December 2020). 73.Dutta,  A.;  Zisserman,  A. The VIA Annotation Software for Images,  Audio and Video. In Proceedings of the 27th ACM\r\nInternational Conference on Multimedia, Nice, France, 21–25 October 2019; pp. 2276–2279. 74.Deng, J.; Dong, W.; Socher, R.; Li, L.J.; Li, K.; Li, F.F. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the\r\n2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Miami, FL, USA, 20–25 June 2009; pp. 248–255. 75.Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. TensorFlow:\r\nLarge-Scale Machine Learning on Heterogeneous Systems. 2015. Available online: https://www.tensorflow.org/ (accessed on 20\r\nDecember 2020). 76.Law, H.; Deng, J. Cornernet:  Detecting Objects as Paired Keypoints.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"7UI/4fIm8bhJOSgAfeXCFX2OiDxRboLw8FJFDkfWry8="},"1dd9e963-85b4-46f2-9670-de006c51fa41":{"id_":"1dd9e963-85b4-46f2-9670-de006c51fa41","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ZLY+fFPyNhf97VL7md4xZV3n4azkfE2rgS0QRUlfwnk="},"PREVIOUS":{"nodeId":"b4559018-f716-4022-83fb-7040c514d999","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"7UI/4fIm8bhJOSgAfeXCFX2OiDxRboLw8FJFDkfWry8="},"NEXT":{"nodeId":"60a51ace-3037-49a3-8fec-5dac1544c633","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"TVUWZOK0O2pG6yb9tjpoTgkFVolInxTLR4br/26ItwM="}},"text":"Cornernet:  Detecting Objects as Paired Keypoints. In Proceedings of the 15th European Conference on\r\nComputer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 734–750. 77.Tan, M.; Pang, R.; Le, Q.V. EfficientDet: Scalable and Efficient Object Detection. In Proceedings of the 2020 IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 14–19 June 2020; pp. 10781–10790. 78.Liu, S.; Huang, D. Receptive Field Block Net for Accurate and Fast Object Detection. In Proceedings of the 15th European\r\nConference on Computer Vision (ECCV), Munich, Germany, 8–14 September 2018; pp. 385–400. 79.Zhang, S.; Wen, L.; Bian, X.; Lei, Z.; Li, S.Z. Single-Shot Refinement Neural Network for Object Detection. In Proceedings\r\nof the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18–22 June 2018;\r\npp. 4203–4212. 80.Lin, T.Y.; Goyal, P.; Girshick, R.; He, K.; Dollár, P. Focal Loss for Dense Object Detection. In Proceedings of the 2017 IEEE\r\nInternational Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017; pp. 2980–2988. 81.Open Images Object Detection RVC 2020 Edition. Available online: https://www.kaggle.com/c/open-images-object-detection-\r\nrvc-2020 (accessed on 20 December 2020). 82.COCO  Detection  Challenge  (Bounding  Box). Available  online:   https://competitions.codalab.org/competitions/20794\r\n(accessed on 20 December 2020). 83.Datalab  Cup:   CNN  Object  Detection. Available  online:   https://www.kaggle.com/c/datalabcup-cnn-object-detection\r\n(accessed on 20 December 2020). 84. Google AI Open Images-Object Detection Track.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HfYdXgUVHGZpDhhf4mfkVkghCS5da7qHUEcbjX/lCHQ="},"60a51ace-3037-49a3-8fec-5dac1544c633":{"id_":"60a51ace-3037-49a3-8fec-5dac1544c633","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ZLY+fFPyNhf97VL7md4xZV3n4azkfE2rgS0QRUlfwnk="},"PREVIOUS":{"nodeId":"1dd9e963-85b4-46f2-9670-de006c51fa41","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"HfYdXgUVHGZpDhhf4mfkVkghCS5da7qHUEcbjX/lCHQ="},"NEXT":{"nodeId":"54e0a88e-005a-48d1-9612-ac2192b38035","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"hnV/Pt1mfVZ+oXYwYrIoc8kGCo4R/nIlg618m9TkbAI="}},"text":"84. Google AI Open Images-Object Detection Track. Available online: https://www.kaggle.com/c/google-ai-open-images-object-\r\ndetection-track (accessed on 20 December 2020). 85.Lyft 3D Object Detection for Autonomous Vehicles. Available online:  https://www.kaggle.com/c/3d-object-detection-for-\r\nautonomous-vehicles/ (accessed on 20 December 2020). . 86. City Intelligence Hackathon. Available online: https://belvisionhack.ru (accessed on 20 December 2020). 87.Dudczyk, J.; Kawalec, A. Identification of Emitter Sources in the Aspect of Their Fractal Features.Bull. Pol. Acad. Sci. Tech. Sci. Tech. Sci.2013,61, 623–628. [CrossRef]\r\n88.Rybak, Ł.; Dudczyk, J. A Geometrical Divide of Data Particle in Gravitational Classification of Moons and Circles Data Sets. Entropy2020,22, 1088–1103. [CrossRef] [PubMed]\r\n89.Jaccard, P. Étude Comparative de la Distribution Florale Dans Une Portion des Alpes et des Jura.Bull. Soc. Vaudoise Des Sci. Nat. 1901,37, 547–579. 90.Hosang, J.; Benenson, R.; Dollár, P.; Schiele, B. What Makes for Effective Detection Proposals?IEEE Trans. Pattern Anal. Mach. Intell.2015,38, 814–830. [CrossRef]\r\n91.Oksuz, K.; Can Cam, B.; Akbas, E.; Kalkan, S. Localization Recall Precision (LRP): A New Performance Metric for Object\r\nDetection. In Proceedings of the 15th European Conference on Computer Vision (ECCV), Munich, Germany, 8–14 September\r\n2018; pp. 504–519. 92. Zhu, X.; Wang, Y.; Dai, J.; Yuan, L.; Wei, Y. Flow-Guided Feature Aggregation for Video Object Detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"TVUWZOK0O2pG6yb9tjpoTgkFVolInxTLR4br/26ItwM="},"54e0a88e-005a-48d1-9612-ac2192b38035":{"id_":"54e0a88e-005a-48d1-9612-ac2192b38035","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"ZLY+fFPyNhf97VL7md4xZV3n4azkfE2rgS0QRUlfwnk="},"PREVIOUS":{"nodeId":"60a51ace-3037-49a3-8fec-5dac1544c633","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"TVUWZOK0O2pG6yb9tjpoTgkFVolInxTLR4br/26ItwM="}},"text":"Flow-Guided Feature Aggregation for Video Object Detection. In Proceedings of the\r\n2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 22–29 October 2017; pp. 408–417. 93.Zhu, X.; Xiong, Y.; Dai, J.; Yuan, L.; Wei, Y. Deep Feature Flow for Video Recognition. In Proceedings of the 2017 IEEE Conference\r\non Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 4141–4150. 94.Zhu, M.; Liu, M. Mobile Video Object Detection with Temporally-Aware Feature Maps. In Proceedings of the 2018 IEEE\r\nConference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, USA, 18–22 June 2018; pp. 5686–5695. 95.Zhang, C.; Kim, J. Modeling Long- and Short-Term Temporal Context for Video Object Detection. In Proceedings of the 26th\r\nIEEE International Conference on Image Processing (ICIP), Tapei, Taiwan, 22–25 Spetember 2019; pp. 71–75. 96.Deng, H.; Hua, Y.; Song, T.; Zhang, Z.; Xue, Z.; Ma, R.; Robertson, N.; Guan, H. Object Guided External Memory Network for\r\nVideo Object Detection. In Proceedings of the 2019 IEEE International Conference on Computer Vision (ICCV), Seoul, Korea, 27\r\nOctober–2 November 2019; pp. 6678–6687.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"hnV/Pt1mfVZ+oXYwYrIoc8kGCo4R/nIlg618m9TkbAI="},"a3430db8-cf59-4355-b484-e5e6ffe58aea":{"id_":"a3430db8-cf59-4355-b484-e5e6ffe58aea","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf","file_name":"A_Comparative_Analysis_of_Object_Detection_Metrics_with_a_Companion_Open-Source_Toolkit.pdf"},"hash":"JJfIsG5QVmPJjrtNUxQ6x0spu7uoEL/+5uY78No/moc="}},"text":"Electronics2021,10, 27928 of 28\r\n97.Beery,  S.;  Wu,  G.;  Rathod,  V.;  Votel,  R.;  Huang,  J. Context R-CNN: Long Term Temporal Context for Per-Camera Object\r\nDetection. In Proceedings of the 2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA,\r\n14–19 June 2020; pp. 13072–13082. 98.Chen, Y.; Cao, Y.; Wang, L. Memory Enhanced Global-Local Aggregation for Video Object Detection. In Proceedings of the 2020\r\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 14–19 June 2020; pp. 10337–10346. 99.Mao, H.; Yang, X.; Dally, W.J. A Delay Metric for Video Object Detection: What Average Precision Fails to Tell. In Proceedings of\r\nthe 2019 IEEE International Conference on Computer Vision (ICCV), Seoul, Korea, 27 October–2 November 2019; pp. 573–582. 100.Padilla, R.; Netto, S.L.; da Silva, E.A.B. Metrics for Object Detection. Available online: https://github.com/rafaelpadilla/Object-\r\nDetection-Metrics (accessed on 20 December 2020). 101.Computer Research Institute of Montreal (CRIM). thelper Package. Available online: https://thelper.readthedocs.io/en/latest/\r\nthelper.optim.html (accessed on 20 December 2020). 102.Padilla, R.; Passos, W.L.; Dias, T.L.B.; Netto, S.L.; da Silva, E.A.B. Evaluation Tool for Object Detecttion Metrics. Available online:\r\nhttps://github.com/rafaelpadilla/review_object_detection_metrics (accessed on 20 December 2020).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VCTsZCOnDSCzsdMGX/4rXOtRJNb4AQGwO/wYwzMq4hk="},"752d9428-0af1-4930-a920-42ae1e77187f":{"id_":"752d9428-0af1-4930-a920-42ae1e77187f","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"iR+6yv5TEANq4kjh2fnWh/J+4J3OAYy9U/rIS+jdXmI="},"NEXT":{"nodeId":"a304fa33-450d-4bee-970c-e6efc4de2dda","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"VIgqxn7dN6knE3DqpyUulsSChGfYNHaJmjGNRT81QiY="}},"text":"A Survey on Performance Metrics for\r\nObject-Detection Algorithms\r\nRafael Padilla1, Sergio L. Netto2, Eduardo A. B. da Silva3\r\n1,2,3PEE, COPPE, Federal University of Rio de Janeiro, P.O. Box 68504, RJ, 21945-970, Brazil\r\nfrafael.padilla, sergioln,eduardog@smt.ufrj.br\r\nAbstract—This  work  explores  and  compares  the  plethora  of\r\nmetrics  for  the  performance  evaluation  of  object-detection  algo-\r\nrithms. Average  precision  (AP),  for  instance,  is  a  popular  metric\r\nfor  evaluating  the  accuracy  of  object  detectors  by  estimating  the\r\narea under the curve (AUC) of the precision\u0002recall relationship. Depending   on   the   point   interpolation   used   in   the   plot,   two\r\ndifferent  AP  variants  can  be  defined  and,  therefore,  different\r\nresults are generated. AP has six additional variants increasing the\r\npossibilities  of  benchmarking. The  lack  of  consensus  in  different\r\nworks and AP implementations is a problem faced by the academic\r\nand  scientific  communities. Metric  implementations  written  in\r\ndifferent   computational   languages   and   platforms   are   usually\r\ndistributed with corresponding datasets sharing a given bounding-\r\nbox  description. Such  projects  indeed  help  the  community  with\r\nevaluation tools, but demand extra work to be adapted for other\r\ndatasets  and  bounding-box  formats. This  work  reviews  the  most\r\nused   metrics   for   object   detection   detaching   their   differences,\r\napplications,  and  main  concepts. It  also  proposes  a  standard\r\nimplementation that can be used as a benchmark among different\r\ndatasets with minimum adaptation on the annotation files. Keywords—object-detection  metrics,  average  precision,  object-\r\ndetection challenges, bounding boxes. I. INTRODUCTION\r\nObject  detection  is  an  extensively  studied  topic  in  the  field\r\nof computer vision.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jec2dAwvV5kysPFdGKDqj2vYz91A9KK9i80Sj4lQyww="},"a304fa33-450d-4bee-970c-e6efc4de2dda":{"id_":"a304fa33-450d-4bee-970c-e6efc4de2dda","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"iR+6yv5TEANq4kjh2fnWh/J+4J3OAYy9U/rIS+jdXmI="},"PREVIOUS":{"nodeId":"752d9428-0af1-4930-a920-42ae1e77187f","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"jec2dAwvV5kysPFdGKDqj2vYz91A9KK9i80Sj4lQyww="},"NEXT":{"nodeId":"819d4a64-4a28-43e3-93ff-620b50cd0f73","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"WbP5CYXrby7wb3TKq5oJb7sRgVSjzCKisgVaayFw4Wk="}},"text":"Different approaches have been employed\r\nto  solve  the  growing  need  for  accurate  object  detection  mod-\r\nels  [1]. The  Viola-Jones  framework  [2],  for  instance,  became\r\npopular  due  to  its  successful  application  in  the  face-detection\r\nproblem [3], and was later applied to different subtasks such as\r\npedestrian  [4]  and  car  [5]  detections. More  recently,  with  the\r\npopularization of the convolutional neural networks (CNN) [6]–\r\n[9]  and  GPU-accelerated  deep-learning  frameworks,  object-\r\ndetection  algorithms  started  being  developed  from  a  new  per-\r\nspective [10], [11]. Works as Overfeat [12], R-CNN [13], Fast\r\nR-CNN [14], Faster R-CNN [15], R-FCN [16], SSD [17] and\r\nYOLO  [18]–[20]  highly  increased  the  performance  stantards\r\non  the  field. World  famous  competitions  such  as  VOC  PAS-\r\nCAL Challenge [21], COCO [22], ImageNet Object Detection\r\nChallenge [23], and Google Open Images Challenge [24] have\r\nas  their  top  object-detection  algorithms  methods  inspired  on\r\nthe aforementioned works. Differently from algorithms such as\r\nthe Viola-Jones, CNN-based detectors are flexible enough to be\r\ntrained with several (hundreds or even a few thousands) classes. A  detector  outcome  is  commonly  composed  of  a  list  of\r\nbounding  boxes,  confidence  levels  and  classes,  as  seen  in\r\nFigure  1. However,  the  standard  output-file  format  varies  a\r\nlot for different detection algorithms. Bounding-box detections\r\n(a)(b)\r\n(c)\r\nFig. 1:  Examples  of  detections  performed  by  YOLO  [20]  in\r\ndifferent datasets. (a) PASCAL VOC; (b) personal dataset; (c)\r\nCOCO.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VIgqxn7dN6knE3DqpyUulsSChGfYNHaJmjGNRT81QiY="},"819d4a64-4a28-43e3-93ff-620b50cd0f73":{"id_":"819d4a64-4a28-43e3-93ff-620b50cd0f73","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"iR+6yv5TEANq4kjh2fnWh/J+4J3OAYy9U/rIS+jdXmI="},"PREVIOUS":{"nodeId":"a304fa33-450d-4bee-970c-e6efc4de2dda","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"VIgqxn7dN6knE3DqpyUulsSChGfYNHaJmjGNRT81QiY="}},"text":"(a) PASCAL VOC; (b) personal dataset; (c)\r\nCOCO. Besides  the  bounding  box  coordinates  of  a  detected\r\nobject,  the  output  also  includes  the  confidence  level  and  its\r\nclass. are  mostly  represented  by  their  top-left  and  bottom-right  co-\r\nordinates  (xini; yini; xend; yend),  with  a  notable  exception  being\r\nthe YOLO [18]–[20] algorithm, that differs from the others by\r\noutlining the bounding boxes by their center coordinates, width,\r\nand height\u0000xcenterimage width;ycenterimage height;box widthimage width;box heightimage height\u0001. Different   challenges,   competitions,   and   hackathons   [21],\r\n[23]–[27]  attempt  to  assess  the  performance  of  object  de-\r\ntections  in  specific  scenarios  by  using  real-world  annotated\r\nimages  [28]–[30]. In  these  events,  participants  are  given  a\r\ntesting  nonannotated  image  set  in  which  objects  have  to  be\r\ndetected by their proposed works. Some competitions provide\r\ntheir own (or 3rd-party) source code, allowing the participants\r\nto  evaluate  their  algorithms  in  an  annotated  validation  image\r\nset  before  submitting  their  testing-set  detections. In  the  end,","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WbP5CYXrby7wb3TKq5oJb7sRgVSjzCKisgVaayFw4Wk="},"3138dd58-5dcd-452f-8d14-f7245a1e4bb8":{"id_":"3138dd58-5dcd-452f-8d14-f7245a1e4bb8","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"QbiXJTtvgYZQEMYAuIM1AUXBu9lNdmj9Ibe23lqQcd8="},"NEXT":{"nodeId":"f0155b57-62c1-43a8-a307-5878b30647e6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"in5eskZbXOupYE2VPd2FGXZQZiD00/YJ2As5LHWuY2s="}},"text":"each team sends a list of bounding-boxes coordinates with their\r\nrespective classes and (sometimes) their confidence levels to be\r\nevaluated. In  most  competitions,  the  average  precision  (AP)  and  its\r\nderivations  are  the  metrics  adopted  to  assess  the  detections\r\nand thus rank the teams. The PASCAL VOC dataset [31] and\r\nchallenge  [21]  provide  their  own  source  code  to  measure  the\r\nAP and the mean AP (mAP) over all object classes. The City\r\nIntelligence  Hackathon  [27]  uses  the  source  code  distributed\r\nin [32] to rank the participants also on AP and mAP. The Ima-\r\ngeNet Object Localization challenge [23] does not recommend\r\nany  code  to  compute  their  evaluation  metric,  but  provides  a\r\npseudo-code  explaining  it. The  Open  Images  2019  [24]  and\r\nGoogle AI Open Images [26] challenges use mAP, referencing\r\na  tool  to  evaluate  the  results  [33],  [34]. The  Lyft  3D  Object\r\nDetection  for  Autonomous  Vehicles  challenge  [25]  does  not\r\nreference any external tool, but uses the AP averaged over 10\r\ndifferent thresholds, the so-called AP@50:5:95 metric. This  work  reviews  the  most  popular  metrics  used  to  evalu-\r\nate object-detection algorithms, including their main concepts,\r\npointing out their differences, and establishing a comparison be-\r\ntween different implementations. In order to introduce its main\r\ncontributions,  this  work  is  divided  into  the  following  topics:\r\nSection  II  explains  the  main  performance  metrics  employed\r\nin  the  field  of  object  detection  and  how  the  AP  metric  can\r\nproduce  ambiguous  results;  Section  III  describes  some  of  the\r\nmost  known  object  detection  challenges  and  their  employed\r\nperformance  metrics,  whereas  Section  IV  presents  a  project\r\nimplementing  the  AP  metric  to  be  used  with  any  annotation\r\nformat. II.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FGEhKl2iCEi9DnG20DvsI7noa+Bedd9kwd+GHmuZ97M="},"f0155b57-62c1-43a8-a307-5878b30647e6":{"id_":"f0155b57-62c1-43a8-a307-5878b30647e6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"QbiXJTtvgYZQEMYAuIM1AUXBu9lNdmj9Ibe23lqQcd8="},"PREVIOUS":{"nodeId":"3138dd58-5dcd-452f-8d14-f7245a1e4bb8","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"FGEhKl2iCEi9DnG20DvsI7noa+Bedd9kwd+GHmuZ97M="},"NEXT":{"nodeId":"83664d64-8233-4c13-83f8-94c656ac6c9e","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"M+y+MpZG2l8D2aEk8Tzqz1MEexvFDJlGeXvGnotYyoQ="}},"text":"II. MAINPERFORMANCEMETRICS\r\nAmong different annotated datasets used by object detection\r\nchallenges  and  the  scientific  community,  the  most  common\r\nmetric used to measure the accuracy of the detections is the AP. Before  examining  the  variations  of  the  AP,  we  should  review\r\nsome concepts that are shared among them. The most basic are\r\nthe ones defined below:\r\n\u000fTrue positive (TP): A correct detection of a ground-truthbounding box;\r\n\u000fFalse positive (FP): An incorrect detection of a nonexistentobject or a misplaced detection of an existing object;\r\n\u000fFalse negative (FN): An undetected ground-truth boundingbox;\r\nIt  is  important  to  note  that,  in  the  object  detection  context,\r\na true negative (TN) result does not apply, as there are infinite\r\nnumber of bounding boxes that should not be detected within\r\nany given image. The  above  definitions  require  the  establishment  of  what  a\r\n“correct detection” and an “incorrect detection” are. A common\r\nway  to  do  so  is  using  the  intersection  over  union  (IOU). It  is\r\na  measurement  based  on  the  Jaccard  Index,  a  coefficient  of\r\nsimilarity  for  two  sets  of  data  [35]. In  the  object  detection\r\nscope,  the  IOU  measures  the  overlapping  area  between  the\r\npredicted bounding boxBpand the ground-truth bounding box\r\nBgtdivided by the area of union between them, that is\r\nJ(Bp; Bgt) = IOU =area(Bp\\Bgt)area(B\r\np[Bgt)\r\n;(1)\r\nas illustrated in Figure 2. Fig. 2: Intersection Over Union (IOU). By  comparing  the  IOU  with  a  given  thresholdt,  we  can\r\nclassify  a  detection  as  being  correct  or  incorrect. IfIOU\u0015t\r\nthen  the  detection  is  considered  as  correct. IfIOU< tthe\r\ndetection is considered as incorrect.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"in5eskZbXOupYE2VPd2FGXZQZiD00/YJ2As5LHWuY2s="},"83664d64-8233-4c13-83f8-94c656ac6c9e":{"id_":"83664d64-8233-4c13-83f8-94c656ac6c9e","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"QbiXJTtvgYZQEMYAuIM1AUXBu9lNdmj9Ibe23lqQcd8="},"PREVIOUS":{"nodeId":"f0155b57-62c1-43a8-a307-5878b30647e6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"in5eskZbXOupYE2VPd2FGXZQZiD00/YJ2As5LHWuY2s="},"NEXT":{"nodeId":"b8b9188b-3e6d-47dc-8c74-6f9f322414b8","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"ZjmRJtAIIa9MwHH2ZsE+CEcbCv1sxlZo3kUFs4S29kQ="}},"text":"IfIOU< tthe\r\ndetection is considered as incorrect. Since, as stated above, the true negatives (TN) are not used in\r\nobject detection frameworks, one refrains to use any metric that\r\nis based on the TN, such as the TPR, FPR and ROC curves [36]. Instead,  the  assessment  of  object  detection  methods  is  mostly\r\nbased  on  the  precisionPand  recallRconcepts,  respectively\r\ndefined as\r\nP=TPTP+FP=TPall detections;(2)\r\nR=TPTP+FN=TPall ground truths:(3)\r\nPrecision  is  the  ability  of  a  model  to  identify  only  relevant\r\nobjects. It  is  the  percentage  of  correct  positive  predictions. Recall  is  the  ability  of  a  model  to  find  all  relevant  cases  (all\r\nground-truth  bounding  boxes). It  is  the  percentage  of  correct\r\npositive predictions among all given ground truths. The  precision\u0002recall  curve  can  be  seen  as  a  trade-off\r\nbetween  precision  and  recall  for  different  confidence  values\r\nassociated to the bounding boxes generated by a detector. If the\r\nconfidence of a detector is such that its FP is low, the precision\r\nwill  be  high. However,  in  this  case,  many  positives  may  be\r\nmissed, yielding a high FN, and thus a low recall. Conversely,\r\nif one accepts more positives, the recall will increase, but the FP\r\nmay  also  increase,  decreasing  the  precision. However,  a  good\r\nobject detector should find all ground-truth objects (F N= 0\u0011\r\nhigh recall) while identifying only relevant objects (F P= 0\u0011\r\nhigh  precision). Therefore,  a  particular  object  detector  can\r\nbe  considered  good  if  its  precision  stays  high  as  its  recall\r\nincreases, which means that if the confidence threshold varies,\r\nthe  precision  and  recall  will  still  be  high.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"M+y+MpZG2l8D2aEk8Tzqz1MEexvFDJlGeXvGnotYyoQ="},"b8b9188b-3e6d-47dc-8c74-6f9f322414b8":{"id_":"b8b9188b-3e6d-47dc-8c74-6f9f322414b8","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"QbiXJTtvgYZQEMYAuIM1AUXBu9lNdmj9Ibe23lqQcd8="},"PREVIOUS":{"nodeId":"83664d64-8233-4c13-83f8-94c656ac6c9e","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"M+y+MpZG2l8D2aEk8Tzqz1MEexvFDJlGeXvGnotYyoQ="}},"text":"Hence,  a  high  area\r\nunder  the  curve  (AUC)  tends  to  indicate  both  high  precision\r\nand high recall. Unfortunately, in practical cases, the precision\r\n\u0002recall plot is often a zigzag-like curve, posing challenges to\r\nan accurate measurement of its AUC. This is circumvented by\r\nprocessing the precision\u0002recall curve in order to remove the\r\nzigzag  behavior  prior  to  AUC  estimation. There  are  basically","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZjmRJtAIIa9MwHH2ZsE+CEcbCv1sxlZo3kUFs4S29kQ="},"e1b66fdc-043e-4be8-ab76-e444eb2dd6db":{"id_":"e1b66fdc-043e-4be8-ab76-e444eb2dd6db","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"6CxH1yP+M+u/1dFKuQ7I8PdQKGzU+iiy1u8EfbSFA4s="},"NEXT":{"nodeId":"b22c0dc6-41ae-459f-ade1-00dc1fc46bb9","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"LUzdo4AOTWTo3xZm37uMpBp3zriyiulFSxtZgx9WzYQ="}},"text":"two  approaches  to  do  so:  the  11-point  interpolation  and  all-\r\npoint interpolation. In  the  11-point  interpolation,  the  shape  of  the  precision\r\n\u0002recall  curve  is  summarized  by  averaging  the  maximum\r\nprecision values at a set of 11 equally spaced recall levels [0,\r\n0.1, 0.2, ... , 1], as given by\r\nAP11=111\r\nX\r\nR2f0;0:1;:::;0:9;1g\r\nPinterp(R);(4)\r\nwhere\r\nPinterp(R) = max\r\n~R:~R\u0015R\r\nP(~R):(5)\r\nIn  this  definition  of  AP,  instead  of  using  the  precision\r\nP(R)observed  at  each  recall  levelR,  the  AP  is  obtained\r\nby considering the maximum precisionPinterp(R)whose recall\r\nvalue is greater thanR. In the all-point interpolation, instead of interpolating only 11\r\nequally  spaced  points,  one  may  interpolate  through  all  points\r\nin such way that:\r\nAPall=\r\nX\r\nn\r\n(Rn+1\u0000Rn)Pinterp(Rn+1);(6)\r\nwhere\r\nPinterp(Rn+1) =   max\r\n~R:~R\u0015Rn+1\r\nP(~R):(7)\r\nIn  this  case,  instead  of  using  the  precision  observed  at\r\nonly  few  points,  the  AP  is  now  obtained  by  interpolating  the\r\nprecision  at  each  level,  taking  the  maximum  precision  whose\r\nrecall value is greater or equal thanRn+1. The  mean  AP  (mAP)  is  a  metric  used  to  measure  the\r\naccuracy  of  object  detectors  over  all  classes  in  a  specific\r\ndatabase.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"odN8u0DB02mJHDpoGHAU5ZAF6D1+n3gFxR64wWQ5tsc="},"b22c0dc6-41ae-459f-ade1-00dc1fc46bb9":{"id_":"b22c0dc6-41ae-459f-ade1-00dc1fc46bb9","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"6CxH1yP+M+u/1dFKuQ7I8PdQKGzU+iiy1u8EfbSFA4s="},"PREVIOUS":{"nodeId":"e1b66fdc-043e-4be8-ab76-e444eb2dd6db","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"odN8u0DB02mJHDpoGHAU5ZAF6D1+n3gFxR64wWQ5tsc="},"NEXT":{"nodeId":"010bb000-a217-4807-8ff3-88c21c65ffb5","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"bxcFadHPFmrZiSlReQtgfqmKAowh6JellW3PxrhwRpY="}},"text":"The  mAP  is  simply  the  average  AP  over  all  classes\r\n[15], [17], that is\r\nmAP =1N\r\nNX\r\ni=1\r\nAPi;(8)\r\nwithAPibeing  the  AP  in  theith  class  andNis  the  total\r\nnumber of classes being evaluated. A. A Practical Example\r\nAs  stated  previously,  the  AP  is  calculated  individually  for\r\neach  class. In  the  example  shown  in  Figure  3,  the  boxes\r\nrepresent  detections  (red  boxes  identified  by  a  letter  -A,B,\r\n...,Y)  and  the  ground  truth  (green  boxes)  of  a  given  class. The  percentage  value  drawn  next  to  each  red  box  represents\r\nthe  detection  confidence  for  this  object  class. In  order  to\r\nevaluate  the  precision  and  recall  of  the  24  detections  among\r\nthe 15 ground-truth boxes distributed in seven images, an IOU\r\nthresholdtneeds  to  be  established. In  this  example,  let  us\r\nconsider as a TP detection box one having IOU\u001530%. Note\r\nthat each value of IOU threshold provides a different AP metric,\r\nand thus the threshold used must always be indicated. Table  I  presents  each  detection  ordered  by  their  confidence\r\nlevel. For  each  detection,  if  its  area  overlaps  30%  or  more  of\r\na  ground  truth  (IOU\u001530%),  the  TP  column  is  identified  as\r\nFig. 3: Example of 24 detections (red boxes) performed by an\r\nobject detector aiming to detect 15 ground-truth objects (green\r\nboxes) belonging to the same class. 1;  otherwise  it  is  set  to  0  and  it  is  considered  as  FP. Some\r\ndetectors  can  output  multiple  detections  overlapping  a  single\r\nground  truth  (e.g.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"LUzdo4AOTWTo3xZm37uMpBp3zriyiulFSxtZgx9WzYQ="},"010bb000-a217-4807-8ff3-88c21c65ffb5":{"id_":"010bb000-a217-4807-8ff3-88c21c65ffb5","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"6CxH1yP+M+u/1dFKuQ7I8PdQKGzU+iiy1u8EfbSFA4s="},"PREVIOUS":{"nodeId":"b22c0dc6-41ae-459f-ade1-00dc1fc46bb9","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"LUzdo4AOTWTo3xZm37uMpBp3zriyiulFSxtZgx9WzYQ="},"NEXT":{"nodeId":"c28f83ef-44e9-4818-924a-79a0057a44bb","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"CI8UcI0Ye30zqHx1Qni2ZaCvZA6CFSBGqHtzPpMaeMQ="}},"text":"detections  D  and  E  in  Image  2;  G,  H  and\r\nI  in  Image  3). For  those  cases  the  detection  with  the  highest\r\nconfidence  is  considered  a  TP  and  the  others  are  considered\r\nas  FP,  as  applied  by  the  PASCAL  VOC  2012  challenge. The\r\ncolumnsAcc  TPandAcc  FPaccumulate  the  total  amount  of\r\nTP  and  FP  along  all  the  detections  above  the  corresponding\r\nconfidence level. Figure 4 depicts the calculated precision and\r\nrecall values for this case.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"bxcFadHPFmrZiSlReQtgfqmKAowh6JellW3PxrhwRpY="},"c28f83ef-44e9-4818-924a-79a0057a44bb":{"id_":"c28f83ef-44e9-4818-924a-79a0057a44bb","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"6CxH1yP+M+u/1dFKuQ7I8PdQKGzU+iiy1u8EfbSFA4s="},"PREVIOUS":{"nodeId":"010bb000-a217-4807-8ff3-88c21c65ffb5","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"bxcFadHPFmrZiSlReQtgfqmKAowh6JellW3PxrhwRpY="}},"text":"Figure 4 depicts the calculated precision and\r\nrecall values for this case. TABLE I: Computation of Precision and Recall Values for IOU\r\nthreshold =30%\r\ndetection      confidence      TP      FP      acc TP      acc FP      precision       recall\r\nR                  95%             1         0             1                0                  1             0.0666Y                  95%             0         1             1                1                0.5           0.0666\r\nJ                  91%             1         0             2                1             0.6666        0.1333A                  88%             0         1             2                2                0.5           0.1333\r\nU                  84%             0         1             2                3                0.4           0.1333C                  80%             0         1             2                4             0.3333        0.1333\r\nM                 78%             0         1             2                5             0.2857        0.1333F                  74%             0         1             2                6               0.25          0.1333\r\nD                  71%             0         1             2                7             0.2222        0.1333B                  70%             1         0             3                7                0.3              0.2\r\nH                  67%             0         1             3                8             0.2727           0.2P                  62%             1         0             4                8             0.3333        0.2666\r\nE                  54%             1         0             5                8             0.3846        0.3333X                  48%             1         0             6                8             0.4285           0.4\r\nN                  45%             0         1             6                9                0.4              0.4T                  45%             0         1             6               10             0.375            0.4\r\nK                  44%             0         1             6               11            0.3529           0.4Q                  44%             0         1             6               12            0.3333           0.4\r\nV                  43%             0         1             6               13            0.3157           0.4I                  38%             0         1             6               14               0.3              0.4\r\nL                  35%             0         1             6               15            0.2857           0.4S                  23%             0         1             6               16            0.2727           0.4\r\nG                  18%             1         0             7               16            0.3043        0.4666O                  14%             0         1             7               17            0.2916        0.4666\r\nAs  mentioned  above,  each  interpolation  method  yields  a\r\ndifferent AP result, as given by (Figure 5):\r\nAP11=111(1 + 0:6666 + 0:4285 + 0:4285 + 0:4285)\r\nAP11= 26:84%;","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CI8UcI0Ye30zqHx1Qni2ZaCvZA6CFSBGqHtzPpMaeMQ="},"38edb0b2-ced7-4792-884b-32ee76cced76":{"id_":"38edb0b2-ced7-4792-884b-32ee76cced76","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"c0zMZf+DUMt1MAFMd+cd4AdGO1tbOLnFrb+yqI2HJdU="},"NEXT":{"nodeId":"3849c9ba-cab6-438d-91e1-15d23e226dd3","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"DZfHAg2g4RzND9q6cibqVgTeZeVLliOhjGHlwAL/UGU="}},"text":"Fig. 4: Precision x Recall curve with values calculated for each\r\ndetection in Table I. and (Figure 6):\r\nAPall= 1\u0003(0:0666\u00000) + 0:6666\u0003(0:1333\u00000:0666)\r\n+ 0:4285\u0003(0:4\u00000:1333) + 0:3043\u0003(0:4666\u00000:4)\r\nAPall= 24:56%:\r\nFig. 5: Precision\u0002Recall curves of points from Table I using\r\nthe 11-point interpolation approach. From  what  we  have  seen  so  far,  benchmarks  are  not  truly\r\ncomparable  if  the  method  used  to  calculate  the  AP  is  not\r\nreported. Works found in the literature [1], [9], [12]–[20], [37]\r\nusually  neither  mention  the  method  used  nor  reference  the\r\nadopted  tool  to  evaluate  their  results. This  problem  does  not\r\noccur  much  often  in  challenges,  as  it  is  a  common  practice\r\nto  have  a  reference  software  tool  included  in  order  for  the\r\nparticipants  to  evaluate  their  results. Also,  it  is  not  rare  to\r\noccur  cases  where  a  detector  sets  the  same  confidence  level\r\nfor  different  detections. Table  I,  for  example,  illustrates  that\r\ndetections R and Y obtained the same confidence level (95%). Depending  on  the  criterion  used  by  a  certain  implementation,\r\none or other detection can be sorted as the first detection in the\r\ntable,  directly  affecting  the  final  result  of  an  object-detection\r\nalgorithm. Some implementations may consider the order that\r\nFig. 6:  Precision\u0002Recall  curves  of  points  from  Table  I\r\napplying interpolation with all points.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"bp5NH0QyYuhWvQ1rYZPHeDht8CyX3WmWY07x2U/EK7E="},"3849c9ba-cab6-438d-91e1-15d23e226dd3":{"id_":"3849c9ba-cab6-438d-91e1-15d23e226dd3","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"c0zMZf+DUMt1MAFMd+cd4AdGO1tbOLnFrb+yqI2HJdU="},"PREVIOUS":{"nodeId":"38edb0b2-ced7-4792-884b-32ee76cced76","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"bp5NH0QyYuhWvQ1rYZPHeDht8CyX3WmWY07x2U/EK7E="},"NEXT":{"nodeId":"03f4674e-88a6-46df-a050-9cc004f3c9da","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"ZCCYeLjIN0JUd8Ao33jgd9p+OOTdHlZz5KkKoLlDs58="}},"text":"each  detection  was  reported  as  the  tiebreaker  (usually  one  or\r\nmore  evaluation  files  contain  the  detections  to  be  evaluated),\r\nbut in general there is no common consensus by the evaluation\r\ntools. III. OBJECT-DETECTIONCHALLENGES ANDTHEIRAP\r\nVARIANTS\r\nConstantly,  new  techniques  are  being  developed  and  new\r\ndifferent state-of-the-art object-detection algorithms are arising. Comparing  their  results  with  different  works  is  not  an  easy\r\ntask. Sometimes the applied metrics vary or the implementation\r\nused by the different authors may not be the same, generating\r\ndissimilar results. This section covers the main challenges and\r\ntheir most popular AP variants found in the literature. The  PASCAL  VOC  [31]  is  an  object-detection  challenge\r\nreleased  in  2005. From  2005  to  2012,  a  new  version  of  the\r\nPascal  VOC  was  released  with  increased  numbers  of  images\r\nand  classes,  starting  at  four  classes,  reaching  20  classes  in\r\nits  last  update. The  PASCAL  VOC  competition  still  accepts\r\nsubmissions,  revealing  state-of-the-art  algorithms  for  object\r\ndetections ever since. In this trail, the challenge applies the 11-\r\npoint interpolated precision (see Section II) and uses the mean\r\nAP over all of its classes to rank the submission performances,\r\nas implemented by the provided development kit. The Open Images 2019 challenge [24] in its object-detection\r\ntrack  uses  the  Open  Images  Dataset  [29]  containing  12.2  M\r\nannotated  bounding  boxes  across  500  object  categories  on\r\n1.7  M  images. Due  to  its  hierarchical  annotations,  the  same\r\nobject  can  belong  to  a  main  class  and  multiple  sub-classes\r\n(e.g. ‘helmet’ and ‘football helmet’). Because of that, the users\r\nshould report the class and subclasses of a given detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DZfHAg2g4RzND9q6cibqVgTeZeVLliOhjGHlwAL/UGU="},"03f4674e-88a6-46df-a050-9cc004f3c9da":{"id_":"03f4674e-88a6-46df-a050-9cc004f3c9da","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"c0zMZf+DUMt1MAFMd+cd4AdGO1tbOLnFrb+yqI2HJdU="},"PREVIOUS":{"nodeId":"3849c9ba-cab6-438d-91e1-15d23e226dd3","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"DZfHAg2g4RzND9q6cibqVgTeZeVLliOhjGHlwAL/UGU="}},"text":"Because of that, the users\r\nshould report the class and subclasses of a given detection. If\r\nsomehow only the main class is correctly reported for a detected\r\nbounding  box,  the  unreported  subclasses  affect  negatively  the\r\nscore, as it is counted as a false negative. The metric employed\r\nby the aforementioned challenge is the mean AP over all classes\r\nusing the Tensorflow Object Detection API [33]. The  COCO  detection  challenge  (bounding  box)  [22]  is  a\r\ncompetition which provides bounding-box coordinates of more\r\nthan  200,000  images  comprising  80  object  categories. The","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZCCYeLjIN0JUd8Ao33jgd9p+OOTdHlZz5KkKoLlDs58="},"1d3f8910-ac03-4ff2-9950-c1e27c683e97":{"id_":"1d3f8910-ac03-4ff2-9950-c1e27c683e97","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"V+2uk21p9IdE3n7wjW0+OVET8LG58DQoOGI3LVky1Ps="},"NEXT":{"nodeId":"1aa5cd1a-b9bf-401d-ae5f-2801fe76d4ff","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"4EXmTDTaygPKwWT4slAn61HYWirTu2XRqNIdVGnpJsc="}},"text":"submitted works are ranked according to metrics gathered into\r\nfour main groups. \u000fAP:  The  AP  is  evaluated  with  different  IOUs. It  can  becalculated for 10 IOUs varying in a range of 50% to 95%\r\nwith steps of 5%, usually reported as AP@50:5:95. It also\r\ncan  be  evaluated  with  single  values  of  IOU,  where  the\r\nmost common values are 50% and 75%, reported as AP50\r\nand AP75 respectively;\r\n\u000fAP  Across  Scales:  The  AP  is  determined  for  objects  inthree  different  sizes:  small  (with  area<322pixels),\r\nmedium (with322<area<962pixels), and large (with\r\narea>962pixels);\r\n\u000fAverage  Recall  (AR):  The  AR  is  estimated  by  the  maxi-mum recall values given a fixed number of detections per\r\nimage (1, 10 or 100) averaged over IOUs and classes;\r\n\u000fAR  Across  Scales:  The  AR  is  determined  for  objects  inthe same three different sizes as in the AP Across Scales,\r\nusually reported as AR-S, AR-M, and AR-L, respectively;\r\nTables II and III present results obtained by different object\r\ndetectors for the COCO and PASCAL VOC challenges, as given\r\nin [20], [38]. Due to different bounding-box annotation formats,\r\nresearchers  tend  to  report  only  the  metrics  supported  by  the\r\nsource code distributed with each dataset. Besides that, works\r\nthat use datasets with other annotation formats [39] are forced\r\nto  convert  their  annotations  to  PASCAL  VOC’s  and  COCO’s\r\nformats before using their evaluation codes. TABLE  II:  Results  using  AP  variants  obtained  by  different\r\nmethods on COCO dataset [40].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"p+nkLlT3BioZfGocOr86om1qRgvaZlnLD5lAxivflPY="},"1aa5cd1a-b9bf-401d-ae5f-2801fe76d4ff":{"id_":"1aa5cd1a-b9bf-401d-ae5f-2801fe76d4ff","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"V+2uk21p9IdE3n7wjW0+OVET8LG58DQoOGI3LVky1Ps="},"PREVIOUS":{"nodeId":"1d3f8910-ac03-4ff2-9950-c1e27c683e97","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"p+nkLlT3BioZfGocOr86om1qRgvaZlnLD5lAxivflPY="},"NEXT":{"nodeId":"62364afb-3397-4ce7-bdaa-8a2bb449712f","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"vWuBrsQzywqrSF1LRc4qhxRF8XwAk4u8GTUTiImO7oI="}},"text":"methods                                 AP@50:5:95      AP50      AP75      AP-S      AP-M      AP-L\r\nFaster R-CNN with ResNet-101 [9], [15]             34.9               55.7        37.4        15.6         38.7         50.9Faster R-CNN with FPN [15], [41]                  36.2               59.1        39.0        18.2         39.0         48.2\r\nFaster R-CNN by G-RMI [15], [42]                  34.7               55.5        36.7        13.5         38.1         52.0Faster R-CNN with TDM [15], [43]                 36.8               57.7        39.2        16.2         39.8         52.1\r\nYOLO v2 [19]                                   21.6               44.0        19.2         5.0          22.4         35.5YOLO v3 [20]                                   33.0               57.9        34.4        18.3         35.4         41.9\r\nSSD513 with ResNet-101 [9], [17]                  31.2               50.4        33.3        10.2         34.5         49.8DSSD513 with ResNet-101 [9], [44]                 33.2               53.3        35.2        13.0         35.4         51.1\r\nRetinaNet [40]                                   39.1               59.1        42.3        21.8         42.7         50.2\r\nTABLE  III:  Results  using  AP  variant  (mAP)  obtained  by\r\ndifferent methods on PASCAL VOC 2012 dataset [38].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4EXmTDTaygPKwWT4slAn61HYWirTu2XRqNIdVGnpJsc="},"62364afb-3397-4ce7-bdaa-8a2bb449712f":{"id_":"62364afb-3397-4ce7-bdaa-8a2bb449712f","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"V+2uk21p9IdE3n7wjW0+OVET8LG58DQoOGI3LVky1Ps="},"PREVIOUS":{"nodeId":"1aa5cd1a-b9bf-401d-ae5f-2801fe76d4ff","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"4EXmTDTaygPKwWT4slAn61HYWirTu2XRqNIdVGnpJsc="},"NEXT":{"nodeId":"26bca356-d630-47c0-9f2a-ae2bc624b044","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"B/MurnlFErJYAX3EmvWtldoOUEnXPxGXtMHvv25qPxw="}},"text":"methods                                                       mAP\r\nFaster R-CNN * [15]                                              70.4YOLO v1 [18]                                                   57.9\r\nYOLO v2 ** [19]                                                 78.2SSD300 ** [17]                                                  79.3\r\nSSD512 ** [17]                                                  82.2\r\n(*)  trained  with  PASCAL  VOC  dataset  images  only,  while  (**)  trained  withCOCO dataset images. The metric AP50 in Table II is calculated in the same way as\r\nthe metric mAP in Table III, but as the methods were trained\r\nand tested in different datasets, one obtains different results in\r\nboth  evaluations. Due  to  the  need  of  conversions  between  the\r\nbounding-box annotations among different datasets, researchers\r\nin general do not evaluate all methods with all possible metrics. In practice, it would be more meaningful if methods trained and\r\ntested with one dataset (PASCAL VOC, for instance) could also\r\nbe evaluated by the metrics employed in other datasets (COCO,\r\nfor instance). IV. ANOPEN-SOURCEPERFORMANCEMETRIC\r\nREPOSITORY\r\nIn  order  to  help  other  researchers  and  the  academic  com-\r\nmunity to obtain trustworthy results that can be comparable re-\r\ngardless the detector, the database, or the format of the ground-\r\ntruth  annotations,  a  library  was  developed  in  Python  with  the\r\nAP  metric  that  can  be  extended  to  its  variations. Easy-to-use\r\nfunctions  implement  the  same  metrics  used  as  benchmark  by\r\nthe most popular competitions and object-detection researches. The  proposed  implementation  does  not  require  modifications\r\nof  the  detection  model  to  match  complicated  input  formats,\r\navoiding  conversions  to  XML  or  JSON  files. To  assure  the\r\naccuracy  of  the  results,  the  implementation  followed  to  the\r\nletter  the  definitions  and  our  results  were  carefully  compared\r\nagainst the official implementations and the results are precisely\r\nthe same.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"vWuBrsQzywqrSF1LRc4qhxRF8XwAk4u8GTUTiImO7oI="},"26bca356-d630-47c0-9f2a-ae2bc624b044":{"id_":"26bca356-d630-47c0-9f2a-ae2bc624b044","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"V+2uk21p9IdE3n7wjW0+OVET8LG58DQoOGI3LVky1Ps="},"PREVIOUS":{"nodeId":"62364afb-3397-4ce7-bdaa-8a2bb449712f","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"vWuBrsQzywqrSF1LRc4qhxRF8XwAk4u8GTUTiImO7oI="},"NEXT":{"nodeId":"9e9aef14-0f0a-4ae4-a63d-138a4485a663","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"iMQWtAgrMcMCDyjW+00SYkIlzjKvA7r0TGt26LuJQ2M="}},"text":"The variations of the AP metric such as mAP, AP50,\r\nAP75  and  AP@50:5:95  using  the  11-point  or  the  all-point\r\ninterpolations can be obtained with the proposed library. The  input  data  (ground-truth  bounding  boxes  and  detected\r\nbounding   boxes)   format   was   simplified   requiring   a   single\r\nformat to compute all AP variation metrics. The format required\r\nis straightforward and can support the most popular detectors. For the ground-truth bounding boxes, a single text file for each\r\nimage should be created with each line in one of the following\r\nformats:\r\n<class> <left> <top> <right> <bottom>\r\n<class> <left> <top> <width> <height>\r\nFor the detections, a text file for each image should include\r\na line for each bounding box in one of the following formats:\r\n<class> <confidence> <left> <top> <right> <bottom>\r\n<class> <confidence> <left> <top> <width> <height>\r\nThe  second  options  support  YOLO’s  output  bounding-box\r\nformats. Besides specifying the input formats of the bounding\r\nboxes,  one  can  also  set  the  IOU  threshold  used  to  consider\r\na  TP  (useful  to  calculate  the  metrics  AP@50:5:95,  AP50  and\r\nAP75) and the interpolation method (11-point interpolation or\r\ninterpolation with all points). The tool will output the plots as\r\nin  Figures  5  and  6,  the  final  mAP  and  the  AP  for  each  class,\r\ngiving  a  better  view  of  the  results  for  each  class. The  tool\r\nalso provides an option to generate the output images with the\r\nbounding boxes drawn on it as shown in Figure 1. The  project  distributed  with  this  paper  can  be  accessed  at:\r\nhttps:// github.com/ rafaelpadilla/ Object-Detection-Metrics.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"B/MurnlFErJYAX3EmvWtldoOUEnXPxGXtMHvv25qPxw="},"9e9aef14-0f0a-4ae4-a63d-138a4485a663":{"id_":"9e9aef14-0f0a-4ae4-a63d-138a4485a663","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"V+2uk21p9IdE3n7wjW0+OVET8LG58DQoOGI3LVky1Ps="},"PREVIOUS":{"nodeId":"26bca356-d630-47c0-9f2a-ae2bc624b044","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"B/MurnlFErJYAX3EmvWtldoOUEnXPxGXtMHvv25qPxw="}},"text":"So\r\nfar, our framework has helped researchers to obtain AP metrics\r\nand its variations in a simple way, supporting the most popular\r\nformats  used  by  datasets,  avoiding  conversions  to  XML  or\r\nJSON  files. The  proposed  tool  has  been  used  as  the  official\r\ntool in the competition [27], adopted in 3rd-party libraries such\r\nas [45] and used by many other works as in [46]–[48].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"iMQWtAgrMcMCDyjW+00SYkIlzjKvA7r0TGt26LuJQ2M="},"c21d166f-065e-46dd-a10d-2e09887dd23e":{"id_":"c21d166f-065e-46dd-a10d-2e09887dd23e","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"NEXT":{"nodeId":"33285597-bf06-4e9a-9b76-2a9aea4d2317","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"F2+8BNoBDuL/Bt8jBfkyNPY7J6DrIGp8a2b5fJBdZAM="}},"text":"REFERENCES\r\n[1]   W. Hu, T. Tan, L. Wang, and S. Maybank, “A survey on visual surveil-lance  of  object  motion  and  behaviors,”IEEE  Transactions  on  Systems,\r\nMan, and Cybernetics, Part C: Applications and Reviews, vol. 34, no. 3,pp. 334–352, Aug 2004. [2]   P. Viola and M. Jones, “Rapid object detection using a boosted cascadeof simple features,” inIEEE Computer Society Conference on Computer\r\nVision and Pattern Recognition, vol. 1, Dec 2001, p. 511518.[3]   R. Padilla,  C. Costa  Filho,  and  M. Costa,  “Evaluation  of  haar  cascade\r\nclassifiers  designed  for  face  detection,”World  Academy  of  Science,Engineering and Technology, vol. 64, pp. 362–365, 2012. [4]   E. Ohn-Bar and M. M. Trivedi, “To boost or not to boost? on the limitsof boosted trees for object detection,” inIEEE International Conference\r\non Pattern Recognition, Dec 2016, pp. 3350–3355.[5]   Z. Sun, G. Bebis, and R. Miller, “On-road vehicle detection: A review,”\r\nIEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28,no. 5, pp. 694–711, May 2006. [6]   A. Krizhevsky,  I. Sutskever,  and  G. E. Hinton,  “Imagenet  classificationwith deep convolutional neural networks,” inInternational Conference on\r\nNeural Information Processing Systems, 2012, pp. 1097–1105.[7]   C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,\r\nV. Vanhoucke,  and  A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"z43E0QubgD77tvncOriyaqmZq8oEAz/roBKtXArMpf0="},"33285597-bf06-4e9a-9b76-2a9aea4d2317":{"id_":"33285597-bf06-4e9a-9b76-2a9aea4d2317","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"c21d166f-065e-46dd-a10d-2e09887dd23e","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"z43E0QubgD77tvncOriyaqmZq8oEAz/roBKtXArMpf0="},"NEXT":{"nodeId":"b6b983a2-d715-4f2f-b638-c791aef4a664","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"/9eHcNLlWRTMXqgvYHg3ndmIf0Q5aRE1UHm9e6riSCk="}},"text":"Erhan,\r\nV. Vanhoucke,  and  A. Rabinovich,  “Going  deeper  with  convolutions,”inIEEE Conference on Computer Vision and Pattern Recognition, June\r\n2015, pp. 1–9.[8]   Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning\r\napplied to document recognition,” inProceedings of the IEEE, 1998, pp.2278–2324. [9]   K. He,  X. Zhang,  S. Ren,  and  J. Sun,  “Deep  residual  learning  forimage recognition,” inIEEE Conference on Computer Vision and Pattern\r\nRecognition, Jun 2016, pp. 770–778.[10]   G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for\r\ndeep  belief  nets,”Neural  Computation,  vol. 18,  no. 7,  pp. 1527–1554,2006. [11]   G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality ofdata  with  neural  networks,”Science,  vol. 313,  no. 5786,  pp. 504–507,\r\nJul. 2006.[12]   P. Sermanet,  D. Eigen,  X. Zhang,  M. Mathieu,  R. Fergus,  and  Y. Le-\r\nCun,  “Overfeat:  Integrated  recognition,  localization  and  detection  usingconvolutional networks,”CoRR, 2013. [13]   R. Girshick,  J. Donahue,  T. Darrell,  and  J. Malik,  “Rich  feature  hierar-chies for accurate object detection and semantic segmentation,” inIEEE\r\nConference on Computer Vision and Pattern Recognition, Jun 2014.[14]   R. Girshick, “Fast r-cnn,” inIEEE International Conference on Computer\r\nVision, Dec 2015.[15]   S.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"F2+8BNoBDuL/Bt8jBfkyNPY7J6DrIGp8a2b5fJBdZAM="},"b6b983a2-d715-4f2f-b638-c791aef4a664":{"id_":"b6b983a2-d715-4f2f-b638-c791aef4a664","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"33285597-bf06-4e9a-9b76-2a9aea4d2317","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"F2+8BNoBDuL/Bt8jBfkyNPY7J6DrIGp8a2b5fJBdZAM="},"NEXT":{"nodeId":"f26d95b0-569f-4fcd-9d2a-85166c402481","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"5uWKPjkKyKobiv5zG/MEo69vziyvzXCqE+dhSZSEbIY="}},"text":"Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time\r\nobject  detection  with  region  proposal  networks,”  inAdvances  in  NeuralInformation Processing Systems 28, 2015, pp. 91–99. [16]   J. Dai,  Y. Li,  K. He,  and  J. Sun,  “R-FCN:  object  detection  via  region-based fully convolutional networks,”CoRR, 2016. [17]   W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C.Berg, “SSD: single shot multibox detector,”CoRR, 2015. [18]   J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:Unified,  real-time  object  detection,”  inIEEE  Conference  on  Computer\r\nVision and Pattern Recognition, 2016, pp. 779–788.[19]   J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” inIEEE\r\nConference on Computer Vision and Pattern Recognition, 2017, pp. 7263–7271. [20]   ——, “Yolov3: An incremental improvement,”Technical Report, 2018.[21]   M. Everingham,  S. M. A. Eslami,  L. Van  Gool,  C. K. I. Williams,\r\nJ. Winn, and A. Zisserman, “The pascal visual object classes challenge: Aretrospective,”International Journal of Computer Vision, vol. 111, no. 1,\r\npp. 98–136, Jan. 2015.[22]   Coco  detection  challenge  (bounding  box). [Online]. Available:  https:\r\n//competitions.codalab.org/competitions/20794[23]   ImageNet. Imagenet  object  localization  challenge. [Online].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/9eHcNLlWRTMXqgvYHg3ndmIf0Q5aRE1UHm9e6riSCk="},"f26d95b0-569f-4fcd-9d2a-85166c402481":{"id_":"f26d95b0-569f-4fcd-9d2a-85166c402481","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"b6b983a2-d715-4f2f-b638-c791aef4a664","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"/9eHcNLlWRTMXqgvYHg3ndmIf0Q5aRE1UHm9e6riSCk="},"NEXT":{"nodeId":"20cd96e7-750d-409b-887b-4c124e595fb2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"JAww5R7CntLOavl/NKMkq71/PlO8ZoFcI84WmBBUJyc="}},"text":"Imagenet  object  localization  challenge. [Online]. Available:\r\nhttps://www.kaggle.com/c/imagenet-object-localization-challenge/[24]   G. Research. Open       images       2019       -       object       detec-\r\ntion      challenge. [Online]. Available:      https://www.kaggle.com/c/open-images-2019-object-detection/\r\n[25]   Lyft. Lyft         3d         object         detection         for         autonomousvehicles. [Online]. Available:          https://www.kaggle.com/c/\r\n3d-object-detection-for-autonomous-vehicles/\r\n[26]   G. Research. Google      ai      open      images      -      object      de-tection      track. [Online]. Available:      https://www.kaggle.com/c/\r\ngoogle-ai-open-images-object-detection-track/\r\n[27]   City intelligence hackathon. [Online]. Available: https://belvisionhack.ru\r\n[28]   O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,A. Karpathy, A. Khosla, M. Bernsteinet al., “Imagenet large scale visual\r\nrecognition  challenge,”International  Journal  of  Computer  Vision,  vol.115, no. 3, pp. 211–252, 2015. [29]   I. Krasin,    T. Duerig,    N. Alldrin,    V. Ferrari,    S. Abu-El-Haija,A. Kuznetsova,  H. Rom,  J. Uijlings,  S. Popov,  S. Kamali,  M. Malloci,\r\nJ. Pont-Tuset,  A. Veit,  S. Belongie,  V. Gomes,  A. Gupta,  C. Sun,G. Chechik, D. Cai, Z. Feng, D. Narayanan, and K.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5uWKPjkKyKobiv5zG/MEo69vziyvzXCqE+dhSZSEbIY="},"20cd96e7-750d-409b-887b-4c124e595fb2":{"id_":"20cd96e7-750d-409b-887b-4c124e595fb2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"f26d95b0-569f-4fcd-9d2a-85166c402481","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"5uWKPjkKyKobiv5zG/MEo69vziyvzXCqE+dhSZSEbIY="},"NEXT":{"nodeId":"b3fb1114-a9bc-4f20-a640-0a4cd30691be","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"ZMufoj2TA8r0QevojVthn8+25rnLWsuFM9y4NirAUWk="}},"text":"Cai, Z. Feng, D. Narayanan, and K. Murphy, “Openim-\r\nages:  A  public  dataset  for  large-scale  multi-label  and  multi-class  imageclassification,” 2017. [30]   T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays,P. Perona, D. Ramanan, P. Doll ́ar, and C. L. Zitnick, “Microsoft COCO:\r\ncommon objects in context,”CoRR, 2014. [31]   M. Everingham,  L. Van  Gool,  C. K. I. Williams,  J. Winn,  and  A. Zis-serman, “The pascal visual object classes (voc) challenge,”International\r\nJournal of Computer Vision, vol. 88, no. 2, pp. 303–338, Jun. 2010. [32]   R. Padilla. Metrics   for   object   detection. [Online]. Available:   https://github.com/rafaelpadilla/Object-Detection-Metrics\r\n[33]   M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.Corrado,  A. Davis,  J. Dean,  M. Devin,  S. Ghemawat,  I. Goodfellow,\r\nA. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,J. Levenberg,  D. Man ́e,  R. Monga,  S. Moore,  D. Murray,  C. Olah,\r\nM. Schuster,  J. Shlens,  B. Steiner,  I. Sutskever,  K.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"JAww5R7CntLOavl/NKMkq71/PlO8ZoFcI84WmBBUJyc="},"b3fb1114-a9bc-4f20-a640-0a4cd30691be":{"id_":"b3fb1114-a9bc-4f20-a640-0a4cd30691be","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"20cd96e7-750d-409b-887b-4c124e595fb2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"JAww5R7CntLOavl/NKMkq71/PlO8ZoFcI84WmBBUJyc="},"NEXT":{"nodeId":"189b36f6-6a68-4e67-9264-6703ccfd0807","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"hHPFm9oa+6zw54TmxNFSlTTgVXT+u3FAsXxJKyOzzto="}},"text":"Steiner,  I. Sutskever,  K. Talwar,  P. Tucker,V. Vanhoucke, V. Vasudevan, F. Vi ́egas, O. Vinyals, P. Warden, M. Watten-\r\nberg, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: Large-scale machinelearning on heterogeneous systems,” 2015. [34]   TensorFlow. Detection  evaluation  protocols. [Online]. Available:  https://github.com/tensorflow\r\n[35]   P. Jaccard, “ ́Etude comparative de la distribution florale dans une portiondes  alpes  et  des  jura,”Bulletin  de  la  Societe  Vaudoise  des  Sciences\r\nNaturelles, vol. 37, pp. 547–579, 1901. [36]   J. A. Hanley and B. J. McNeil, “The meaning and use of the area under areceiver operating characteristic (roc) curve.”Radiology, vol. 143, no. 1,\r\npp. 29–36, 1982. [37]   D. Yoo, S. Park, J.-Y. Lee, A. S. Paek, and I. So Kweon, “Attentionnet:Aggregating  weak  directions  for  accurate  object  detection,”  inIEEE\r\nInternational Conference on Computer Vision, 2015, pp. 2659–2667. [38]   Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deeplearning: A review,”IEEE Transactions on Neural Networks and Learning\r\nSystems, vol. 30, no. 11, pp. 3212–3232, 2019. [39]   “An  annotated  video  database  for  abandoned-object  detection  in  a  clut-tered  environment,”  inInternational  Telecommunications  Symposium,\r\n2014, pp. 1–5. [40]   T.-Y. Lin,  P. Goyal,  R.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZMufoj2TA8r0QevojVthn8+25rnLWsuFM9y4NirAUWk="},"189b36f6-6a68-4e67-9264-6703ccfd0807":{"id_":"189b36f6-6a68-4e67-9264-6703ccfd0807","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"b3fb1114-a9bc-4f20-a640-0a4cd30691be","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"ZMufoj2TA8r0QevojVthn8+25rnLWsuFM9y4NirAUWk="},"NEXT":{"nodeId":"c338ba39-81f7-40c2-922a-a685e6d4db15","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"GGv0Y28i4lMOupEPLzhZQfR2wzVjndJjGm6VtiyPJXM="}},"text":"Lin,  P. Goyal,  R. Girshick,  K. He,  and  P. Doll ́ar,  “Focal  loss  fordense object detection,” inIEEE International Conference on Computer\r\nVision, 2017, pp. 2980–2988. [41]   T.-Y. Lin, P. Doll ́ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,“Feature pyramid networks for object detection,” inIEEE Conference on\r\nComputer Vision and Pattern Recognition, 2017, pp. 2117–2125. [42]   J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,Z. Wojna, Y. Song, S. Guadarramaet al., “Speed/accuracy trade-offs for\r\nmodern convolutional object detectors,” inIEEE Conference on ComputerVision and Pattern Recognition, 2017, pp. 7310–7311. [43]   A. Shrivastava,  R. Sukthankar,  J. Malik,  and  A. Gupta,  “Beyond  skipconnections: Top-down modulation for object detection,”arXiv, 2016. [44]   C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “Dssd: Deconvo-lutional single shot detector,”arXiv, 2017. [45]   C. R. I. of  Montreal  (CRIM). thelper  package. [Online]. Available:https://thelper.readthedocs.io/en/latest/thelper.optim.html\r\n[46]   C. Adleson  and  D. C. Conner,  “Comparison  of  classical  and  cnn-baseddetection  techniques  for  state  estimation  in  2d,”Journal  of  Computing\r\nSciences in Colleges, vol. 35, no. 3, pp.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"hHPFm9oa+6zw54TmxNFSlTTgVXT+u3FAsXxJKyOzzto="},"c338ba39-81f7-40c2-922a-a685e6d4db15":{"id_":"c338ba39-81f7-40c2-922a-a685e6d4db15","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"H1Q83//Fe1CnaLStLPbrp2kwMT9USoc2WA/aTyuliwI="},"PREVIOUS":{"nodeId":"189b36f6-6a68-4e67-9264-6703ccfd0807","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/A_survey_on_performance_metrics_for_object_detection_algorithms.pdf","file_name":"A_survey_on_performance_metrics_for_object_detection_algorithms.pdf"},"hash":"hHPFm9oa+6zw54TmxNFSlTTgVXT+u3FAsXxJKyOzzto="}},"text":"35, no. 3, pp. 122–133, 2019. [47]   A. Borji   and   S. M. Iranmanesh,   “Empirical   upper-bound   in   objectdetection and more,”arXiv, 2019. [48]   D. Caschili,  M. Poncino,  and  T. Italia,  “Optimization  of  cnn-basedobject detection algorithms for embedded systems,” Masters dissertation,\r\nPolitecnico di Torino, 2019.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GGv0Y28i4lMOupEPLzhZQfR2wzVjndJjGm6VtiyPJXM="},"b8b7f27d-61d3-41b6-80bc-fb5a76fc981a":{"id_":"b8b7f27d-61d3-41b6-80bc-fb5a76fc981a","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"wAxl7z3HWXL9r8vq7t5cBWRhtIDq3+JyyjE9uh4Vh+s="},"NEXT":{"nodeId":"2ddb9b2f-3244-4847-88ff-0b9f29c872f1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"wlBza15jRmbAuJKPQJvO4KYmHjbjxsZmDyQL5JJTp9Q="}},"text":"Diagnosing Error in Object Detectors\r\nDerek Hoiem, Yodsawalai Chodpathumwan, and Qieyun Dai? Department of Computer Science\r\nUniversity of Illinois at Urbana-Champaign\r\nAbstract.This paper shows how to analyze the influences of object characteris-\r\ntics on detection performance and the frequency and impact of different types of\r\nfalse positives. In particular, we examine effects of occlusion, size, aspect ratio,\r\nvisibility of parts, viewpoint, localization error, and confusion with semantically\r\nsimilar objects, other labeled objects, and background. We analyze two classes\r\nof detectors: the Vedaldi et al. multiple kernel learning detector and different ver-\r\nsions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size,\r\nlocalization error, and confusion with similar objects are the most impactful forms\r\nof error. Our analysis also reveals that many different kinds of improvement are\r\nnecessary to achieve large gains, making more detailed analysis essential for the\r\nprogress of recognition research. By making our software and annotations avail-\r\nable, we make it effortless for future researchers to perform similar analysis. 1    Introduction\r\nLarge datasets are a boon to object recognition, yielding powerful detectors trained on\r\nhundreds of examples. Dozens of papers are published every year citing recognition\r\naccuracy or average precision results, computed over thousands of images from Cal-\r\nTech or PASCAL VOC datasets [1, 2]. Yet such performance summaries do not tell us\r\nwhyone method outperforms another or help understand how it could be improved. For authors, it is difficult to find the most illustrative qualitative results, and the read-\r\ners may be suspicious of a few hand-picked successes or “intuitive” failures. To make\r\nmatters worse, a dramatic improvement in one aspect of recognition may produce only\r\na tiny change in overall performance. For example, our study shows that complete ro-\r\nbustness  to  occlusion  would  lead  to  an  improvement  of  only  a  few  percent  average\r\nprecision. There are many potential causes of failure: occlusion, intra-class variations,\r\npose, camera position, localization error, and confusion with similar objects or textured\r\nbackgrounds. To make progress, we need to better understand what most needs im-\r\nprovement and whether a new idea produces the desired effect.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"d4YN3TCwyAfql8LSGmo88Q6MXEiuaiVpH6NSJSlAJxQ="},"2ddb9b2f-3244-4847-88ff-0b9f29c872f1":{"id_":"2ddb9b2f-3244-4847-88ff-0b9f29c872f1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"wAxl7z3HWXL9r8vq7t5cBWRhtIDq3+JyyjE9uh4Vh+s="},"PREVIOUS":{"nodeId":"b8b7f27d-61d3-41b6-80bc-fb5a76fc981a","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"d4YN3TCwyAfql8LSGmo88Q6MXEiuaiVpH6NSJSlAJxQ="}},"text":"We need to measure the\r\nmodes of failure of our algorithms. Fortunately, we already have excellent large, diverse\r\ndatasets; now, we propose annotations and analysis tools to take full advantage. This paper analyzes the influences of object characteristics on detection performance\r\nand the frequency and impact of different types of false positives. In particular, we ex-\r\namine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization\r\n?This work was supported by NSF awards IIS-1053768 and IIS-0904209, ONR MURI Grant\r\nN000141010934, and a research award from Google.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wlBza15jRmbAuJKPQJvO4KYmHjbjxsZmDyQL5JJTp9Q="},"84d2a46b-934a-498f-9ae9-e658aac0d208":{"id_":"84d2a46b-934a-498f-9ae9-e658aac0d208","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"J+MbbDcem4maiJkLvB2f4UaEC07P4XpuhgxPtqORrTw="},"NEXT":{"nodeId":"2ab24bc5-8ecf-4938-8f8e-7dff205b0ae7","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"f/IBnafqEfz9oyykDDq5+YLqYOJv0xTV+hFKbh989o4="}},"text":"2             Diagnosing Error in Object Detectors\r\nerror, confusion with semantically similar objects, confusion with other labeled objects,\r\nand confusion with background. We analyze two types of detectors on the PASCAL\r\nVOC 2007 dataset [2]: the Vedaldi et al. [3] multiple kernel learning detector (called\r\nVGVZ for authors’ initials) and the Felzenszwalb et al. [4, 5] detector (called FGMR). By making our analysis software and annotations available, we will make it effortless\r\nfor future researchers to perform similar analysis. Relation to Existing Studies:Many methods have been proposed to address a par-\r\nticular recognition challenge, such as occlusion (e.g., [6–8]), variation in aspect ratio\r\n(e.g., [5, 3, 9]), or changes of viewpoint (e.g., [10–12]). Such methods are usually eval-\r\nuated based on overall performance, home-brewed datasets, or artificial manipulations,\r\nmaking it difficult to determine whether the motivating challenge is addressed for nat-\r\nurally varying examples. Researchers are aware of the value of additional analysis, but\r\nthere are no standards for performing or compactly summarizing detailed evaluations. Some studies focus on particular aspects of recognition, such as interest point detec-\r\ntion (e.g., [13, 14]), contextual methods [15, 16], dataset design [17], and cross-dataset\r\ngeneralization [18]. The work by Divvala et al. [15] is particularly relevant: through\r\nanalysis of false positives, they show that context reduces confusion with textured back-\r\nground patches and increases confusion with semantically similar objects. The report on\r\nPASCAL VOC 2007 by Everingham et al. [19] is also related in its sensitivity analysis\r\nof size and qualitative analysis of failures. For size analysis, Everingham et al. measure\r\nthe average precision (AP) for increasing size thresholds (smaller-than-threshold ob-\r\njects are treated as “don’t cares”). Because the measure is cumulative, some effects of\r\nsize are obscured, causing the authors to conclude that detectors have a “limited prefer-\r\nence for large objects”. In contrast, our experiments indicate that object size is the best\r\nsingle predictor of performance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rEjhMhsyNRtaecn0TGTmUewSbjHFgeDLagdTLOukg78="},"2ab24bc5-8ecf-4938-8f8e-7dff205b0ae7":{"id_":"2ab24bc5-8ecf-4938-8f8e-7dff205b0ae7","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"J+MbbDcem4maiJkLvB2f4UaEC07P4XpuhgxPtqORrTw="},"PREVIOUS":{"nodeId":"84d2a46b-934a-498f-9ae9-e658aac0d208","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"rEjhMhsyNRtaecn0TGTmUewSbjHFgeDLagdTLOukg78="}},"text":"In contrast, our experiments indicate that object size is the best\r\nsingle predictor of performance. Studies  in  specialized  domains,  such  as  pedestrian  detection  [20]  and  face  recogni-\r\ntion  [21, 22],  have  provided  useful  insights. For  example,  Dollar  et  al. [20]  analyze\r\neffects  of  scale,  occlusion,  and  aspect  ratio  for  a  large  dataset  of  pedestrian  videos,\r\nsummarizing performance for different subsets with a single point on the ROC curve. Contributions:Our main contribution is to provide analysis tools (annotations, soft-\r\nware,  and  techniques)  that  facilitate  detailed  and  meaningful  investigation  of  object\r\ndetector performance. Although benchmark metrics, such as AP, are well-established,\r\nwe had much difficulty to quantitatively explore causes and correlates of error; we wish\r\nto share some of the methodology that we found most useful with the community. Our second contribution is the analysis of two state-of-the-art detectors. We intend our\r\nanalysis to serve as an example of how other researchers can evaluate the frequency\r\nand correlates of error for their own detectors. We sometimes include detailed analysis\r\nfor the purpose of exemplification, beyond any insights that can be gathered from the\r\nresults. Third, our analysis also helps identify significant weaknesses of current approaches and\r\nsuggests directions for improvement. Equally important, our paper highlights the danger\r\nof relying on overall benchmarks to measure short-term progress. Currently, detectors\r\nperform well for the most common cases of objects and avoid egregious errors. Large","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"f/IBnafqEfz9oyykDDq5+YLqYOJv0xTV+hFKbh989o4="},"7cf32399-2626-48a3-842b-00e9993857f9":{"id_":"7cf32399-2626-48a3-842b-00e9993857f9","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"M2EYESc7MyyPuCT2qs5jmwln2UFoaQdEeid15J4UC0s="},"NEXT":{"nodeId":"e547f10c-f8ab-412a-9c83-d700217ea8cb","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"UHR0PZE2tNybgeT6E5D94OtigkD3I1S65242+auNy+g="}},"text":"Diagnosing Error in Object Detectors             3\r\nimprovements in any one aspect of object recognition may yield small overall gains and\r\ngo under-appreciated without further analysis. Details of Dataset and Detectors:Our experiments are based on thePASCAL VOC\r\n2007 dataset[2], which is widely used to evaluate performance in object category de-\r\ntection. The detection task is to find instances of a specific object category within each\r\ninput image, localizing each object with a tight bounding box. For the 2007 dataset,\r\nroughly  10,000  images  were  collected  from  Flickr.com  and  split  evenly  into  train-\r\ning/validation and testing sets. The dataset is favored for its representative sample of\r\nconsumer photographs and rigorous annotation and collection procedures. The dataset\r\ncontains bounding box annotations for 20 object categories, including some auxiliary\r\nlabels for truncation and five categories of viewpoint. We extend these annotations with\r\nmore detailed labels for occlusion, part visibility, and viewpoint. Object detections con-\r\nsist of a bounding box and confidence. The highest confidence bounding box with 50%\r\noverlap is considered correct; all others are incorrect. Detection performance is mea-\r\nsured with precision-recall curves and summarized with average precision. We use the\r\n2007 version of VOC because the test set annotations are available (unlike later ver-\r\nsions), enabling analysis on test performance of various detectors. Experiments on later\r\nversions are very likely to yield similar conclusions. TheFGMR detector[23] consists of a mixture of deformable part models for each\r\nobject category, where each mixture component has a global template and a set of de-\r\nformable parts. Both the global template and the deformable parts are represented by\r\nHOG features captured at a coarser and finer scale respectively. A hypothesis score con-\r\ntains both a data term (filter responses) and a spatial prior (deformable cost), and the\r\noverall score for each root location is computed based on the best placement of parts. Search is performed by scoring the root template at each position and scale. The training\r\nof the object model is posed as a latent SVM problem where the latent variables specify\r\nthe object configuration. The main differences between v4 and v2 are that v4 includes\r\nmore components (3 vs. 2) and more latent parts (8 vs.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"X9676MYvxyaQYau+MTiVZeurCbEwZ66aq1I/6wy/Es4="},"e547f10c-f8ab-412a-9c83-d700217ea8cb":{"id_":"e547f10c-f8ab-412a-9c83-d700217ea8cb","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"M2EYESc7MyyPuCT2qs5jmwln2UFoaQdEeid15J4UC0s="},"PREVIOUS":{"nodeId":"7cf32399-2626-48a3-842b-00e9993857f9","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"X9676MYvxyaQYau+MTiVZeurCbEwZ66aq1I/6wy/Es4="}},"text":"2) and more latent parts (8 vs. 6) and has a latent left/right flip\r\nterm. We do not use the optional context rescoring. TheVGVZ detector[3] adopts a cascade approach by training a three-stage classifier,\r\nusing as features Bag of Visual Words, Dense Words, Histogram of Oriented Edges\r\nand Self-Similarity Features. For each feature channel, a three-level pyramid of spatial\r\nhistograms is computed. In the first stage, a linear SVM is used to output a set of can-\r\ndidate regions, which are passed to the more powerful second and third stages, which\r\nuses quasi-linear and non-linear kernel SVMs respectively. Search is performed using\r\n“jumping windows” proposed based on a learned set of detected keypoints. 2    Analysis of False Positives\r\nOne major type of error is false positives, detections that do not correspond to the tar-\r\nget category. There are different types of false positives which likely require different\r\nkinds of solutions.Localization erroroccurs when an object from the target category\r\nis detected with a misaligned bounding box (0:1<=overlap<0:5). Other overlap","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"UHR0PZE2tNybgeT6E5D94OtigkD3I1S65242+auNy+g="},"7bf51ec0-ce2e-41c3-a35c-cdbcd2c49c4a":{"id_":"7bf51ec0-ce2e-41c3-a35c-cdbcd2c49c4a","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"rzT75t2cNtePQUtPWwjnWyTgPICZkdiOjmVxhG5A470="},"NEXT":{"nodeId":"84f8c098-7b03-4993-82fe-80ce8fe02aa7","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"x24O2LgKzGAsSYQuTybDp1iiKAnO971kfYzQK1k/Jkk="}},"text":"4             Diagnosing Error in Object Detectors\r\naeroplane (loc): ov=0.47  1−r=0.95aeroplane (sim): ov=0.00  1−r=0.93aeroplane (bg): ov=0.00  1−r=0.88aeroplane (loc): ov=0.50  1−r=0.91aeroplane (loc): ov=0.49  1−r=0.87aeroplane (sim): ov=0.00  1−r=0.85\r\nTop Airplane False Positives\r\ncat (loc): ov=0.40  1−r=0.99cat (loc): ov=0.38  1−r=0.98cat (loc): ov=0.16  1−r=0.97cat (loc): ov=0.47  1−r=0.97cat (loc): ov=0.35  1−r=0.96cat (loc): ov=0.63  1−r=0.96\r\nTop Cat False Positives\r\ncow (sim): ov=0.00  1−r=0.93cow (sim): ov=0.00  1−r=0.91cow (sim): ov=0.00  1−r=0.92cow (sim): ov=0.00  1−r=0.90cow (sim): ov=0.00  1−r=0.96cow (sim): ov=0.00  1−r=0.90\r\nTop Cow False Positives\r\nFig. 1. Examples of top false positives:We show the top six false positives (FPs) for the FGMR\r\n(v4) airplane, cat, and cow detectors. The text indicates the type of error (“loc”=localization;\r\n“bg”=background; “sim”=confusion with similar object), the amount of overlap (“ov”) with a\r\ntrue object, and the fraction of correct examples that are ranked lower than the given false positive\r\n(“1-r”, for 1-recall). Localization errors could be insufficient overlap (less than 0.5) or duplicate\r\ndetections.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GOPKaln4mZsKKYH3CYLPlbauh+VzXMys/+vwGV7VW1o="},"84f8c098-7b03-4993-82fe-80ce8fe02aa7":{"id_":"84f8c098-7b03-4993-82fe-80ce8fe02aa7","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"rzT75t2cNtePQUtPWwjnWyTgPICZkdiOjmVxhG5A470="},"PREVIOUS":{"nodeId":"7bf51ec0-ce2e-41c3-a35c-cdbcd2c49c4a","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"GOPKaln4mZsKKYH3CYLPlbauh+VzXMys/+vwGV7VW1o="}},"text":"Localization errors could be insufficient overlap (less than 0.5) or duplicate\r\ndetections. Qualitative examples, such as these, indicate that confusion with similar objects and\r\nlocalization error are much more frequent causes of false positives than mislabeled background\r\npatches (which provide many more opportunities for error). thresholds (e.g.,0:2<=overlap<0:5) led to similar conclusions. “Overlap” is de-\r\nfined as the intersection divided by union of the ground truth and detection bounding\r\nboxes. We also consider a duplicate detection (two detections for one object) to be lo-\r\ncalization error because such mistakes are avoidable with good localization. Remaining\r\nfalse positives that have at least 0.1 overlap with an object from a similar category are\r\ncounted asconfusion with similar objects. For example a “dog” detector may assign\r\na high score to a “cat” region. We consider two categories to be semantically similar\r\nif they are both within one of these sets:fall vehiclesg,fall animals including per-\r\nsong,fchair, diningtable, sofag,faeroplane, birdg.Confusion with dissimilar objects\r\ndescribes remaining false positives that have at least 0.1 overlap with another labeled\r\nVOC object. For example, the FGMR bottle detector very frequently detects people\r\nbecause the exterior contours are similar. All other false positives are categorized as\r\nconfusion with background. These could be detections within highly textured areas\r\nor confusions with unlabeled objects that are not within the VOC categories. See Fig. 1\r\nfor examples of confident false positives. In Figure 2, we show the frequency and impact on performance of each type of false\r\npositive. We count “top-ranked” false positives that are within the most confidentNj\r\ndetections. We choose parameterNjto be the number of positive examples for the cat-\r\negory, so that if all objects are correctly detected, no false positives would remain. A\r\nsurprisingly small fraction of confident false positives are due to confusion with back-\r\nground (e.g., only9%for VGVZ animal detectors on average). For animals, most false\r\npositives are due to confusion with other animals; for vehicles, localization and con-","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"x24O2LgKzGAsSYQuTybDp1iiKAnO971kfYzQK1k/Jkk="},"59ea4c24-efce-45c9-8c55-a77cd5bc850b":{"id_":"59ea4c24-efce-45c9-8c55-a77cd5bc850b","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"n3naMl4LZptMIuSITsBnJN1OAV0MbQmqfWdKgXM6cY4="},"NEXT":{"nodeId":"78b15ac1-8055-4784-8ab4-66370a12af18","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"DNcjbN0wClEZuazIow8eBZDQ0g6fhmZqkKe7SJSA4Z8="}},"text":"Diagnosing Error in Object Detectors             5\r\n% Top False Positives, AP Impact: FGMR (v4) Detector % Top False Positives, AP Impact: VGVZ’09 Detector \r\nLoc17%Sim51%\r\nOth10%\r\nBG22%Animals\r\nLoc36%\r\nSim30%\r\nOth11%\r\nBG23%Vehicles\r\nLoc19%Sim17%\r\nOth35%BG29%Furniture\r\nLoc56%\r\nSim10%Oth\r\n12%\r\nBG22%Person\r\nLoc10%\r\nSim71%\r\nOth5%\r\nBG14%Cow\r\nLoc53%\r\nSim21%\r\nOth6%\r\nBG20%Car\r\nLoc30%\r\nSim37%\r\nOth19%\r\nBG14%Motorbike\r\nLoc21%Sim16%\r\nOth40%BG23%Furniture\r\nLoc31%Sim32%\r\nOth13%BG24%\r\nVehicles\r\nLoc22%Sim63%\r\nOth6%BG9%Animals\r\nLoc50%\r\nSim11%Oth\r\n15%BG24%\r\nPerson\r\nLoc26%\r\nSim39%\r\nOth27%\r\nBG8%Motorbike\r\nLoc20%\r\nSim66%\r\nOth3%\r\nBG11%Cow\r\nLoc24%Sim31%\r\nOth12%BG33%\r\nCar\r\ndog horse\r\n \r\nsheep\r\nbus\r\nbicycle\r\ndog horse\r\n \r\nsheep\r\nmbike\r\nbicycle\r\nFig. 2. Analysis of Top-Ranked False Positives.Pie charts: fraction of top-ranked false positives\r\nthat are due to poor localization (Loc), confusion with similar objects (Sim), confusion with other\r\nVOC  objects  (Oth),  or  confusion  with  background  or  unlabeled  objects  (BG). Each  category\r\nnamed within ‘Sim’ is the source of at least 10% of the top false positives. Bar graphs display\r\nabsolute AP improvement by removing all false positives of one type (‘B’ removes confusion\r\nwith  background  and  non-similar  objects). ’L’:  the  first  bar  segment  displays  improvement  if\r\nduplicate or poor localizations are removed; the second displays improvement if the localization\r\nerrors were corrected, turning false detections into true positives. fusion with similar categories are both common.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6zb+9M0AoFvMH6SOyDK8l8h2nYa2GmZH1fWF1KXxQ7I="},"78b15ac1-8055-4784-8ab4-66370a12af18":{"id_":"78b15ac1-8055-4784-8ab4-66370a12af18","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"n3naMl4LZptMIuSITsBnJN1OAV0MbQmqfWdKgXM6cY4="},"PREVIOUS":{"nodeId":"59ea4c24-efce-45c9-8c55-a77cd5bc850b","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"6zb+9M0AoFvMH6SOyDK8l8h2nYa2GmZH1fWF1KXxQ7I="}},"text":"fusion with similar categories are both common. In looking at trends of false positives\r\nwith increasing rank, localization errors and confusion with similar objects tend to be\r\nmore common among the top-ranked than the lower-ranked false positives. Confusion\r\nwith “other” objects and confusion with background may be similar types of errors. Some categories were often confused with semantically dissimilar categories. For ex-\r\nample, bottles were often confused with people, due to the similarity of exterior con-\r\ntours. Removing only one type of false positives may have a small effect, due to the\r\nT PT P+F Pform of precision. In particular, the potential improvement by removing all\r\nbackground false detections is surprisingly small (e.g., 0.02 AP for animals, 0.04 AP\r\nfor vehicles). Improvements in localization or differentiating between similar categories\r\nwould lead to the largest gains. If poor localizations were corrected, e.g. with an effec-\r\ntive  category-based  segmentation  method,  performance  would  improve  greatly  from\r\nadditional high-confidence true positives, as well as fewer false positives. 3    False Negatives and Impact of Object Characteristics\r\nDetectors may incur a false negative by assigning a low confidence to an object or by\r\nmissing it completely. Intuitively, an object may be difficult to detect due to occlusion,\r\ntruncation, small size, or unusual viewpoint. In this section, we measure the sensitivity\r\nof detectors to these characteristics and others and also try to answer why so many\r\nobjects (typically about 40%) are not detected with even very low confidence.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DNcjbN0wClEZuazIow8eBZDQ0g6fhmZqkKe7SJSA4Z8="},"c8ae5955-43e6-4bd9-aef6-7e8635fd6d85":{"id_":"c8ae5955-43e6-4bd9-aef6-7e8635fd6d85","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"RaD4/MY86okY/A0l1MKguBYrSrp1DImwfFX1574zIIo="},"NEXT":{"nodeId":"ca38d417-98ed-459f-bcfa-356b91863eda","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"NY7z4rwlSfD8MwTtQESlTizAsFRe1Gs6CXdU0Ibydss="}},"text":"6             Diagnosing Error in Object Detectors\r\nNone \r\nLow \r\nMedium \r\nHigh \r\nFig. 3.Example of 4 levels of occlusion for the aeroplane class. 3.1    Definitions of Object Characteristics\r\nTo perform our study, we added annotations to the PASCAL VOC dataset, including\r\nlevel of occlusion and which sides and parts of an object are visible. We also use stan-\r\ndard annotations, such as the bounding box. We created the extra annotations for seven\r\ncategories (‘aeroplane’, ‘bicycle’, ‘bird’, ‘boat’, ‘cat’, ‘chair’, ‘diningtable’) that span\r\nthe major groups of vehicles, animals, and furniture. Annotations were created by one\r\nauthor to ensure consistency and are publicly available. Object sizeis measured as the pixel area of the bounding box. We also considered\r\nbounding box height as a size measure, which led to similar conclusions. We assign\r\neach object to a size category, depending on the object’s percentile size within its object\r\ncategory: extra-small (XS: bottom 10%); small (S: next 20%); medium (M: next 40%);\r\nlarge (L: next 20%); extra-large (XL: next 10%).Aspect ratiois defined as object width\r\ndivided by object height, computed from the VOC bounding box annotation. Similarly\r\nto object size, objects are categorized into extra-tall (XT), tall (T), medium (M), wide\r\n(W), and extra-wide (XW), using the same percentiles.Occlusion(part of the object\r\nis obscured by another surface) andtruncation(part of the object is outside the im-\r\nage) have binary annotations in the standard VOC annotation. We replace the occlusion\r\nlabels with degree of occlusion (see Fig. 3): ‘None’, ‘Low’ (slight occlusion), ‘Moder-\r\nate’ (significant part is occluded), and ‘High’ (many parts missing or 75% occluded). Visibility of partsinfluences detector performance, so we add annotations for whether\r\neach part of an object is visible. We annotateviewpointas whether each side (‘bottom’,\r\n‘front’, ‘top’, ‘side’, ‘rear’) is visible.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"diZ5rGDzWExcLDc18HmMsBmbzEp4o8g7Y2GuLJYtUwo="},"ca38d417-98ed-459f-bcfa-356b91863eda":{"id_":"ca38d417-98ed-459f-bcfa-356b91863eda","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"RaD4/MY86okY/A0l1MKguBYrSrp1DImwfFX1574zIIo="},"PREVIOUS":{"nodeId":"c8ae5955-43e6-4bd9-aef6-7e8635fd6d85","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"diZ5rGDzWExcLDc18HmMsBmbzEp4o8g7Y2GuLJYtUwo="}},"text":"For example, an object seen from the front-right\r\nmay be labeled as ‘bottom’=0, ‘front’=1, ‘top’=0, ‘side’=1, ‘rear’=0. 3.2    Normalized Precision Measure\r\nTo analyze sensitivity to object characteristics, we would like to summarize the per-\r\nformance  of  different  subsets  (e.g. small  vs. large)  of  objects. Current  performance\r\nmeasures do not suffice: ROC curves are difficult to summarize, and average precision\r\nis sensitive to the number of positive examples. We propose a simple way to normal-\r\nize precision so that we can easily measure and compare performance for objects with\r\nparticular characteristics. The  standard  PASCAL  VOC  measure  is  average  precision  (AP),  which  summarizes\r\nprecision-recall curves with the average interpolated precision value of the positive ex-\r\namples. RecallR(c)is the fraction of objects detected with confidence of at leastc.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NY7z4rwlSfD8MwTtQESlTizAsFRe1Gs6CXdU0Ibydss="},"7f5da26d-77a0-4a9a-992f-4dbbbbb6d1f0":{"id_":"7f5da26d-77a0-4a9a-992f-4dbbbbb6d1f0","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"+i1lWuUtbJdNFBvgUBo3ygqt0TkfaEAOop+Z5vP/hzo="},"NEXT":{"nodeId":"93cb06c8-9eac-42f7-9e7d-e00fbf4b6e0b","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"5eK9tCQW0iqCDWyumztGqvAnvRjq5u8Se7whiguC3bs="}},"text":"Diagnosing Error in Object Detectors             7\r\naero     bike     boat     bus       car      mbike     trainbird      cat      cow     dog     horse     sheepbottle     chair     table     plant     sofa       tvpersNum Objs285      337      263      213     1201       325       282459      358      244      489      348       242469       756      206      480      239      3084528\r\nVGVZAP.364     .468     .113     .513     .508       .450      .447.106     .277     .312     .174     .512      .208.193      .131     .191     .073     .266     .477.200APN.443     .531     .181     .612     .480       .511      .527.136     .372     .418     .222     .565      .294.222      .130     .318     .094     .294     .567.073\r\nFGMRAP.277     .581     .134     .481     .555       .475      .436.028     .149     .214     .034     .582      .154.225      .191     .204     .068     .306     .405.410APN.343     .631     .187     .574     .517      0.537     .528.039     .207     .311     .048     .631      .215.252      .190     .346     .086     .409     .477.214\r\nTable 1.Detection Results on PASCAL VOC2007 Dataset. For each object category, we show the\r\ntotal number of objects and the average precision (AP) and average normalized precision (APN)\r\nfor  the  VGVZ  and  FGMR  detectors. For  APN,  the  precision  is  normalized  to  be  comparable\r\nacross categories. Categories with many positive examples (e.g., “person”) will have lower APN\r\nthan AP; the reverse is true for categories with few examples.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"i7A+r3kxSqm6ldtyCxfB3uoAW7LZKnOyTpQOyrrc1dU="},"93cb06c8-9eac-42f7-9e7d-e00fbf4b6e0b":{"id_":"93cb06c8-9eac-42f7-9e7d-e00fbf4b6e0b","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"+i1lWuUtbJdNFBvgUBo3ygqt0TkfaEAOop+Z5vP/hzo="},"PREVIOUS":{"nodeId":"7f5da26d-77a0-4a9a-992f-4dbbbbb6d1f0","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"i7A+r3kxSqm6ldtyCxfB3uoAW7LZKnOyTpQOyrrc1dU="}},"text":"PrecisionP(c)is the fraction of detections that are correct:\r\nP(c) =R(c)\u0001NjR(c)\u0001N\r\nj+F(c)\r\n(1)\r\nwhereNjis the number of objects in classjandF(c)is the number of incorrect detec-\r\ntions with at leastcconfidence. Before computing AP, precision is “interpolated”, such\r\nthat the interpolated precision value atcis the maximum precision value for any exam-\r\nple with at least confidencec. IfNjis large (such as for pedestrians), then the precision\r\nwould be higher than ifNjwere small for the same detection rate. This sensitivity to\r\nNjinvalidates AP comparisons for different sets of objects. For example, we cannot use\r\nAP to determine whether people are easier to detect than cows, or whether big objects\r\nare easier to detect than small ones. We propose to replaceNjwith a constantNto create a normalized precision:\r\nPN(c) =R(c)\u0001NR(c)\u0001N+F(c):(2)\r\nIn our experiments, we setN= 0:15Nimages= 742:8, which is roughly equal to the\r\naverageNjover the PASCAL VOC categories. Detectors with similar detection rates\r\nand false positive rates will have similar normalized precision-recall curves. We can\r\nsummarize normalized precision-recall by averaging the normalized precision values of\r\nthe positive examples to create average normalized precision (APN). When computing\r\nAPN, undetected objects are assigned a precision value of 0. We compare AP and APN\r\nin Table 1. 3.3    Analysis\r\nTo understand performance for a category, it helps to inspect the performance variations\r\nfor each characteristic. We include this detailed analysis primarily as an example of how\r\nresearchers can use our tool to understand the strengths and weaknesses of their own\r\ndetectors. Upon careful inspection of Figure 4, for example, we can learn the following\r\nabout airplane detectors: both detectors perform similarly, preferring similar subsets\r\nof non-occluded, non-truncated, medium to extra-wide, side views in which all major\r\nparts are visible. Performance for very small and heavily occluded airplanes is poor,","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5eK9tCQW0iqCDWyumztGqvAnvRjq5u8Se7whiguC3bs="},"72afb688-e244-44df-b142-6e4a9c11510c":{"id_":"72afb688-e244-44df-b142-6e4a9c11510c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"ONG8l1fqZMf4uWgHxXdKLHcBKLkoROttHi2BfI5z7Ak="},"NEXT":{"nodeId":"acdb9bb2-96fa-4055-b2d9-308f265ab3c7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"oEeWJAAiOyAogu/y+klalIuhn6NM+hlx/wUliC2jhMI="}},"text":"8             Diagnosing Error in Object Detectors\r\nNLMHNTXSSMLXLXTTMWXW                  0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.49\r\n0.230.08\r\n0.00\r\nOcclusn.VGVZ 2009: aeroplane\r\n0.480.34\r\nTrnc. 0.02\r\n0.350.47\r\n0.76\r\n0.35\r\nBBox Area\r\n0.11\r\n0.380.500.49\r\n0.60\r\nAspect Rat. 0.470.380.48\r\n0.290.450.360.18\r\n0.520.440.44\r\nSides Visiblebttm0/1front0/1rear0/1side0/1\r\ntop0/1\r\n0.10\r\n0.47\r\n0.25\r\n0.47\r\n0.12\r\n0.50\r\n0.12\r\n0.46\r\nParts Visiblebody0/1head0/1tail0/1wing0/1\r\nNLMHNTXSSMLXLXTTMWXW                  0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.370.23\r\n0.080.03\r\nOcclusn.FGMR (v4): aeroplane\r\n0.38\r\n0.22\r\nTrnc. 0.000.10\r\n0.48\r\n0.64\r\n0.11\r\nBBox Area\r\n0.250.120.33\r\n0.48\r\n0.65Aspect Rat.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"lmyDYpq/nzqk/qLaVA0ctjCCD/ayBmr7IQLgYGnvNyY="},"acdb9bb2-96fa-4055-b2d9-308f265ab3c7":{"id_":"acdb9bb2-96fa-4055-b2d9-308f265ab3c7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"ONG8l1fqZMf4uWgHxXdKLHcBKLkoROttHi2BfI5z7Ak="},"PREVIOUS":{"nodeId":"72afb688-e244-44df-b142-6e4a9c11510c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"lmyDYpq/nzqk/qLaVA0ctjCCD/ayBmr7IQLgYGnvNyY="},"NEXT":{"nodeId":"6afc8e68-200b-441e-a78f-e567138cddd3","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"FxHsOhhsIyd+HhLAdig2slVtcFJFY5rINS9bkNk9FnI="}},"text":"0.40\r\n0.20\r\n0.40\r\n0.15\r\n0.350.32\r\n0.08\r\n0.420.350.29\r\nSides Visiblebttm0/1front0/1rear0/1side0/1\r\ntop0/1\r\n0.03\r\n0.37\r\n0.13\r\n0.37\r\n0.09\r\n0.39\r\n0.16\r\n0.36\r\nParts Visiblebody0/1head0/1tail0/1wing0/1\r\nNLMHNTXSSMLXLXTTMWXW                    0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.410.32\r\n0.210.09\r\nOcclusn.VGVZ 2009: cat\r\n0.370.38\r\nTrnc. 0.010.15\r\n0.45\r\n0.600.53\r\nBBox Area\r\n0.15\r\n0.330.450.410.32\r\nAspect Rat. 0.380.330.360.390.380.280.350.380.380.34\r\nSides Visiblebttm0/1front0/1rear0/1side0/1\r\ntop0/1\r\n0.440.370.310.38\r\n0.270.390.400.350.400.33\r\nParts Visiblebody0/1ear0/1face0/1leg0/1\r\ntail0/1\r\nNLMHNTXSSMLXLXTTMWXW                    0\r\n0.10.2\r\n0.30.4\r\n0.50.6\r\n0.230.17\r\n0.120.11\r\nOcclusn.FGMR (v4): cat\r\n0.160.27\r\nTrnc. 0.00\r\n0.160.21\r\n0.310.40\r\nBBox Area\r\n0.140.22\r\n0.28\r\n0.150.11\r\nAspect Rat.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oEeWJAAiOyAogu/y+klalIuhn6NM+hlx/wUliC2jhMI="},"6afc8e68-200b-441e-a78f-e567138cddd3":{"id_":"6afc8e68-200b-441e-a78f-e567138cddd3","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"ONG8l1fqZMf4uWgHxXdKLHcBKLkoROttHi2BfI5z7Ak="},"PREVIOUS":{"nodeId":"acdb9bb2-96fa-4055-b2d9-308f265ab3c7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"oEeWJAAiOyAogu/y+klalIuhn6NM+hlx/wUliC2jhMI="},"NEXT":{"nodeId":"474c583e-bd21-40ff-986f-597c3745940b","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"77NgkBbLuB8rVgX+b9+S84Yi1ThukXIFE+Jw57KS35c="}},"text":"0.220.130.150.290.22\r\n0.07\r\n0.250.200.210.15\r\nSides Visiblebttm0/1front0/1rear0/1side0/1\r\ntop0/1\r\n0.240.21\r\n0.120.210.05\r\n0.250.310.150.26\r\n0.12\r\nParts Visiblebody0/1ear0/1face0/1leg0/1\r\ntail0/1\r\nFig. 4. Per-Category Analysis of Characteristics:APN(’+’) with standard error bars (red). Black dashed lines indicate overall APN. Key:Occlusion: N=none; L=low; M=medium; H=high. Truncation:  N=not  truncated;  T=truncated.Bounding Box Area:  XS=extra-small;  S=small;\r\nM=medium; L=large; XL =extra-large.Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium;\r\nW=wide; XW =extra-wide.Part Visibility / Viewpoint: ’1’=part/side is visible; ’0’=part/side is\r\nnot visible. Standard error is used for the average precision statistic as a measure of significance,\r\nrather than confidence bounds, due to the difficulty of modeling the precision distribution. though performance on non-occluded airplanes is near average (because few airplanes\r\nare heavily occluded). FGMR shows a stronger preference for exact side-views than\r\nVGVZ, which may also account for differences in performance on small and extra-\r\nlarge objects, which are both less likely to be side views. We can learn similar things\r\nabout the other categories. For example, both detectors work best for large cats, and\r\nFGMR performs best with truncated cats, where all parts except the face and ears are\r\nnot visible. Note that both detectors seem to vary in similar ways, indicating that their\r\nsensitivities may be due to some objects being intrinsically more difficult to recognize. The VGVZ cat detector is less sensitive to viewpoint and part visibility, which may be\r\ndue to its textural bag of words features. Since size and occlusion are such important characteristics, we think it worthwhile to\r\nexamine their effects across several categories (Fig. 5).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FxHsOhhsIyd+HhLAdig2slVtcFJFY5rINS9bkNk9FnI="},"474c583e-bd21-40ff-986f-597c3745940b":{"id_":"474c583e-bd21-40ff-986f-597c3745940b","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"ONG8l1fqZMf4uWgHxXdKLHcBKLkoROttHi2BfI5z7Ak="},"PREVIOUS":{"nodeId":"6afc8e68-200b-441e-a78f-e567138cddd3","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"FxHsOhhsIyd+HhLAdig2slVtcFJFY5rINS9bkNk9FnI="}},"text":"5). Typically, detectors work best\r\nfor non-occluded objects, but when objects are frequently occluded (bicycle, chair, din-\r\ningtable) there is sometimes a small gain in performance for lightly occluded objects. Although detectors are bad at detecting medium-heavy occluded objects, the impact on\r\noverall performance is small. Researchers working on the important problem of occlu-\r\nsion robustness should be careful to examine effects within the subset of occluded ob-\r\njects. Both detectors tend to prefer medium to large objects (the 30th to 90th percentile\r\nin area). The difficulty with small objects is intuitive. The difficulty with extra-large ob-\r\njects may initially surprise, but qualitative analysis (e.g., Fig. 7) shows that large objects\r\nare often highly truncated or have unusual viewpoints. Fig. 6  provides  a  compact  summary  of  the  sensitivity  to  each  characteristic  and  the\r\npotential impact of improving robustness. The worst-performing and best-performing\r\nsubsets for each characteristic are averaged over 7 categories: the difference between\r\nbest and worst indicates sensitivity; the difference between best and overall indicates","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"77NgkBbLuB8rVgX+b9+S84Yi1ThukXIFE+Jw57KS35c="},"9789e821-0a8e-4e9b-b66f-fed37a16c33c":{"id_":"9789e821-0a8e-4e9b-b66f-fed37a16c33c","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"95PCp3zBOsMda3rXGUCpVxoNM2Qo37l0WwvJ+qZ/63k="},"NEXT":{"nodeId":"b6be192f-ef7d-4aa3-a30f-1d4506a154cd","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"Kz9JASd+GbUzti6s8E+KTB5JCoPqu7o1p6ICuE1O9HE="}},"text":"Diagnosing Error in Object Detectors             9\r\nNLMHNLMHNLMHNLMHNLMHNLMHNLMH0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.49\r\n0.23\r\n0.080.00\r\n0.560.65\r\n0.140.170.150.120.020.000.210.110.120.05\r\n0.410.32\r\n0.210.090.220.14\r\n0.040.020.10\r\n0.350.330.25\r\nairplane           bicycle             bird                  boat                 cat                   chair                tableVGVZ 2009: Occlusion\r\nNLMHNLMHNLMHNLMHNLMHNLMHNLMH0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.370.23\r\n0.080.03\r\n0.680.72\r\n0.27\r\n0.46\r\n0.050.020.000.000.220.130.080.00\r\n0.230.170.120.110.240.210.12\r\n0.030.15\r\n0.290.420.36\r\nairplane           bicycle             bird                  boat                 cat                   chair                tableFGMR (v4): Occlusion\r\nXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXL0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.02\r\n0.350.47\r\n0.76\r\n0.35\r\n0.14\r\n0.500.560.620.66\r\n0.000.010.11\r\n0.410.29\r\n0.010.11\r\n0.220.330.12\r\n0.010.15\r\n0.450.600.53\r\n0.010.09\r\n0.210.17\r\n0.020.00\r\n0.350.450.37\r\n0.11\r\nairplane           bicycle             bird                  boat                 cat                   chair                tableVGVZ 2009: BBox Area\r\nXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXLXSSMLXL0\r\n0.2\r\n0.4\r\n0.6\r\n0.8\r\n0.000.10\r\n0.480.64\r\n0.110.20\r\n0.770.600.690.74\r\n0.000.020.050.100.020.000.06\r\n0.22\r\n0.43\r\n0.140.000.160.210.31\r\n0.40\r\n0.010.17\r\n0.260.26\r\n0.050.030.12\r\n0.520.47\r\n0.32\r\nairplane           bicycle             bird                  boat                 cat                   chair                tableFGMR (v4): BBox Area\r\nFig.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jhMI7VXPvp/ROBYRka0BrQyV3xegBLD51+eI1gGTtis="},"b6be192f-ef7d-4aa3-a30f-1d4506a154cd":{"id_":"b6be192f-ef7d-4aa3-a30f-1d4506a154cd","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"95PCp3zBOsMda3rXGUCpVxoNM2Qo37l0WwvJ+qZ/63k="},"PREVIOUS":{"nodeId":"9789e821-0a8e-4e9b-b66f-fed37a16c33c","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"jhMI7VXPvp/ROBYRka0BrQyV3xegBLD51+eI1gGTtis="},"NEXT":{"nodeId":"c652abff-f60c-4fde-a2fc-fedee3426386","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"zTurFXPV6W+Btt945n8Rp5nkzhH6t6RCWZP+yvCPMAc="}},"text":"5. Sensitivity and Impact of Object Characteristics:APN(’+’) with standard error bars\r\n(red). Black dashed lines indicate overall APN. Key:Occlusion: N=none; L=low; M=medium;\r\nH=high.Bounding Box Area: XS=extra-small; S=small; M=medium; L=large; XL =extra-large. occtrnsizeaspviewpart0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\n0.5\r\n0.6\r\n0.098\r\n0.356\r\n0.213\r\n0.361\r\n0.026\r\n0.488\r\n0.120\r\n0.406\r\n0.187\r\n0.373\r\n0.105\r\n0.3820.302\r\nVGVZ 2009: Sensitivity and Impact\r\nocctrnsizeaspviewpart0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\n0.5\r\n0.6\r\n0.100\r\n0.278\r\n0.153\r\n0.288\r\n0.060\r\n0.419\r\n0.074\r\n0.403\r\n0.065\r\n0.301\r\n0.069\r\n0.324\r\n0.238\r\nFGMR (v2): Sensitivity and Impact\r\nocctrnsizeaspviewpart0\r\n0.1\r\n0.2\r\n0.3\r\n0.4\r\n0.5\r\n0.6\r\n0.103\r\n0.323\r\n0.201\r\n0.323\r\n0.060\r\n0.445\r\n0.121\r\n0.435\r\n0.110\r\n0.375\r\n0.082\r\n0.362\r\n0.277\r\nFGMR (v4): Sensitivity and Impact\r\nFig. 6. Summary of Sensitivity and Impact of Object Characteristics:We show the average\r\n(over  categories)  APNperformance  of  the  highest  performing  and  lowest  performing  subsets\r\nwithin each characteristic (occlusion, truncation, bounding box area, aspect ratio, viewpoint, part\r\nvisibility). Overall APNis indicated by the black dashed line. The difference between max and\r\nmin indicates sensitivity; the difference between max and overall indicates the impact. potential impact.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Kz9JASd+GbUzti6s8E+KTB5JCoPqu7o1p6ICuE1O9HE="},"c652abff-f60c-4fde-a2fc-fedee3426386":{"id_":"c652abff-f60c-4fde-a2fc-fedee3426386","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"95PCp3zBOsMda3rXGUCpVxoNM2Qo37l0WwvJ+qZ/63k="},"PREVIOUS":{"nodeId":"b6be192f-ef7d-4aa3-a30f-1d4506a154cd","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"Kz9JASd+GbUzti6s8E+KTB5JCoPqu7o1p6ICuE1O9HE="}},"text":"potential impact. For example, detectors are very sensitive to occlusion and truncation,\r\nbut the impact is small (about 0.05 APN) because the most difficult cases are not com-\r\nmon. Object area and aspect have a much larger impact (roughly 0.18 for area, 0.13\r\nfor aspect). Overall, VGVZ is more robust than FGMR to viewpoint and part visibility,\r\nlikely because it is better at encoding texture (due to bags of words features), while\r\nFGMR can accommodate only limited deformations. The performance of VGVZ varies\r\nmore than FGMR with object size. Efforts to improve occlusion or viewpoint robust-\r\nness should be validated with specialized analysis so that improvements are not diluted\r\nby the more common easier cases. One  of  the  aims  of  our  study  is  to  understand  why  current  detectors  fail  to  detect\r\n30\u000040%of objects, even at small confidence thresholds. We consider an object to\r\nbe undetected if there is no detection above 0.05 PN(roughly 1.5 FP/image). Our anal-\r\nysis indicates that size is the best single explanation, as nearly all extra-small objects\r\ngo undetected and roughly half of undetected objects are in the smallest30%. But size\r\naccounts for only 20% of the entropy of whether an object is detected (measured by\r\ninformation gain divided by entropy). Contrary to intuition, occlusion does not increase\r\nthe chance of going undetected (though it decreases the expected confidence); there is","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"zTurFXPV6W+Btt945n8Rp5nkzhH6t6RCWZP+yvCPMAc="},"25682033-a746-4a42-9687-51ac9494af3d":{"id_":"25682033-a746-4a42-9687-51ac9494af3d","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"7M6RnpHcwCFUmkyFjzVYy7zE6XCApw1sDsNmGttr01A="},"NEXT":{"nodeId":"9ff490d7-6cc6-41a2-94f3-d48f5c2d7de6","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"UeG8UIE/7pIU1v+j3bp0kBTEP/wmhsu2ihQ8OkevZHc="}},"text":"10           Diagnosing Error in Object Detectors\r\n0.136\r\n0.6540.0210.021\r\n0.6540.0300.0300.654\r\n0.0910.0910.6730.8760.538\r\n0.0260.0260.364\r\n0.008\r\n0.0320.3640.1240.3190.5940.3190.0120.0120.301\r\n0.1150.403\r\n0.0560.3970.030\r\n0.386\r\n0.0400.0400.386\r\n0.0200.0200.323\r\nFig. 7. Unexpectedly Difficult Detections:We fit a linear regressor to predict confidence (PN)\r\nbased on size, aspect ratio, occlusion and truncation for the FGMR (v4) detector. We show 5\r\nof the 15 objects with the greatest difference between predicted confidence and actual detection\r\nconfidence. The ground truth object is in red, with predicted confidence in upper-right corner\r\nin italics. The other box shows the highest scoring correct detection (green), if any, or highest\r\nscoring overlapping detection (blue dashed) with the detection confidence in the upper-left corner. also only a weak correlation with aspect ratio. As can be seen in Figure 7, misses are\r\ndue to a variety of factors, such as unusual appearance, unusual viewpoints, in-plane\r\nrotation, and particularly disguising occlusions. Some missed detections are actually\r\ndetected with high confidence but poor localization. With a 10% overlap criteria, the\r\nnumber of missed detections is reduced by 40-85% (depending on category, detector). Conclusions and Discussion:As expected, objects that are small, heavily occluded,\r\nseen from unusual views (e.g., the bottom), or that have important occluded parts are\r\nhard  to  detect. But  the  deviations  are  interesting:  slightly  occluded  bicycles  and  ta-\r\nbles are easier; detectors often have trouble with very large objects; cats are easiest for\r\nFGMR when only the head is visible. Despite high sensitivity, the overall impact of oc-\r\nclusion and many other characteristics is surprisingly small. Even if the gap between no\r\nocclusions and heavy occlusions were completely closed, the overall AP gain would be\r\nonly a few percent.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"TuCOCIgkgzKhbgm9ugIX+16VRrtO7SMo6gwElELE+QU="},"9ff490d7-6cc6-41a2-94f3-d48f5c2d7de6":{"id_":"9ff490d7-6cc6-41a2-94f3-d48f5c2d7de6","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"7M6RnpHcwCFUmkyFjzVYy7zE6XCApw1sDsNmGttr01A="},"PREVIOUS":{"nodeId":"25682033-a746-4a42-9687-51ac9494af3d","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"TuCOCIgkgzKhbgm9ugIX+16VRrtO7SMo6gwElELE+QU="}},"text":"Also, note that smaller objects, heavily occluded objects, and those\r\nseen from unusual viewpoints will be intrinsically more difficult, so we cannot expect\r\nto close the gap completely. We can also see the impact of design differences for two detectors. The VGVZ detec-\r\ntor incorporates more textural features and also has a more flexible window proposal\r\nmethod. These detector properties are advantageous for detecting highly deformable\r\nobjects, such as cats, and they lead to reduced sensitivity to part visibility and view-\r\npoint. However, VGVZ is more sensitive to size (performing better for large objects\r\nand worse for small ones) because the bag-of-words models are size-dependent, while\r\nthe FGMR templates are not. Comparing the FGMR (v2) and (v4) detectors in Figure 6,\r\nwe see that the additional components and left/right latent flip term lead to improved\r\nrobustness (improvement in both worst and best cases) to aspect ratio, viewpoint, and\r\ntruncation.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"UeG8UIE/7pIU1v+j3bp0kBTEP/wmhsu2ihQ8OkevZHc="},"0c22ff33-4afa-422d-a6b4-034d78df7830":{"id_":"0c22ff33-4afa-422d-a6b4-034d78df7830","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"l1emvUE2/jq8LCshjS3J4HUBpqMKEMec1t9s8A9Wo0w="},"NEXT":{"nodeId":"a59e8c00-9178-4028-90c6-5a81785cc40a","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"6G9Nia/ppfTO95kUfhhzkag1cpsFDrMX7kcZhLph5Ag="}},"text":"Diagnosing Error in Object Detectors           11\r\n4    Conclusion\r\n4.1    Diagnosis\r\nWe have analyzed the patterns of false and missed detections made by currently top-\r\nperforming detectors. The good news is that egregious errors are rare. Most false posi-\r\ntives are due to misaligned detection windows or confusion with similar objects (Fig. 2). Most missed detections are atypical in some way – small, occluded, viewed from an\r\nunusual  angle,  or  simply  odd  looking. Detectors  seem  to  excel  at  latching  onto  the\r\ncommon modes of object appearance and avoiding confusion with miscellaneous back-\r\nground patches. For example, detectors more easily detect lightly occluded objects for\r\ncategories  that  are  typically  occluded,  such  as  bicycles  and  tables. Airplanes  in  the\r\nstereotypical direct side-view are detected by FGMR with almost double the accuracy\r\nof the average airplane. Further progress, however, is likely to be slow, incremental, and to require careful vali-\r\ndation. Localization error is not easily handled by detectors because determining object\r\nextent often requires looking well outside the window. A box around a cat face could\r\nbe a perfect detection in one image but only a small part of the visible cat in another\r\n(Figs. 1, 8). Gradient histogram-based models, designed to accommodate moderate po-\r\nsitional slop, may be too coarse for differentiation of similar objects, such as cows and\r\nhorses or cats and dogs. Although detectors fare well for common views of objects,\r\nthere are many types of deviations that may require different solutions. Solving only\r\none problem will lead to small gains. For example, even if occluded objects could be\r\ndetected as easily as unoccluded ones, the overall AP would increase by 0.05 or less\r\n(Fig. 6). Better detection of small objects could yield large gains (roughly 0.17 AP), but\r\nthe smallest objects are intrinsically difficult to detect. Our biggest concern is that suc-\r\ncessful attempts to address problems of occlusion, size, localization, or other problems\r\ncould look unpromising if viewed only through the lens of overall performance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oFzYLl0IC/i6pCrgqk6ZwbczxvfRxdhQr9n4GGv/NRs="},"a59e8c00-9178-4028-90c6-5a81785cc40a":{"id_":"a59e8c00-9178-4028-90c6-5a81785cc40a","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"l1emvUE2/jq8LCshjS3J4HUBpqMKEMec1t9s8A9Wo0w="},"PREVIOUS":{"nodeId":"0c22ff33-4afa-422d-a6b4-034d78df7830","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"oFzYLl0IC/i6pCrgqk6ZwbczxvfRxdhQr9n4GGv/NRs="}},"text":"There\r\nmay be a gridlock in recognition research: new approaches that do not immediately ex-\r\ncel at detecting objects typical of the standard datasets may be discarded before they\r\nare fully developed. One way to escape that gridlock is through more targeted efforts\r\nthat specifically evaluate gains along the various dimensions of error. 4.2    Recommendations\r\nDetecting Small/Large Objects:An object’s size is a good predictor of whether it will\r\nbe detected. Small objects have fewer visual features; large objects often have unusual\r\nviewpoints or are truncated. Current detectors make a weak perspective assumption that\r\nan object’s appearance does not depend on its distance to the camera. This assumption\r\nis violated when objects are close, making the largest objects difficult to detect. When\r\nobjects are far away, only low resolution template models can be applied, and contextual\r\nmodels may be necessary for disambiguation. There are interesting challenges in how\r\nto benefit from the high resolution of nearby objects while being robust to near-field\r\nperspective effects. Park et al. [24] offer one simple approach of combining detectors","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6G9Nia/ppfTO95kUfhhzkag1cpsFDrMX7kcZhLph5Ag="},"a6b2c359-4dbb-493e-88d8-2ef9775427c2":{"id_":"a6b2c359-4dbb-493e-88d8-2ef9775427c2","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"2Jogkk/mZF9rUeA5mgAPyMtOlj5Qkhs6d04ONVRpHZc="}},"text":"12           Diagnosing Error in Object Detectors\r\nRight Wrong \r\nDog Model \r\nRight Wrong Incorrect Localization \r\nChallenges of Confusion with Similar Categories \r\nFig. 8. Challenging False Positives:In the top row, we show examples of dog detections that\r\nare considered correct or incorrect, according to the VOC localization criteria. On the right, we\r\ncropped out the rest of the image; it is not possible to determine correctness from within the win-\r\ndow alone. On the bottom, we show several confident dog detections, some of which correspond\r\nto objects from other similar categories. The robust HOG template detector (right), though good\r\nfor sorting through many background patches quickly, may be too robust for more fine differen-\r\ntiations. trained at different resolutions. Other possibilities include using scene layout to predict\r\nviewpoint of nearby objects, including features that take advantage of texture and small\r\nparts that are visible for close objects, and occlusion reasoning for distant objects. Improving  Localization:Difficulty  with  localization  and  identification  of  duplicate\r\ndetections has a large impact on recognition performance for many categories. In part,\r\nthe problem is that an template-based detectors cannot accommodate flexible objects. For example, the FGMR (v4) detector works very well for localizing cat heads, but not\r\ncat bodies [25]. Additionally, an object part, such as a head, could be considered correct\r\nif the body is occluded, or incorrect otherwise; it is impossible to tell from within the\r\nwindow alone (Fig. 8). Template detectors should play a major role in finding likely\r\nobject positions, but specialized processes are required for segmenting out the objects. In some applications, precise localization is unimportant; even so, the ability to identify\r\noccluding and interior object contours would also be useful for category verification,\r\nattribute recognition, or pose estimation. Current approaches to segment objects from\r\nknown categories (e.g., [8, 25]) tend to combine simple shape and/or color priors with\r\nmore generic segmentation techniques, such as graph cuts. There is an excellent oppor-\r\ntunity for a more careful exploration of the interaction of material, object shape, pose,\r\nand object category, through contour and texture-based inference. We encourage such\r\nwork to evaluate specifically in terms of improved localization of objects and to avoid\r\nconflating detection and localization performance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rrPUAwTQP1oC1vAC8C/YoQYIkhnxqS/E5cjH9NqSrwg="},"9c8cf7bf-1601-45e9-b7c3-85a820616e67":{"id_":"9c8cf7bf-1601-45e9-b7c3-85a820616e67","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"piay1Qykqm2JvjSKDaWqiDczk448HinFd84lYAzIbhY="},"NEXT":{"nodeId":"aed7453a-9643-48e3-b71b-85f493006630","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"sd+0TDNrmgSh3Pr3XHK3MoSp13itOyZloMjddu/Lq3Q="}},"text":"Diagnosing Error in Object Detectors           13\r\nReducing Confusion with Similar Categories:By far, the biggest task of detectors\r\nis to quickly discard random background patches, and they do well with features that\r\nare made robust to illumination, small shifts, and other variations. But a detector that\r\nsorts through millions of windows per second may not be suited for differentiating be-\r\ntween dogs and cats or horses and cows. Such differences require detailed comparison\r\nof particular features, such as the shape of the head or eyes. Some recent work has\r\naddressed fine-grained differentiation of birds and plants [26–28], and the ideas of find-\r\ning important regions for comparison may apply to category recognition as well. Also,\r\nwhile HOG features [29] are well-suited to whole-object detection, some of the feature\r\nrepresentations  originally  developed  for detection  of  small  faces  (e.g., [30])  may be\r\nbetter for differentiating similar categories based on their localized parts. We encourage\r\nsuch work to evaluate specifically on differentiating between similar objects. It may be\r\nworthwhile to pose the problem simply as categorizing a set of well-localized objects\r\n(even categorizing objects given PASCAL VOC bounding boxes is not easy). Robustness to Object Variation:One interpretation of our results is that existing de-\r\ntectors do well on the most common modes of appearance (e.g., side views of airplanes)\r\nbut fail when a characteristic, such as viewpoint, is unusual. Greatly improved recogni-\r\ntion will require appearance models that better encode shape with robustness to moder-\r\nate changes in viewpoint, pose, lighting, and texture. One approach is to learn feature\r\nrepresentations from paired 3D and RGB images; a second is to learn the natural vari-\r\nations of existing features within categories for which examples are plentiful and to\r\nextend that variational knowledge to other categories. More Detailed Analysis:Most important, we hope that our work will inspire research\r\nthat targets and evaluates reduction in specific modes of error. In the supplemental ma-\r\nterial, we include automatically generated reports for four detectors. Authors of future\r\npapers can use our tools to perform similar analysis, and the results can be compactly\r\nsummarized in1=4page, as shown in Figs. 2, 6.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5WSRSRtAl/3SVXPYSn1QnwgLZ0zOfU2Skp1/ZasU5Xw="},"aed7453a-9643-48e3-b71b-85f493006630":{"id_":"aed7453a-9643-48e3-b71b-85f493006630","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"piay1Qykqm2JvjSKDaWqiDczk448HinFd84lYAzIbhY="},"PREVIOUS":{"nodeId":"9c8cf7bf-1601-45e9-b7c3-85a820616e67","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"5WSRSRtAl/3SVXPYSn1QnwgLZ0zOfU2Skp1/ZasU5Xw="}},"text":"2, 6. We also encourage analysis of other\r\naspects of recognition, such as the effects of training sample size, cross-dataset gener-\r\nalization, cross-category generalization, and recognition of pose and other properties. References\r\n1. Griffin, G., Holub, A., Perona, P.:  Caltech-256 object category dataset. Technical Report\r\n7694, California Institute of Technology (2007)\r\n2. Everingham,  M.,  Van  Gool,  L.,  Williams,  C.K.I.,  Winn,  J.,  Zisserman,  A.:    The  PAS-\r\nCAL  Visual  Object  Classes  Challenge  2007  (VOC2007)  Results. (http://www.pascal-\r\nnetwork.org/challenges/VOC/voc2007/workshop/index.html)\r\n3. Vedaldi, A., Gulshan, V., Varma, M., Zisserman, A.:  Multiple kernels for object detection. In: ICCV. (2009)\r\n4. Felzenszwalb, P., McAllester, D., Ramanan, D.:  A discriminatively trained, multiscale, de-\r\nformable part model. In: CVPR. (2008)\r\n5. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object detection with discrim-\r\ninatively trained part based models. PAMI (2009)\r\n6. Wu, B., Nevatia, R.:  Detection of multiple, partially occluded humans in a single image by\r\nbayesian combination of edgelet part detectors. In: ICCV. (2005)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"sd+0TDNrmgSh3Pr3XHK3MoSp13itOyZloMjddu/Lq3Q="},"ea07cce3-a852-4d4c-8e96-00fa2b4b47f2":{"id_":"ea07cce3-a852-4d4c-8e96-00fa2b4b47f2","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"eNH4X7o24ZSDCCBSuZbr8h/OalYIlHHohBWx2Cy9w3w="},"NEXT":{"nodeId":"9e79b3d9-ebc6-4b82-aca7-b27f17bd5795","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"fS0CkLn6UO6TRLl3mkXRN+/867tF+NGUw+cyiHlKu00="}},"text":"14           Diagnosing Error in Object Detectors\r\n7. Wang, X., Han, T.X., Yan, S.:  An hog-lbp human detector with partial occlusion handling. In: ICCV. (2009)\r\n8. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.:  Layered object detection for multi-class\r\nsegmentation. In: CVPR. (2010)\r\n9. Vedaldi, A., Zisserman, A.: Structured output regression for detection with partial occulsion. In: NIPS. (2009)\r\n10. Kushal, A., Schmid, C., Ponce, J.: Flexible object models for category-level 3d object recog-\r\nnition. In: CVPR. (2007)\r\n11. Hoiem, D., Rother, C., Winn, J.:  3d layoutcrf for multi-view object class recognition and\r\nsegmentation. In: CVPR. (2007)\r\n12. Sun, M., Su, H., Savarese, S., Fei-Fei, L.:  A multi-view probabilistic model for 3d object\r\nclasses. In: CVPR. (2009)\r\n13. Schmid, C., Mohr, R., Bauckhage, C.: Evaluation of interest point detectors. IJCV37(2000)\r\n151–172\r\n14. Gil, A., Mozos, O.M., Ballesta, M., Reinoso, O.:  A comparative evaluation of interest point\r\ndetectors and local descriptors for visual slam. Machine Vision and Applications21(2009)\r\n905–920\r\n15. Divvala, S., Hoiem, D., Hays, J., Efros, A., Hebert, M.:  An empirical study of context in\r\nobject detection. In: CVPR. (2009)\r\n16. Rabinovich, A., Belongie, S.:  Scenes vs. objects: a comparative study of two approaches to\r\ncontext based recognition. In: Intl. Wkshp. on Visual Scene Understanding (ViSU). (2009)\r\n17.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"RUu6eH5CpOh/dgSBNZdeVY3ESl29TA8Fht89aOBm6LE="},"9e79b3d9-ebc6-4b82-aca7-b27f17bd5795":{"id_":"9e79b3d9-ebc6-4b82-aca7-b27f17bd5795","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"eNH4X7o24ZSDCCBSuZbr8h/OalYIlHHohBWx2Cy9w3w="},"PREVIOUS":{"nodeId":"ea07cce3-a852-4d4c-8e96-00fa2b4b47f2","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"RUu6eH5CpOh/dgSBNZdeVY3ESl29TA8Fht89aOBm6LE="},"NEXT":{"nodeId":"1e88c1a0-4ef5-462a-8c37-3ebcba0537ca","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"NA4A0MWkzQxvTQYT68ecnzkBmhvXZCdNrjCTi6g0K5s="}},"text":"on Visual Scene Understanding (ViSU). (2009)\r\n17. Pinto, N., Cox, D.D., DiCarlo, J.J.: Why is real-world visual object recognition hard? PLoS\r\nComputational Biology4(2008) e27\r\n18. Torralba, A., Efros, A.: Unbiased look at dataset bias. In: CVPR. (2011)\r\n19. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual\r\nobject classes (voc) challenge. IJCV88(2010) 303–338\r\n20. Dollar, P., Wojek, C., Schiele, B., Perona, P.: Pedestrian detection: A benchmark. In: CVPR. (2009)\r\n21. Sim, T., Baker, S., Bsat, M.:  The CMU Pose, Illumination, and Expression (PIE) database\r\nof human faces. Technical Report CMU-RI-TR-01-02, Carnegie Mellon, Robotics Institute\r\n(2001)\r\n22. Phillips, P.J., Flynn, P.J., Scruggs, T., Bowyer, K.W., Chang, J., Hoffman, K., Marques, J.,\r\nMin, J., Worek, W.: Overview of the face recognition grand challenge. (2005) 947–954\r\n23. Felzenszwalb, P.F., Girshick, R.B., McAllester, D.: Discriminatively trained deformable part\r\nmodels, release 4. (http://people.cs.uchicago.edu/ pff/latent-release4/)\r\n24. Park, D., Ramanan, D., Fowlkes, C.: Multiresolution models for object detection. In: ECCV. (2010)\r\n25. Parkhi, O., Vedaldi, A., Jawahar, C.V., Zisserman, A.:  The truth about cats and dogs. In:\r\nICCV. (2011)\r\n26. Shirdhonkar, S., White, S., Feiner, S., Jacobs, D., Kress, J., Belhumeur, P.N.:  Searching the\r\nworlds herbaria: A system for the visual identification of plant species.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"fS0CkLn6UO6TRLl3mkXRN+/867tF+NGUw+cyiHlKu00="},"1e88c1a0-4ef5-462a-8c37-3ebcba0537ca":{"id_":"1e88c1a0-4ef5-462a-8c37-3ebcba0537ca","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"eNH4X7o24ZSDCCBSuZbr8h/OalYIlHHohBWx2Cy9w3w="},"PREVIOUS":{"nodeId":"9e79b3d9-ebc6-4b82-aca7-b27f17bd5795","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Diagnosing_Error_in_Object_Detectors.pdf","file_name":"Diagnosing_Error_in_Object_Detectors.pdf"},"hash":"fS0CkLn6UO6TRLl3mkXRN+/867tF+NGUw+cyiHlKu00="}},"text":"In: ECCV. (2008)\r\n27. Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., Perona, P.:  Caltech-\r\nUCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology\r\n(2010)\r\n28. Khosla,  A.,  Yao,  B.,  Fei-Fei,  L.:   Combining  randomization  and  discrimination  for  fine-\r\ngrained image categorization. In: CVPR. (2011)\r\n29. Dalal, N., Triggs, B.:   Histograms of oriented gradients for human detection. In: CVPR. (2005)\r\n30. Schneiderman, H., Kanade, T.:  A statistical model for 3-d object detection applied to faces\r\nand cars. In: CVPR. (2000)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NA4A0MWkzQxvTQYT68ecnzkBmhvXZCdNrjCTi6g0K5s="},"af76c164-5d75-410e-82e7-d8d7cd34dfb8":{"id_":"af76c164-5d75-410e-82e7-d8d7cd34dfb8","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"bQPNreXts1ItTy91B+bQ1MoJL8+XRHvZkbRbeCI6CSo="},"NEXT":{"nodeId":"4a9f7d65-489d-4cd3-ae06-8325cf9979e8","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jvkc0acqBvcSeCMfuUPP57hxLST9hPK9gKUS+H+SYPk="}},"text":"Do We Need More Training Data? Xiangxin Zhu\u0001Carl Vondrick\u0001Charless C. Fowlkes\u0001Deva Ramanan\r\nAbstractDatasets for training object recognition sys-\r\ntems are steadily increasing in size. This paper inves-\r\ntigates the question of whether existing detectors will\r\ncontinue to improve as data grows, or saturate in perfor-\r\nmance due to limited model complexity and the Bayes\r\nrisk associated with the feature spaces in which they\r\noperate. We focus on the popular paradigm of discrimi-\r\nnatively trained templates de\fned on oriented gradient\r\nfeatures. We investigate the performance of mixtures of\r\ntemplates as the number of mixture components and\r\nthe amount of training data grows. Surprisingly, even\r\nwith proper treatment of regularization and \\outliers\",\r\nthe performance of classic mixture models appears to\r\nsaturate quickly (\u001810 templates and\u0018100 positive train-\r\ning examples per template). This is not a limitation of\r\nthe feature space as compositional mixtures that share\r\ntemplate parameters via parts and that can synthesize\r\nnew templates not encountered during training yield\r\nsigni\fcantly better performance. Based on our analy-\r\nsis, we conjecture that the greatest gains in detection\r\nperformance will continue to derive from improved rep-\r\nresentations  and  learning  algorithms  that  can  make\r\ne\u000ecient use of large datasets. KeywordsObject detection\u0001mixture models\u0001part\r\nmodels\r\nFunding for this research was provided by NSF IIS-0954083,\r\nNSF DBI-1053036, ONR-MURI N00014-10-1-0933, a Google\r\nResearch award to CF, and a Microsoft Research gift to DR. The \fnal publication is available at Springer via:\r\nhttp://dx.doi.org/10.1007/s11263-015-0812-2\r\nX. Zhu\u0001C. Fowlkes\u0001D. Ramanan\r\nDepartment of Computer Science, UC Irvine\r\nE-mail:fxzhu,fowlkes,dramanang@ics.uci.edu\r\nC. Vondrick\r\nCSAIL, MIT\r\nE-mail: vondick@mit.edu\r\n1 Introduction\r\nMuch of the impressive progress in object detection is\r\nbuilt on the methodologies of statistical machine learn-\r\ning, which make use of large training datasets to tune\r\nmodel parameters.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KxPjB3n26Jp6/di/d+1wR8scSNLCgLIR9yaB9/Qm8Fg="},"4a9f7d65-489d-4cd3-ae06-8325cf9979e8":{"id_":"4a9f7d65-489d-4cd3-ae06-8325cf9979e8","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"bQPNreXts1ItTy91B+bQ1MoJL8+XRHvZkbRbeCI6CSo="},"PREVIOUS":{"nodeId":"af76c164-5d75-410e-82e7-d8d7cd34dfb8","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"KxPjB3n26Jp6/di/d+1wR8scSNLCgLIR9yaB9/Qm8Fg="}},"text":"Consider the benchmark results of\r\nthe well-known PASCAL VOC object challenge (Fig. 1). There is a clear trend of increased benchmark perfor-\r\nmance over the years as new methods have been de-\r\nveloped. However, this improvement is also correlated\r\nwith increasing amounts of training data. One might be\r\ntempted to simply view this trend as a another case of\r\nthe so-called \\e\u000bectiveness of big-data\", which posits\r\nthat even very complex problems in arti\fcial intelligence\r\nmay be solved by simple statistical models trained on\r\nmassive datasets (Halevy et al, 2009). This leads us to\r\nconsider a basic question about the \feld: will continu-\r\nally increasing amounts of training data be su\u000ecient to\r\ndrive continued progress in object recognition absent the\r\ndevelopment of more complex object detection models? To tackle this question, we collected a massive train-\r\ning set that is an order of magnitude larger than existing\r\ncollections such as PASCAL (Everingham et al, 2010). We follow the dominant paradigm of scanning-window\r\ntemplates trained with linear SVMs on HOG features\r\n(Dalal and Triggs, 2005; Felzenszwalb et al, 2010; Bour-\r\ndev and Malik, 2009; Malisiewicz et al, 2011), and eval-\r\nuate detection performance as a function of the amount\r\nof training data and the model complexity. Challenges:We found there is a surprising amount\r\nof subtlety in scaling up training data sets in current sys-\r\ntems. For a \fxed model, one would expect performance\r\nto generally increase with the amount of data and even-\r\ntually saturate (Fig. 2). Empirically, we often saw the\r\nbizarre result that o\u000b-the-shelf implementations show\r\ndecreased performance with additional data! One would\r\nalso expect that to take advantage of additional train-\r\narXiv:1503.01508v1  [cs.CV]  5 Mar 2015","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jvkc0acqBvcSeCMfuUPP57hxLST9hPK9gKUS+H+SYPk="},"921bafbe-8b23-463f-9967-302c33ff1f14":{"id_":"921bafbe-8b23-463f-9967-302c33ff1f14","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"aRyCIOl48PXJ/ZNXZBAcnwqfnxaSPN7+t8sbjk/LucM="},"NEXT":{"nodeId":"45bb8d51-d3dd-4436-a03f-35d23583b260","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"VIZw2AivIif98B8Hxr72yElCVVLPrQ1a/NONUuuly0E="}},"text":"2Xiangxin Zhu et al. Fig. 1The  best  reported  performance  on  PASCAL  VOC\r\nchallenge has shown marked increases since 2006 (top). This\r\ncould be due to various factors: the dataset itself has evolved\r\nover time, the best-performing methods di\u000ber across years,\r\netc. In the bottom-row, we plot a particular factor { training\r\ndata size { which appears to correlate well with performance. This begs the question: has the increase been largely driven\r\nfrom the availability of larger training sets? ing data, it is necessary to grow the model complexity,\r\nin this case by adding mixture components to capture\r\ndi\u000berent object sub-categories and viewpoints. However,\r\neven with non-parametric models that grow with the\r\namount of training data, we quickly encountered dimin-\r\nishing returns in performance with only modest amounts\r\nof training data. We show that the apparent performance ceiling is not\r\na consequence of HOG+linear classi\fers. We provide an\r\nanalysis of the popular deformable part model (DPM),\r\nshowing that it can be viewed as an e\u000ecient way to\r\nimplicitly encode and score an exponentially-large set of\r\nrigid mixture components with shared parameters. With\r\nthe appropriate sharing, DPMs produce substantial per-\r\nformance gains over standard non-parametric mixture\r\nmodels. However, DPMs have \fxed complexity and still\r\nsaturate in performance with current amounts of train-\r\ning data, even when scaled to mixtures of DPMs. This\r\ndi\u000eculty is further exacerbated by the computational\r\ndemands of non-parametric mixture models, which can\r\nbe impractical for many applications. Proposed solutions:In  this  paper,  we  o\u000ber  ex-\r\nplanations and solutions for many of these di\u000eculties. First, we found it crucial to set model regularization as\r\na function of training dataset using cross-validation, a\r\nstandard technique which is often overlooked in current\r\nobject detection systems. Second, existing strategies for\r\ndiscovering sub-category structure, such as clustering\r\naspect ratios (Felzenszwalb et al, 2010), appearance fea-\r\ntures (Divvala et al, 2012), and keypoint labels (Bourdev\r\nFig. 2We plot idealized curves of performance versus train-\r\ning dataset size and model complexity.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+1uxK/0Aot2Th5cH/HFnT0lD1F9R6rMqrlIOuZ1xoss="},"45bb8d51-d3dd-4436-a03f-35d23583b260":{"id_":"45bb8d51-d3dd-4436-a03f-35d23583b260","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"aRyCIOl48PXJ/ZNXZBAcnwqfnxaSPN7+t8sbjk/LucM="},"PREVIOUS":{"nodeId":"921bafbe-8b23-463f-9967-302c33ff1f14","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"+1uxK/0Aot2Th5cH/HFnT0lD1F9R6rMqrlIOuZ1xoss="}},"text":"2We plot idealized curves of performance versus train-\r\ning dataset size and model complexity. The e\u000bect of additional\r\ntraining examples is diminished as the training dataset grows\r\n(left), while we expect performance to grow with model com-\r\nplexity up to a point, after which an overly-\rexible model\r\nover\fts the training dataset (right). Both these notions can be\r\nmade precise with learning theory bounds, see e.g. (McAllester,\r\n1999). and Malik, 2009) may not su\u000ece. We found this was re-\r\nlated to the inability of classi\fers to deal with \\polluted\"\r\ndata  when  mixture  labels  were  improperly  assigned. Increasing model complexity is thus only useful when\r\nmixture components capture the \\right\" sub-category\r\nstructure. To e\u000eciently take advantage of additional training\r\ndata,  we  introduce  a  non-parametric  extension  of  a\r\nDPM which we call an exemplar deformable part model\r\n(EDPM). Notably, EDPMs increase the expressive power\r\nof DPMs with only a negligible increase in computation,\r\nmaking them practically useful. We provide evidence\r\nthat suggests that compositional representations of mix-\r\nture templates provide an e\u000bective way to help target\r\nthe \\long-tail\" of object appearances by sharing local\r\npart appearance parameters across templates. Extrapolating beyond our experiments, we see the\r\nstriking di\u000berence between classic mixture models and\r\nthe non-parametric compositional model (both mixtures\r\nof linear classi\fers operating on the same feature space)\r\nas evidence that the greatest gains in the near future\r\nwill not be had with simple models+bigger data, but\r\nrather through improved representations and learning\r\nalgorithms. We introduce our large-scale dataset in Sec. 2, de-\r\nscribe  our  non-parametric  mixture  models  in  Sec. 3,\r\npresent extensive experimental results in Sec. 4, and\r\nconclude with a discussion in Sec. 5 including related\r\nwork. 2 Big Detection Datasets\r\nThroughout the paper we carry out experiments using\r\ntwo datasets. We vary the number of positive training\r\nexamples, but in all cases keep the number of negative\r\ntraining images \fxed. We found that performance was\r\nrelatively static with respect to the amount of negative","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VIZw2AivIif98B8Hxr72yElCVVLPrQ1a/NONUuuly0E="},"edeb0cf3-9b5b-4159-93e4-22114db9ab0e":{"id_":"edeb0cf3-9b5b-4159-93e4-22114db9ab0e","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jnUDyeYZxdUWFgKbMQq9T+R7JAOqFrxyLFGua9DA/jM="},"NEXT":{"nodeId":"1bcd4cc4-fd9e-4ae5-8af1-bf0e074b7e4d","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"W7rTMufDLDga6mNj6ESH2TtR5dR+i1GdRGTDQTdCr+w="}},"text":"Do We Need More Training Data?3\r\ntraining data, once a su\u000eciently large negative training\r\nset was used. PASCAL:Our  \frst  dataset  is  a  newly  collected\r\ndata set that we refer to as PASCAL-10X and describe\r\nin detail in the following section1. This dataset covers\r\nthe  11  PASCAL  categories  (see  Fig. 1)  and  includes\r\napproximately 10 times as many training examples per\r\ncategory as the standard training data provided by the\r\nPASCAL detection challenge, allowing us to explore the\r\npotential gains of larger numbers of positive training\r\ninstances. We  evaluate  detection  accuracy  on  the  11\r\nPASCAL categories from the PASCAL 2010 trainval\r\ndataset (because test annotations are not public), which\r\ncontains 10000+ images. Faces:In  addition  to  examining  performance  on\r\nPASCAL object categories, we also trained models for\r\nface detection. We found faces to contain more struc-\r\ntured appearance variation, which often allowed for more\r\neasily interpretable diagnostic experiments. Face models\r\nare trained using the CMU MultiPIE dataset(Gross et al,\r\n2010), a well-known benchmark dataset of faces span-\r\nning multiple viewpoints, illumination conditions, and\r\nexpressions. We use up to 900 faces across 13 view points. Each viewpoint was spaced 15\u000eapart spanning 180\u000e. 300 of the faces are frontal, while the remaining 600 are\r\nevenly distributed among the remaining viewpoints. For\r\nnegatives, we use 1218 images from the INRIAPerson\r\ndatabase (Dalal and Triggs, 2005). Detection accuracy of\r\nface models are evaluated on the annotated face in-the-\r\nwild (AFW) (Zhu and Ramanan, 2012), which contains\r\nimages from real-world environments and tend to have\r\ncluttered backgrounds with large variations in both face\r\nviewpoint and appearance. 2.1 Collecting PASCAL-10X\r\nIn this section, we describe our procedure for building\r\na  large,  annotated  dataset  that  is  as  similar  as  pos-\r\nsible  to  the  PASCAL  2010  for  object  detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/yZoyVZH0y8eXitYF6hlkbmn4uObtGdNgqTof4tuDxE="},"1bcd4cc4-fd9e-4ae5-8af1-bf0e074b7e4d":{"id_":"1bcd4cc4-fd9e-4ae5-8af1-bf0e074b7e4d","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jnUDyeYZxdUWFgKbMQq9T+R7JAOqFrxyLFGua9DA/jM="},"PREVIOUS":{"nodeId":"edeb0cf3-9b5b-4159-93e4-22114db9ab0e","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"/yZoyVZH0y8eXitYF6hlkbmn4uObtGdNgqTof4tuDxE="},"NEXT":{"nodeId":"7c075a96-e35b-46d8-ad92-1f4470054801","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"QhWplqXQwg6g+q4sJ04YnCTZoh6cE2Me2/Trl57t4Eo="}},"text":"We\r\ncollected images from Flickr and annotations from Ama-\r\nzon Mechanical Turk (MTurk), resulting in the data set\r\nsummarized in Tab. 1. We built training sets for 11 of\r\nthe PASCAL VOC categories that are an order of mag-\r\nnitude larger than the VOC 2010 standard trainval set. We selected these classes as they contain the smallest\r\namount of training examples, and so are most likely to\r\nimprove from additional training data. We took care\r\nto ensure high-quality bounding box annotations and\r\nhigh-similarity to the PASCAL 2010 dataset. To our\r\n1The dataset can be downloaded fromhttp://vision.ics. uci.edu/datasets/\r\nPASCAL 2010           Our Data Set\r\nCategory           Images     Objects     Images     Objects\r\nBicycle                    471            614        5,027         7,401\r\nBus                         353            498       3,405         4,919\r\nCat                      1,005         1,132      12,204       13,998\r\nCow                        248            464       3,194         6,909\r\nDining Table          415            468       3,905         5,651\r\nHorse                      425            621       4,086         6,488\r\nMotorbike               453            611       5,674         8,666\r\nSheep                      290            701       2,351         6,018\r\nSofa                        406            451        4,018         5,569\r\nTrain                      453            524       6,403         7,648\r\nTV Monitor            490            683       5,053         7,808\r\nTotals                  4,609         6,167      50,772       81,075\r\nTable 1PASCAL 2010 trainval and our data set for select\r\ncategories. Our data set is an order of magnitude larger.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"W7rTMufDLDga6mNj6ESH2TtR5dR+i1GdRGTDQTdCr+w="},"7c075a96-e35b-46d8-ad92-1f4470054801":{"id_":"7c075a96-e35b-46d8-ad92-1f4470054801","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jnUDyeYZxdUWFgKbMQq9T+R7JAOqFrxyLFGua9DA/jM="},"PREVIOUS":{"nodeId":"1bcd4cc4-fd9e-4ae5-8af1-bf0e074b7e4d","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"W7rTMufDLDga6mNj6ESH2TtR5dR+i1GdRGTDQTdCr+w="},"NEXT":{"nodeId":"eed9dc7f-ed62-4e00-b106-09fb529d5dfe","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"WROmvzLkZZhVq3a8peXRvyWfb12Hh2c53V8JFDl1ExI="}},"text":"Our data set is an order of magnitude larger. PASCAL\r\nAttributes              Us     2010     2007\r\nTruncated            30.8      31.515.8\r\nOccluded               5.9       8.6        7.1\r\nJumping                4.0       4.315.8\r\nStanding              69.9      68.854.6\r\nTrotting               23.5      24.9      26.6\r\nSitting                   2.0       1.4        0.7\r\nOther                     0.0       0.5          0\r\nPerson Top          24.8      29.157.5\r\nPerson Besides       8.8      10.0        8.6\r\nNo Person            66.0      59.833.8\r\nTable 2Frequencies of attributes (percent) across images in\r\nour 10x horse data set compared to the PASCAL 2010 train-\r\nval data set. Bolded entries highlight signi\fcant di\u000berences\r\nrelative to our collected data. Our dataset has similar attribute\r\ndistribution to the PASCAL 2010, but di\u000bers signi\fcantly from\r\n2007, which has many more sporting events. knowledge, this is the largest publicly available positive\r\ntraining set for these PASCAL categories. Collection:We downloaded over one hundred thou-\r\nsand large images from Flickr to build our dataset. We\r\ntook  care  to  directly  mimic  the  collection  procedure\r\nused by the PASCAL organizers. We begin with a set of\r\nkeywords (provided by the organizers) associated with\r\neach object class. For each class, we picked a random\r\nkeyword, chose a random date since Flickr's launch, se-\r\nlected a random page on the results, and \fnally took a\r\nrandom image from that page. We repeat this procedure\r\nuntil we had downloaded an order of magnitude larger\r\nnumber of images for each class. Filtering:The downloaded images from Flickr did\r\nnot  necessarily  contain  objects  for  the  category  that\r\nwe were targeting. We created MTurk tasks that asked\r\nworkers to classify the downloaded images on whether\r\nthey contained the category of interest. Our user inter-\r\nface in Fig.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QhWplqXQwg6g+q4sJ04YnCTZoh6cE2Me2/Trl57t4Eo="},"eed9dc7f-ed62-4e00-b106-09fb529d5dfe":{"id_":"eed9dc7f-ed62-4e00-b106-09fb529d5dfe","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jnUDyeYZxdUWFgKbMQq9T+R7JAOqFrxyLFGua9DA/jM="},"PREVIOUS":{"nodeId":"7c075a96-e35b-46d8-ad92-1f4470054801","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"QhWplqXQwg6g+q4sJ04YnCTZoh6cE2Me2/Trl57t4Eo="}},"text":"Our user inter-\r\nface in Fig. 3 gave workers instructions on how to handle\r\nspecial cases and this resulted in acceptable annotation\r\nquality without \fnding agreement between workers.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WROmvzLkZZhVq3a8peXRvyWfb12Hh2c53V8JFDl1ExI="},"658fd130-490d-4e36-b64e-298256e55fb8":{"id_":"658fd130-490d-4e36-b64e-298256e55fb8","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"NqitOEnwY7je2dEJ5MIvoITvuo60DbEoKKKC/grlphE="},"NEXT":{"nodeId":"b3020a95-65dd-4010-a3e0-60058995a6e0","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"vUbBaQVn3qSo6O6AzcebhEm5CprP5TXYbsT+kW4IqIQ="}},"text":"4Xiangxin Zhu et al. Fig. 3Our  MTurk  user  interfaces  for  image  classi\fcation\r\nand object annotation. We provided detailed instructions to\r\nworkers, resulting in acceptable annotation quality. Annotation:After \fltering the images, we created\r\nMTurk  tasks  instructing  workers  to  draw  bounding\r\nboxes around a speci\fc class. Workers were only asked\r\nto annotate up to \fve objects per image using our inter-\r\nface as in Fig.3, although many workers gave us more\r\nboxes. On average, our system received annotations at\r\nthree images per second, allowing us to build bounding\r\nboxes for 10,000 images in under an hour. As not every\r\nobject is labeled, our data set cannot be used to perform\r\ndetection benchmarking (it is not possible to distinguish\r\nfalse-positives from true-negatives). We experimented\r\nwith additional validation steps, but found they were\r\nnot necessary to obtain high-quality annotations. 2.2 Data Quality\r\nTo verify the quality of our annotations, we performed\r\nan in-depth diagnostic analysis of a particular category\r\n(horses). Overall,  our  analysis  suggests  that  our  col-\r\nlection and annotation pipeline produces high-quality\r\ntraining data that is similar to PASCAL. Attribute distribution:We \frst compared vari-\r\nous distributions of attributes of bounding boxes from\r\nPASCAL-10X to those from both PASCAL 2010 and\r\n2007 trainval. Attribute annotations were provided by\r\nmanual labeling. Our \fndings are summarized in Tab. 2. Interestingly,  horses  collected  in  2010  and  2007  vary\r\nsigni\fcantly, while 2010 and PASCAL-10X match fairly\r\nwell. Our images were on average twice the resolution\r\nas those in PASCAL so we scaled our images down to\r\nconstruct our \fnal dataset. User assessment:We also gauged the quality of\r\nour bounding boxes compared to PASCAL with a user\r\nstudy.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"79oBZaaM+vaWVSZYXWIykOnFSYEW1OqQ1cU1DWggLl4="},"b3020a95-65dd-4010-a3e0-60058995a6e0":{"id_":"b3020a95-65dd-4010-a3e0-60058995a6e0","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"NqitOEnwY7je2dEJ5MIvoITvuo60DbEoKKKC/grlphE="},"PREVIOUS":{"nodeId":"658fd130-490d-4e36-b64e-298256e55fb8","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"79oBZaaM+vaWVSZYXWIykOnFSYEW1OqQ1cU1DWggLl4="}},"text":"We \rashed a pair of horse bounding boxes, one\r\nfrom PASCAL-10X and one from PASCAL 2010, on\r\na  screen  and  instructed  a  subject  to  label  which  ap-\r\npeared to be better example. Our subject preferred the\r\nPASCAL 2010 data set 49% of the time and our data\r\nset 51% of the time. Since chance is 50%-50% and our\r\nsubject operated close to chance, this further suggests\r\nPASCAL-10X  matched  well  with  PASCAL. Qualita-\r\ntively, the biggest di\u000berence observed between the two\r\ndatasets was that PASCAL-10X bounding boxes tend to\r\nbe somewhat \\looser\" than the (hand curated) PASCAL\r\n2010 data. Redundant annotations:We  tested  the  use  of\r\nmultiple annotations for removing poorly labeled posi-\r\ntive examples. All horse images were labeled twice, and\r\nonly those bounding boxes that agreed across the two\r\nannotation sessions were kept for training. We found\r\nthat  training  on  these  cross-veri\fed  annotations  did\r\nnot signi\fcantly a\u000bect the performance of the learned\r\ndetector. 3 Mixture models\r\nTo take full advantage of additional training data, it\r\nis vital to grow model complexity. We accomplish this\r\nby adding a mixture component to capture additional\r\n\\sub-category\" structure. In this section, we describe\r\nvarious approaches for learning and representing mixture\r\nmodels. Our basic building block will be a mixture of\r\nlinear classi\fers, or templates. Formally speaking, we\r\ncompute the detection score of an image windowIas:\r\nS(I) = maxm\r\nh\r\nwm\u0001\u001e(I) +bm\r\ni\r\n(1)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"vUbBaQVn3qSo6O6AzcebhEm5CprP5TXYbsT+kW4IqIQ="},"0fd230d3-9422-4fd5-8292-d35f4cc5e95e":{"id_":"0fd230d3-9422-4fd5-8292-d35f4cc5e95e","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"871KU7jd2hsG3o1Wh4G4kEzo9slOA97MmQZq7Nm7CKs="},"NEXT":{"nodeId":"9c049219-3943-4b2f-b647-57dddc6d7ccb","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"/fM2001qcYWFdbWrv7nD9veRIKstiMPg4eLajecI2QI="}},"text":"Do We Need More Training Data?5\r\n(a) Unsupervised\r\n(b) Supervised\r\nFig. 4We compare supervised versus automatic (k-means)\r\napproaches for clustering by displaying the average RGB image\r\nof each cluster. The supervised methods use viewpoint labels\r\nto cluster the training data. Because our face data is relatively\r\nclean, both obtain reasonably good clusters. However, at some\r\nlevels of the hierarchy, unsupervised clustering does seem to\r\nproduce suboptimal partitions - for example, atK= 2. There\r\nis no natural way to group multi-view faces into two groups. Automatically selectingKis a key di\u000eculty with unsupervised\r\nclustering algorithms. wheremis a discrete mixture variable,\b(I) is a HOG im-\r\nage descriptor (Dalal and Triggs, 2005),wmis a linearly-\r\nscored template, andbmis an (optional) bias parameter\r\nthat acts as a prior that favors particular templates over\r\nothers. 3.1 Independent mixtures\r\nIn  this  section,  we  describe  approaches  for  learning\r\nmixture models by clustering positive examples from\r\nour training set. We train independent linear classi\fers\r\n(wm;bm) using positive examples from each cluster. One\r\ndi\u000eculty in evaluating mixture models is that \ructua-\r\ntions in the (non-convex) clustering results may mask\r\nvariations in performance we wish to measure. We took\r\ncare to devise a procedure for varyingK(the number of\r\nclusters) andN(the amount of training data) in such a\r\n(a) Unsupervised\r\n(b) Supervised\r\nFig. 5We compare supervised versus automatic (k-means)\r\napproaches for clustering images of PASCAL buses. Supervised\r\nclustering produces more clear clusters, e.g. the 21 supervised\r\nclusters correspond to viewpoints and object type (single vs\r\ndouble-decker). Supervised clusters perform better in practice,\r\nas we show in Fig. 11. manner that would reduce stochastic e\u000bects of random\r\nsampling. Unsupervised clustering:For our unsupervised\r\nbaseline, we cluster the positive training images of each\r\ncategory into 16 clusters using hierarchical k-means, re-\r\ncursively splitting each cluster intok= 2 subclusters.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GW1PPqj9OrfPlOiqfUuWKUplnlj/8vDP2eSN28AR08A="},"9c049219-3943-4b2f-b647-57dddc6d7ccb":{"id_":"9c049219-3943-4b2f-b647-57dddc6d7ccb","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"871KU7jd2hsG3o1Wh4G4kEzo9slOA97MmQZq7Nm7CKs="},"PREVIOUS":{"nodeId":"0fd230d3-9422-4fd5-8292-d35f4cc5e95e","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"GW1PPqj9OrfPlOiqfUuWKUplnlj/8vDP2eSN28AR08A="}},"text":"For example, given a \fxed training set, we would like the\r\ncluster partitions forK= 8 to respect the cluster parti-\r\ntion ofK= 4. To capture both appearance and shape\r\nwhen  clustering, we  warp  an instance  to  a  canonical\r\naspect ratio, compute its HOG descriptor (reduce the\r\ndimensionality with PCA for computational e\u000eciency),\r\nand append the aspect ratio to the resulting feature\r\nvector. Partitioned sampling:Given a \fxed training set\r\nofNmaxpositive images, we would like to construct a\r\nsmaller sampled subset, say ofN=Nmax2images, whose\r\ncluster partitions respect those in the full dataset. This\r\nis similar in spirit to strati\fed sampling and attempts\r\nto reduce variance in our performance estimates due\r\nto \\binning artifacts\" of inconsistent cluster partitions\r\nacross re-samplings of the data. To do this, we \frst hierarchically-partition the full\r\nset ofNmaximages by recursively applying k-means. We then subsample the images in the leaf nodes of the","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/fM2001qcYWFdbWrv7nD9veRIKstiMPg4eLajecI2QI="},"ee3d289d-815b-4421-8c13-e352b610b2e8":{"id_":"ee3d289d-815b-4421-8c13-e352b610b2e8","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"Woy1lnTdzWPVfwexsVfo2g0ju6cQLPGeN32wu5xl46I="},"NEXT":{"nodeId":"75b7836c-a5cc-494c-a717-9d094dd6bb9b","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"6BUHs6SkLdQT+RPpIjRthGThQ2B6Tc6wsr7kvgl6y8o="}},"text":"6Xiangxin Zhu et al. Input:fNng;fS(i)g\r\nOutput:fC(i)ng\r\n1C(i)0=S(i);   C(i)n=;  8i;8n >1\r\n2forn= 1 :enddo// For eachNn\r\n3fort= 1 :Nndo\r\n4z\u0018jC(z)n\u00001jPjjC(j)n\u00001j;// Pick a cluster randomly\r\n5C(z)n(C(z)n\u00001;// samplezth clusterwithout replacement\r\n6end\r\n7end\r\nAlgorithm 1:Partitioned sampling of the clus-\r\nters.Nnis the number of samples to return for\r\nsetnwithN0=Nmax;Nn> Nn+1.S(i)is the\r\nithcluster from the lowest level of the hierarchy\r\n(e.g., withK= 16 clusters) computed on the full\r\ndatasetNmax. Steps  4-5  randomly  samplesNn\r\ntraining samples fromfC(i)n\u00001gto constructKsub-\r\nsampled clustersfC(i)ng, each of which contain a\r\nsubset of the training data while keeping the same\r\ndistribution of the data over clusters. hierarchy in order to generate a smaller hierarchically\r\npartitioned dataset by using the same hierarchical tree\r\nde\fned over the original leaf clusters. This sub-sampling\r\nprocedure can be applied repeatedly to produce train-\r\ning datasets with fewer and fewer examples that still\r\nrespects the original data distribution and clustering. The sampling algorithm, shown in Alg. 1, yields a\r\nset of partitioned training sets, indexed by (K;N) with\r\ntwo properties: (1) for a \fxed number of clustersK,\r\neach smaller training set is a subset of the larger ones,\r\nand (2) given a \fxed training set sizeN, small clusters\r\nare strict re\fnements of larger clusters. We compute\r\ncon\fdence intervals in our experiments by repeating this\r\nprocedure multiple times to resample the dataset and\r\nproduce multiple sets of (K;N)\u0000consistent partitions. Supervised clustering:To examine the e\u000bect of\r\nsupervision, we cluster the training data by manually\r\ngrouping visually similar samples. For CMU MultiPIE,\r\nwe de\fne clusters using viewpoint annotations provided\r\nwith the dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jbUW8SZxpVyV2BT/Ou2lATCDdWU75Q5Fg0BLozH1Coo="},"75b7836c-a5cc-494c-a717-9d094dd6bb9b":{"id_":"75b7836c-a5cc-494c-a717-9d094dd6bb9b","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"Woy1lnTdzWPVfwexsVfo2g0ju6cQLPGeN32wu5xl46I="},"PREVIOUS":{"nodeId":"ee3d289d-815b-4421-8c13-e352b610b2e8","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"jbUW8SZxpVyV2BT/Ou2lATCDdWU75Q5Fg0BLozH1Coo="},"NEXT":{"nodeId":"45b7a4ab-8401-4fd8-8979-40305bde21d0","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"xhwa0aRg3gMiYas7OKGuj4JoXh1ZMSoaTQwt38GQEh0="}},"text":"We generate a hierarchical clustering\r\nby having a human operator merge similar viewpoints,\r\nfollowing the partitioned sampling scheme above. Since\r\nPASCAL-10X does not have viewpoint labels, we gener-\r\nate an \\over-clustering\" with k-means with a largeK,\r\nand have a human operator manually merge clusters. Fig. 4 and Fig. 5 show example clusters for faces and\r\nbuses. 3.2 Compositional mixtures\r\nIn  this  section,  we  describe  various  architectures  for\r\ncompositional mixture models that share information\r\nbetween mixture components. We share local spatial\r\nregions of templates, or parts. We begin our discussion\r\nby reviewing standard architectures for deformable part\r\nmodels (DPMs), and show how they can be interpreted\r\nand extended as high-capacity mixture models. Deformable Part Models (DPMs):We  begin\r\nwith an analysis that shows that DPMs are equivalent\r\nto  an  exponentially-large  mixture  of  rigid  templates\r\nEqn.(1). This allows us to analyze (both theoretically\r\nand empirically) under what conditions a classic mixture\r\nmodel will approach the behavior of a DPM. Let the\r\nlocation of partibe (xi;yi). Given an imageI, a DPM\r\nscores a con\fguration ofPparts (x;y) =f(xi;yi) :i=\r\n1::Pgas:\r\nSDPM(I) = maxx;yS(I;x;y)    where\r\nS(I;x;y) =\r\nPX\r\ni=1\r\nX\r\n(u;v)2Wi\r\n\u000bi[u;v]\u0001\u001e(I;xi+u;yi+v)\r\n+\r\nX\r\nij2E\r\n\fij\u0001 (xi\u0000xj\u0000a(x)ij;yi\u0000yj\u0000a(y)ij)     (2)\r\nwhereWide\fnes the spatial extent (length and width)\r\nof  parti. The  \frst  term  de\fnes  a  local  appearance\r\nscore,  where\u000biis  the  appearance  template  for  part\r\niand\u001e(I;xi;yi) is the appearance feature vector ex-\r\ntracted from location (xi;yi).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6BUHs6SkLdQT+RPpIjRthGThQ2B6Tc6wsr7kvgl6y8o="},"45b7a4ab-8401-4fd8-8979-40305bde21d0":{"id_":"45b7a4ab-8401-4fd8-8979-40305bde21d0","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"Woy1lnTdzWPVfwexsVfo2g0ju6cQLPGeN32wu5xl46I="},"PREVIOUS":{"nodeId":"75b7836c-a5cc-494c-a717-9d094dd6bb9b","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"6BUHs6SkLdQT+RPpIjRthGThQ2B6Tc6wsr7kvgl6y8o="}},"text":"The second term de\fnes\r\na pairwise deformation model that scores the relative\r\nplacement of a pair of parts with respect to an anchor\r\nposition (a(x)ij;a(y)ij). For simplicity, we have assumed all\r\n\flters are de\fned at the same scale, though the above\r\ncan be extended to the multi-scale case. When the as-\r\nsociated relational graphG= (V;E) is tree-structured,\r\none  can  compute  the  best-scoring  part  con\fguration\r\nmax(x;y)2\nS(I;x;y) with dynamic programming, where\r\n\nis the space of possible part placements. Given that\r\neach ofPparts can be placed at one ofLlocations,\r\nj\nj=LP\u00191020for our models. By  de\fning  index  variables  in  image  coordinates\r\nu0=xi+uandv0=yi+v, we can rewrite Eqn.(2)as:\r\nS(I;x;y) =\r\nX\r\nu0;v0\r\nPX\r\ni=1\r\n\u000bi[u0\u0000xi;v0\u0000yi]\u0001\u001e(I;u0;v0)\r\n+\r\nX\r\nij2E\r\n\fij\u0001 ij(xi\u0000xj\u0000a(x)ij;yi\u0000yj\u0000a(y)ij)\r\n=\r\n\u0010X\r\nu0;v0\r\nw(x;y)[u0;v0]\u0001\u001e(I;u0;v0)\r\n\u0011\r\n+b(x;y)\r\n=w(x;y)\u0001\u001e(I) +b(x;y)                                      (3)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"xhwa0aRg3gMiYas7OKGuj4JoXh1ZMSoaTQwt38GQEh0="},"60aeccde-7e2d-4116-8a3d-006d34b696ea":{"id_":"60aeccde-7e2d-4116-8a3d-006d34b696ea","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"mE0OsYBw7iAiU41AITXjPFtaUOBgZd1JBf8jCEgdlLs="},"NEXT":{"nodeId":"a5230f36-ecfa-4a73-93d2-d6873d652816","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"Tv9pXCGo8Y9K/W2+Z5wMmqMb98vtaR6n+3Ey68icY24="}},"text":"Do We Need More Training Data?7\r\nwherew(x;y)[u0;v0] =PPi=1\u000bi[u0\u0000xi;v0\u0000yi]. For no-\r\ntational  convenience,  we  assume  parts  templates  are\r\npadded with zeros outside of their default spatial ex-\r\ntent. From  the  above  expression  it  is  easy  to  see  that\r\nthe DPM scoring function is formally equivalent to an\r\nexponentially-large mixture model where each mixture\r\ncomponentmis indexed by a particular con\fguration of\r\nparts (x;y). The template corresponding to each mixture\r\ncomponentw(x;y) is constructed by adding together\r\nparts at shifted locations. The bias corresponding to\r\neach  mixture  componentb(x;y)  is  equivalent  to  the\r\nspatial deformation score for that con\fguration of parts. DPMs di\u000ber from classic mixture models previously\r\nde\fned in that they (1) share parameters across a large\r\nnumber of mixtures or rigid templates, (2) extrapolate\r\nby \\synthesizing\" new templates not encountered during\r\ntraining, and \fnally, (3) use dynamic programming to\r\ne\u000eciently search over a large number of templates. Exemplar Part Models (EPMs):To analyze the\r\nrelative importance of part parameter sharing and ex-\r\ntrapolation to new part placements, we de\fne a part\r\nmodel that limits the possible con\fgurations of parts to\r\nthose seen in theNtraining images, written as\r\nSEPM(I) =    max\r\n(x;y)2\nN\r\nS(I;x;y)    where\nN\u0012\n:(4)\r\nWe call such a model an Exemplar Part Model (EPM),\r\nsince  it  can  also  be  interpreted  as  set  ofNrigid  ex-\r\nemplars with shared parameters. EPMs are not to be\r\nconfused  with  exemplar  DPMs  (EDPMs),  which  we\r\nwill shortly introduce as their deformable counterpart. EPMs can be optimized with a discrete enumeration\r\noverNrigid templates rather than dynamic program-\r\nming. However,  by  caching  scores  of  the  local  parts,\r\nthis enumeration can be made quite e\u000ecient even for\r\nlargeN.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"2Ik4s6KS5sG1oPdLiyP1gzBJ28Wr96LZW1R3FdsbQew="},"a5230f36-ecfa-4a73-93d2-d6873d652816":{"id_":"a5230f36-ecfa-4a73-93d2-d6873d652816","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"mE0OsYBw7iAiU41AITXjPFtaUOBgZd1JBf8jCEgdlLs="},"PREVIOUS":{"nodeId":"60aeccde-7e2d-4116-8a3d-006d34b696ea","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"2Ik4s6KS5sG1oPdLiyP1gzBJ28Wr96LZW1R3FdsbQew="},"NEXT":{"nodeId":"0b020c3a-ebfa-4fa6-ba99-50d8c6401852","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"VogOjYrHF61xgpr7VpcOWXNoVXd4QUWsevh+4+SjDTM="}},"text":"EPMs have the bene\ft of sharing, but cannot\r\nsynthesize new templates that were not present in the\r\ntraining data. We visualize example EPM templates in\r\nFig. 6. To take advantage of additional training data, we\r\nwould like to explore non-parametric mixtures of DPMs. One practical issue is that of computation. We show\r\nthat with a particular form of sharing, one can construct\r\nnon-parametric DPMs that are no more computationally\r\ncomplex than standard DPMs or EPMs, but consider-\r\nably more \rexible in that they extrapolate multi-modal\r\nshape models to unseen con\fgurations. Exemplar DPMs (EDPMs):To  describe  our\r\nmodel, we \frst de\fne a mixture of DPMs with a shared\r\nappearance model, but mixture-speci\fc shape models. In the extreme case, each mixture will consist of a sin-\r\ngle training exemplar. We describe an approach that\r\nFig. 7We visualize exponentiated shape modelseb(z)corre-\r\nsponding to di\u000berent part models. A DPM uses a unimodal\r\nGaussian-like model (left), while a EPM allows for only a\r\ndiscrete set of shape con\fgurations encountered at training\r\n(middle). An EDPM non-parametrically models an arbitrary\r\nshape function using a small set of basis functions. From this\r\nperspective, one can view EPMs as special cases of EDPMs\r\nusing scaled delta functions as basis functions. shares both the part \flter computationsanddynamic\r\nprogramming messages across all mixtures, allowing us\r\nto eliminate almost all of the mixture-dependant com-\r\nputation. Speci\fcally, we consider mixture of DPMs of\r\nthe form:\r\nS(I) =     max\r\nm2f1:::Mg\r\nmax\r\nz2\n\r\nh\r\nw(z)\u0001\u001e(I) +bm(z)\r\ni\r\n(5)\r\nwherez= (x;y) and we write a DPM as an inner max-\r\nimization over an exponentially-large set of templates\r\nindexed byz2\n, as in Eqn.(3).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Tv9pXCGo8Y9K/W2+Z5wMmqMb98vtaR6n+3Ey68icY24="},"0b020c3a-ebfa-4fa6-ba99-50d8c6401852":{"id_":"0b020c3a-ebfa-4fa6-ba99-50d8c6401852","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"mE0OsYBw7iAiU41AITXjPFtaUOBgZd1JBf8jCEgdlLs="},"PREVIOUS":{"nodeId":"a5230f36-ecfa-4a73-93d2-d6873d652816","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"Tv9pXCGo8Y9K/W2+Z5wMmqMb98vtaR6n+3Ey68icY24="}},"text":"Because the appear-\r\nance model does not depend onm, we can write:\r\nS(I) = max\r\nz2\n\r\nh\r\nw(z)\u0001\u001e(I) +b(z)\r\ni\r\n(6)\r\nwhereb(z) =maxmbm(z). Interestingly, we can write\r\nthe DPM, EPM, and EDPM in the form of Eqn.(6)by\r\nsimply changing the shape modelb(z):\r\nbDPM(z) =\r\nX\r\nij2E\r\n\fij\u0001 (zi\u0000zj\u0000aij)                     (7)\r\nbEDPM(z) =     max\r\nm2f1:::Mg\r\nX\r\nij2E\r\n\fij\u0001 (zi\u0000zj\u0000amij)      (8)\r\nbEPM(z) =bDPM(z) +b\u0003EDPM(z)                          (9)\r\nwhereamijis  the  anchor  position  for  partiandjin\r\nmixturem. We writeb\u0003EDPM(z) to denote a limiting\r\ncase ofbEDPM(z) with\fij=\u00001and thus takes on a\r\nvalue of 0 whenzhas the same relative part locations\r\nas some exemplarmand\u00001otherwise. While the EPM only considersMdi\u000berent part con-\r\n\fgurations to occur at test time, the EDPM extrapolates\r\naway from these shape exemplars. The spring param-\r\neters\fin the EDPM thus play a role similar to the\r\nkernel width in kernel density estimation. We show a vi-\r\nsualization of these shape models as probabilistic priors\r\nin Fig. 7.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VogOjYrHF61xgpr7VpcOWXNoVXd4QUWsevh+4+SjDTM="},"0d810a9c-0e4d-411b-8769-5813c746c98c":{"id_":"0d810a9c-0e4d-411b-8769-5813c746c98c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"IaJj6+mhgWRAhYOhRrSLXxIrn+gHE18YNuq+V6vfmhk="},"NEXT":{"nodeId":"b9397731-63a9-4719-9a22-d91dfffab5d2","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"ggNO+ou+s+NW/PAmMoXHDqaZqivQ391ecV8NOkyz3NI="}},"text":"8Xiangxin Zhu et al. Fig. 6Classic exemplars vs EPMs. On the top row, we show three rigid templates trained as independent exemplar mixtures. Below them, we show their counterparts from an exemplar part model (EPM), along with their corresponding training images. EPMs share spatially-localized regions (or \\parts\") between mixtures. Each rigid mixture is a superposition of overlapping\r\nparts. A single part is drawn in blue. We show parts on the top row to emphasize that these template regions are trained\r\nindependently. On the [right], we show a template which is implicitly synthesized by a DPM for a novel test image on-the-\ry. In\r\nFig. 15, we show that both sharing of parameters between mixture components and implicit generation of mixture components\r\ncorresponding to unseen part con\fgurations contribute to the strong performance of a DPM. Inference:We now show that inference on EDPMs\r\n(Eqn. 8) can be quite e\u000ecient. Speci\fcally, inference on a\r\nstar-structured EDPM is no more expensive than a EPM\r\nbuilt from the same training examples. Recall that EPMs\r\ncan be e\u000eciently optimized with a discrete enumeration\r\nofNrigid templates with \\intelligent caching\" of part\r\nscores. Intuitively, one computes a response map for each\r\npart, and then scores a rigid template by looking up\r\nshifted locations in the response maps. EDPMs operate\r\nin a similar same manner, but one convolves a \\min-\r\n\flter\" with each response map before looking up shifted\r\nlocations.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ntDkdQdirySqUVQjcQukuE6IvyJtM0J0Yrml27Q7dfc="},"b9397731-63a9-4719-9a22-d91dfffab5d2":{"id_":"b9397731-63a9-4719-9a22-d91dfffab5d2","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"IaJj6+mhgWRAhYOhRrSLXxIrn+gHE18YNuq+V6vfmhk="},"PREVIOUS":{"nodeId":"0d810a9c-0e4d-411b-8769-5813c746c98c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"ntDkdQdirySqUVQjcQukuE6IvyJtM0J0Yrml27Q7dfc="}},"text":"To  be  precise,  we  explicitly  write  out  the\r\nmessage-passing equations for a star-structured EDPM\r\nbelow, where we assume parti= 1 is the root without\r\nloss of generality:\r\nSEDPM(I) = maxz\r\n1;m\r\nh\r\n\u000b1\u0001\u001e(I;z1)+\r\nX\r\nj>1\r\nmj(z1+am1j)\r\ni\r\n(10)\r\nmj(z1) = maxz\r\nj\r\nh\r\n\u000bj\u0001\u001e(I;zj) +\f1j\u0001 (z1\u0000zj)\r\ni\r\n(11)\r\nThe  maximization  in  Eqn.(11)needs  only  be  per-\r\nformed once across mixtures, and can be computed e\u000e-\r\nciently with a single min-convolution or distance trans-\r\nform (Felzenszwalb and Huttenlocher, 2012). The result-\r\ning message is then shifted by mixture-speci\fc anchor\r\npositionsam1jin Eqn.(10). Such mixture-independent\r\nmessages can be computed only for leaf parts, because\r\ninternal parts in a tree will receive mixture-speci\fc mes-\r\nsages from downstream children. Hence star EDPMs\r\nare  essentially  no  more  expensive  than  a  EPM  (be-\r\ncause a single min-convolution per part adds a negligible\r\namount of computation). In our experiments, running\r\na 2000-mixture EDPM is almost as fast as a standard\r\n6-mixture DPM. Other topologies beyond stars might\r\nprovide greater \rexibility. However, since EDPMs en-\r\ncode  shape  non-parametrically  using  many  mixtures,\r\neach individual mixture may need not deform too much,\r\nmaking a star-structured deformation model a reason-\r\nable approximation (Fig. 7). 4 Experiments\r\nArmed with our array of non-parametric mixture models\r\nand datasets, we now present an extensive diagnostic","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ggNO+ou+s+NW/PAmMoXHDqaZqivQ391ecV8NOkyz3NI="},"3f416b1c-c2fb-4229-b353-6ac1e0a59c4b":{"id_":"3f416b1c-c2fb-4229-b353-6ac1e0a59c4b","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"d+Nd2WQGgpLhPaI68R5QzAMzkn7By/9poWStkm1wp1U="},"NEXT":{"nodeId":"91aee3ce-07f2-4b45-b618-1b234ebf6fe1","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"rvf8+fLIqNMl/6w74s3uiquAd8LkRyLoFh435ulzmrY="}},"text":"Do We Need More Training Data?9\r\nanalysis on 11 PASCAL categories from the 2010 PAS-\r\nCAL trainval set and faces from the Annotated Faces in\r\nthe Wild test set (Zhu and Ramanan, 2012). For each\r\ncategory, we train the model with varying number of\r\nsamples (N) and mixtures (K). To train our indepen-\r\ndent mixtures, we learn rigid HOG templates (Dalal and\r\nTriggs, 2005) with linear SVMs (Chang and Lin, 2011). We calibrated SVM scores using Platt scaling (Platt,\r\n1999). Since the goal is to calibrate scores of mixture\r\ncomponents relative to each other, we found it su\u000ecient\r\nto train scaling parameters using the original training\r\nset rather than using a held-out validation set. To train\r\nour compositional mixtures, we use a locally-modi\fed\r\nvariant of the codebase from (Felzenszwalb et al, 2010). To show the uncertainty of the performance with re-\r\nspect to di\u000berent sets of training samples, we randomly\r\nre-sample  the  training  data  5  times  for  eachNand\r\nKfollowing the partitioned sampling scheme described\r\nin Sec. 3. The best regularization parameterCfor the\r\nSVM was selected by cross validation. For diagnostic\r\nanalysis, we \frst focus on faces and buses. Evaluation:We adopt the PASCAL VOC precision-\r\nrecall protocol for object detection (requiring 50% over-\r\nlap), and report average precision (AP). While learning\r\ntheory often focuses on analyzing 0-1 classi\fcation error\r\nrather than AP (McAllester, 1999), we experimentally\r\nveri\fed that AP typically tracks 0-1 classi\fcation error\r\nand so focus on the former in our experiments. 4.1 The importance of proper regularization\r\nWe begin with a rather simple experiment: how does\r\na single rigid HOG template tuned for faces perform\r\nwhen we give it more training dataN? Fig. 8 shows\r\nthe surprising result that additional training data can\r\ndecreaseperformance!","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ygqL50CA1wt9+CSjg+n6GSv6FM0urb7uPXY6bpbMFcc="},"91aee3ce-07f2-4b45-b618-1b234ebf6fe1":{"id_":"91aee3ce-07f2-4b45-b618-1b234ebf6fe1","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"d+Nd2WQGgpLhPaI68R5QzAMzkn7By/9poWStkm1wp1U="},"PREVIOUS":{"nodeId":"3f416b1c-c2fb-4229-b353-6ac1e0a59c4b","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"ygqL50CA1wt9+CSjg+n6GSv6FM0urb7uPXY6bpbMFcc="},"NEXT":{"nodeId":"922ed99e-6139-4c6e-84fc-226d92db1209","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"lkabYSbJgdn9UPJfXw04+VIRNvkJ9Ey88CjqYZKgKvQ="}},"text":"Fig. 8 shows\r\nthe surprising result that additional training data can\r\ndecreaseperformance! For imbalanced object detection\r\ndatasets with many more negatives than positives, the\r\nhinge loss appears to grow linearly with the amount\r\nof positive training data; if one doubles the number of\r\npositives, the total hinge loss also doubles. This leads to\r\nover\ftting. To address this problem, we found itcrucial\r\nto cross-validateCacross di\u000berentN. By doing so, we\r\ndo  see  better  performance  with  more  data  (Fig. 8a). While  cross-validating  regularization  parameters  is  a\r\nstandard procedure when applying a classi\fer to a new\r\ndataset, most o\u000b-the-shelf detectors are trained using\r\na \fxedCacross object categories with large variations\r\nin the number of positives. We suspect other systems\r\nbased on standard detectors (Felzenszwalb et al, 2010;\r\nDalal and Triggs, 2005) may also be su\u000bering from sub-\r\noptimal regularization and might show an improvement\r\nby proper cross-validation. (a) Single face template (test)\r\n(b) Single face template (train)\r\n(c) Single face template (test)\r\nFig. 8(a)  More  training  data  could  hurt  if  we  did  not\r\ncross-validate  to  select  the  optimal  C. (b)  Training  error,\r\nwhen measured on a \fxed training set of 900 faces and 1218\r\nnegative images, always decreases as we train with more of\r\nthose  images. This  further  suggests  that  over\ftting  is  the\r\nculprit,  and  that  proper  regularization  is  the  solution. (c)\r\nTest performance can change drastically with C. Importantly,\r\nthe optimal setting of C depends on the amount of positive\r\ntraining examplesN. 4.2 The importance of clean training data\r\nAlthough proper regularization parameters proved to\r\nbe crucial, we still discovered scenarios where additional\r\ntraining data hurt performance. Fig.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rvf8+fLIqNMl/6w74s3uiquAd8LkRyLoFh435ulzmrY="},"922ed99e-6139-4c6e-84fc-226d92db1209":{"id_":"922ed99e-6139-4c6e-84fc-226d92db1209","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"d+Nd2WQGgpLhPaI68R5QzAMzkn7By/9poWStkm1wp1U="},"PREVIOUS":{"nodeId":"91aee3ce-07f2-4b45-b618-1b234ebf6fe1","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"rvf8+fLIqNMl/6w74s3uiquAd8LkRyLoFh435ulzmrY="}},"text":"Fig. 9 shows an experi-\r\nment with a \fxed set ofNtraining examples where we\r\ntrain two detectors: (1)Allis trained with with allN\r\nexamples, while (2)Frontalis trained with a smaller,\r\n\\clean\" subset of examples containing frontal faces. We\r\ncross-validateCfor each model for eachN. Surprisingly,\r\nFrontaloutperformsAlleven though it is trained with\r\nless data. This outcome cannot be explained by a failure of\r\nthe model to generalize from training to test data. We\r\nexamined the training loss for both models, evaluated on\r\nthe full training set. As expected,Allhas a lower SVM","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"lkabYSbJgdn9UPJfXw04+VIRNvkJ9Ey88CjqYZKgKvQ="},"2c909e0a-3f3b-405b-862c-2adbcf24a3d2":{"id_":"2c909e0a-3f3b-405b-862c-2adbcf24a3d2","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"gzUWi81CDMWlOQsefzjU8A3TdE042dfxY0DhM7S1my8="},"NEXT":{"nodeId":"46e2e521-4d68-4584-9fb2-4900cdafc184","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"HGIP7NvxrLxyX6KI7YGS1SgiMMs67rwbtq98t2nne60="}},"text":"10Xiangxin Zhu et al. (a)\r\n(b) Frontal(c) All\r\nFig. 9In (a), we compare the performance of a single HOG\r\ntemplate trained withNmulti-view face examples, versus a\r\ntemplate trained with a subset of thoseNexamples corre-\r\nsponding to frontal faces. The frontal-face template (b) looks\r\n\\cleaner\" and makes fewer classi\fcation errors on both testing\r\nand training data. The fully-trained template (c) looks noisy\r\nand performs worse, even though it produces a lower SVM\r\nobjective value (when both (b) and (c) are evaluated on the\r\nfull training set). This suggests that SVMs are sensitive to\r\nnoise and bene\ft from training with \\clean\" data. Fig. 10The single bicycle template (marked with red) alone\r\nachieves ap=29.4%, which is almost equivalent to the per-\r\nformance of using all 8 mixtures (ap=29.7%). Both models\r\nstrongly outperform a single-mixture model trained on the full\r\ntraining set. This suggests that these additional mixtures are\r\nuseful during training to capture outliers and prevent \\noisy\"\r\ndata from polluting a \\clean\" template that does most of the\r\nwork at test time. objective function thanFrontal(1.29 vs 3.48). But in\r\nterms of 0-1 loss,Allmakes nearly twice as many classi-\r\n\fcation errors on the same training images (900 vs 470). This observation suggests thatthe hinge loss is a poor\r\nsurrogate to the 0-1 lossbecause \\noisy\" hard examples\r\ncan wildly distort the decision boundary as they incur\r\na large, unbounded hinge penalty. Interestingly, latent\r\nmixture models can mimic the behavior of non-convex\r\nFace\r\nBus\r\nFig. 11We compare the human clustering and automatic k-\r\nmeans clustering at near-identicalK. We \fnd that supervised\r\nclustering  provides  a  small  but  noticeable  improvement  of\r\n2-5%. bounded loss functions (Wu and Liu, 2007) by placing\r\nnoisy examples into junk clusters that simply serve to\r\nexplain outliers in the training set. In some cases, a sin-\r\ngle \\clean\" mixture component by itself explains most\r\nof the test performance (Fig. 10).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"k3C0ZAKDn/VnMO4vMqCL2/7/EJne4Y2BWF6rv8CSwk4="},"46e2e521-4d68-4584-9fb2-4900cdafc184":{"id_":"46e2e521-4d68-4584-9fb2-4900cdafc184","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"gzUWi81CDMWlOQsefzjU8A3TdE042dfxY0DhM7S1my8="},"PREVIOUS":{"nodeId":"2c909e0a-3f3b-405b-862c-2adbcf24a3d2","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"k3C0ZAKDn/VnMO4vMqCL2/7/EJne4Y2BWF6rv8CSwk4="}},"text":"10). The importance of \\clean\" training data suggests it\r\ncould be fruitful to correctly cluster training data into\r\nmixture components where each component is \\clean\". We evaluated the e\u000bectiveness of providing fully super-\r\nvised clustering in producing clean mixtures. In Fig. 11,\r\nwe see a small 2% to 5% increase for manual cluster-\r\ning. In  general,  we  \fnd  that  unsupervised  clustering\r\ncan work reasonably well but depends strongly on the\r\ncategory and features used. For example, the DPM im-\r\nplementation  of  (Felzenszwalb  et  al,  2010)  initializes\r\nmixtures based on aspect ratios. Since faces in di\u000berent\r\nviewpoint share similar aspect ratios, this tends to pro-\r\nduce \\unclean\" mixtures compared to our non-latent\r\nclustering. 4.3 Performance of independent mixtures\r\nGiven  the  right  regularization  and  clean  mixtures\r\ntrained independently, we now evaluate whether perfor-\r\nmance asymptotes as the amount of training data and\r\nthe model complexity increase. Fig. 12  shows  performance  as  we  varyKandN\r\nafter cross-validatingCand using supervised clustering. Fig. 12a demonstrates that increasing the amount of\r\ntraining data yields a clear improvement in performance","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HGIP7NvxrLxyX6KI7YGS1SgiMMs67rwbtq98t2nne60="},"3ff6dc15-848e-44a5-a568-85c6c0e9a148":{"id_":"3ff6dc15-848e-44a5-a568-85c6c0e9a148","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"GsxpnPAyyuDN64eoxotRki0We6oTPABTMds8+Q8vqbA="},"NEXT":{"nodeId":"79350efb-ec53-41a9-a6c4-93b963952e64","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"t7DZW7dTCZe9UA1r3yY5iIyMiNeWbEN/WnvFDJrmZyU="}},"text":"Do We Need More Training Data?11\r\n(a) Face (AP vs N)(b) Face (AP vs K)\r\n(c) Bus (AP vs N)(d) Bus (AP vs K)\r\nFig. 12(a)(c)  show  the  monotonic  non-decreasing  curves\r\nwhen we add more training data. The performance saturates\r\nquickly at a few hundred training samples. (b)(d) show how\r\nthe performance changes with more mixturesK. Given a \fxed\r\nnumber of training samplesN, the performance increases at\r\nthe beginning, and decreases when we split the training data\r\ntoo much so that each mixture only has few samples. at the beginning, and the gain quickly becomes smaller\r\nlater. Larger models with more mixtures tend to per-\r\nform worse with fewer examples due to over \ftting, but\r\neventually win with more data. Surprisingly, improve-\r\nment tends to saturate at\u0018100 training examples per\r\nmixture and with\u001810 mixtures. Fig. 12b shows perfor-\r\nmance as we vary model complexity for a \fxed amount\r\nof training data. Particularly at small data regimes, we\r\nsee the critical point one would expect from Fig. 2: a\r\nmore complex model performs better up to a point, after\r\nwhich it over\fts. We found similar behavior for the buses\r\ncategory which we manually clustered by viewpoint. We performed similar experiments for all 11 PAS-\r\nCAL  object  categories  in  our  PASCAL-10X  dataset\r\nshown  in  Fig. 13. We  evaluate  performance  on  the\r\nPASCAL  2010  trainval  set  since  the  testset  annota-\r\ntions are not public. We cluster the training data into\r\nK=[1,2,4,8,16] mixture components, andN=[50, 100,\r\n500, 1000, 3000,Nmax] training samples, whereNmaxis\r\nthe number of training samples collected for the given\r\ncategory. For  eachN,  we  select  the  bestCandK\r\nthrough cross-validation. Fig. 13a, appears to suggest\r\nthat performance is saturating across all categories as\r\nwe increase the amount of training data.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"g2d8DuKD32I9aBdUJZZUnurIpeX0uSbQqqHav8RZIHI="},"79350efb-ec53-41a9-a6c4-93b963952e64":{"id_":"79350efb-ec53-41a9-a6c4-93b963952e64","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"GsxpnPAyyuDN64eoxotRki0We6oTPABTMds8+Q8vqbA="},"PREVIOUS":{"nodeId":"3ff6dc15-848e-44a5-a568-85c6c0e9a148","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"g2d8DuKD32I9aBdUJZZUnurIpeX0uSbQqqHav8RZIHI="},"NEXT":{"nodeId":"e67ff989-3e50-4689-a718-11c5b1ff5557","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"oDGkdkpu7S+WyNzB3Y4olkKMG/fBPxKp+EgrlPiz0JU="}},"text":"However, if we\r\nplot performance on a log scale (Fig. 13b), it appears\r\nto increase roughly linearly. This suggests that the re-\r\nquired training data may need to grow exponentially to\r\nproduce a \fxed improvement in accuracy. For example,\r\nif we extrapolate the steepest curve in Fig. 13b (mo-\r\ntorbike), we will need 1012motorbike samples to reach\r\n95% AP! Of course 95% AP may not be an achievable level\r\nof performance. There is some upper-bound imposed by\r\nthe Bayes risk associated with the HOG feature space\r\nwhich no amount of training data will let us surpass.Are\r\nclassic mixtures of rigid templates approaching the Bayes\r\noptimal performance?Of course we cannot compute the\r\nBayes risk so this is hard to answer in general. However,\r\nthe performance of any system operating on the same\r\ndata and feature space provides a lower bound on the\r\noptimal performance. We next analyze the performance\r\nof compositional mixtures to provide much better lower\r\nbound on optimal performance. 4.4 Performance of compositional mixtures\r\nWe now perform a detailed analysis of compositional\r\nmixture models, including DPMs, EPMs, and EDPMs. We focus on face detection and Pascal buses. We con-\r\nsider the latent star-structured DPM of (Felzenszwalb\r\net al, 2010) as our primary baseline. For face detection,\r\nwe also compare to the supervised tree-structured DPM\r\nof (Zhu and Ramanan, 2012), which uses facial land-\r\nmark annotations in training images as supervised part\r\nlocations. Each of these DPMs makes use of di\u000berent\r\nparts, and so can be used to de\fne di\u000berent EPMs and\r\nEDPMs. We plot performance of faces in Fig. 15 and\r\nbuses in Fig.16. Supervised DPMs:For  face  detection,  we  \frst\r\nnote that a supervised DPM can perform quite well (91%\r\nAP) with less than 200 example faces. This represents a\r\nlower bound on the maximum achievable performance\r\nwith a mixture of linear templates given a \fxed training\r\nset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"t7DZW7dTCZe9UA1r3yY5iIyMiNeWbEN/WnvFDJrmZyU="},"e67ff989-3e50-4689-a718-11c5b1ff5557":{"id_":"e67ff989-3e50-4689-a718-11c5b1ff5557","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"GsxpnPAyyuDN64eoxotRki0We6oTPABTMds8+Q8vqbA="},"PREVIOUS":{"nodeId":"79350efb-ec53-41a9-a6c4-93b963952e64","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"t7DZW7dTCZe9UA1r3yY5iIyMiNeWbEN/WnvFDJrmZyU="}},"text":"This performance is noticeably higher than that of\r\nour cross-validated rigid mixture model, which maxes\r\nout at an AP of 76% with 900 training examples. By\r\nextrapolation, we predict that one would needN= 1010\r\ntraining examples to achieve the DPM performance. To\r\nanalyze where this performance gap is coming from, we\r\nnow evaluate the performance of various compositional\r\nmixtures models. Latent parts:We begin by analyzing the perfor-\r\nmance of compositional mixtures de\fned by latent parts,\r\nas they can be constructed for both faces and Pascal\r\nbuses. Recall that EPMs have the bene\ft of sharing\r\nparameters between rigid templates, but they cannot\r\nextrapolate to new shape con\fgurations not seen among\r\ntheNtraining  examples. EPMs  noticeably  improve\r\nperformance over independent mixtures, improving AP","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oDGkdkpu7S+WyNzB3Y4olkKMG/fBPxKp+EgrlPiz0JU="},"67c560ba-33bd-42bc-bb4b-487774794bf9":{"id_":"67c560ba-33bd-42bc-bb4b-487774794bf9","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"HMrezKae41Y6N/oUnON8WPC0IgBFgn+O1c4Ra0esPFs="},"NEXT":{"nodeId":"0411d0e8-0946-4c24-b78f-5d525dca1525","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"sMlOt81tYMwRMAy9fI51I5lExZz3UgRAtgNlRdD6wVs="}},"text":"12Xiangxin Zhu et al. (a)\r\n(b)\r\nFig. 13We plot the best performance at varying amount of\r\ntraining data for 11 PASCAL categories on PASCAL 2010\r\ntrainval set. (a) shows that all the curves look saturated with\r\na relatively small amount of training data; but in log scale\r\n(b) suggests a diminishing return instead of true saturation. However the performance increases so slow that we will need\r\nmore than 1012examples per category to reach 95% AP if we\r\nkeep growing at the same rate. from 76% to 78.5% for faces and improving AP from\r\n56% to 64% for buses. In fact, for largeN, they approach\r\nthe performance of latent DPMs, which is 79% for faces\r\nand 63% for buses. For smallN, EPMs somewhat un-\r\nderperform  DPMs. This  makes  sense:  with  very  few\r\nobserved shape con\fgurations, exemplar-based methods\r\nare limited. But interestingly, with a modest number\r\nof observed shapes (\u00191000), exemplar-based methods\r\nwith parameter sharing can approach the performance\r\nof DPMs. This in turn suggests that extrapolation to\r\nunseen  shapes  is  may  not  be  crucial,  at  least  in  the\r\nlatent case. This is further evidenced by the fact that\r\nEDPMs, the deformable counterpart to EPMs, perform\r\nsimilarly to both EPMs and DPMs. Supervised parts:The story changes somewhat for\r\nsupervised parts. Here, supervised EPMs outperform\r\nindependent mixtures 85% to 76%. Perhaps surprisingly,\r\nEPMs even outperform latent DPMs. However, super-\r\nvised EPMs still underperform a supervised DPM. This\r\nsuggests that, in the supervised case,the performance\r\ngap (85% vs 91%) stems from the ability of DPMs to\r\nsynthesize con\fgurations that are not seen during train-\r\ning. Moreover,  the  reduction  in  relative  error  due  to\r\nextrapolation is more signi\fcant than the reduction due\r\nto part sharing.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ieTkrNqrgFXC2rvlb8ri2dNCoVCHlmsybVJ/mVU+O18="},"0411d0e8-0946-4c24-b78f-5d525dca1525":{"id_":"0411d0e8-0946-4c24-b78f-5d525dca1525","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"HMrezKae41Y6N/oUnON8WPC0IgBFgn+O1c4Ra0esPFs="},"PREVIOUS":{"nodeId":"67c560ba-33bd-42bc-bb4b-487774794bf9","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"ieTkrNqrgFXC2rvlb8ri2dNCoVCHlmsybVJ/mVU+O18="},"NEXT":{"nodeId":"5ad00868-9b09-4205-99d3-8e5768832210","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"7fLeBkoRdIbdU9oZu2Dk5JHGg8pj+GPvcY3hhsXcMf8="}},"text":"(Zhu and Ramanan, 2012) point out\r\nthat a tree-structured DPM signi\fcantly outperforms\r\na  star-structured  DPM,  even  when  both  are  trained\r\nwith the same supervised parts. One argument is that\r\ntrees better capture nature spatial constraints of the\r\nmodel, such as the contour-like continuity of small parts. Indeed, we also \fnd that a star-structured DPM does a\r\n\\poorer\" job of extrapolation. In fact, we show that an\r\nEDPM does as well a supervised star model, but not\r\nquite up to the performance of a tree DPM. Analysis:Our results suggest that part models can\r\nbe seen as a mechanism for performing intelligent pa-\r\nrameter sharing across observed mixture components\r\nand extrapolation to implicit, unseen mixture compo-\r\nnents. Both these aspects contribute to the strong per-\r\nformance of DPMs. However, with the \\right\" set of\r\n(supervised) parts and the \\right\" geometric (tree- struc-\r\ntured) constraints, extrapolation to unseen templates\r\nhas the potential to be much more signi\fcant. We see\r\nthis  as  a  consequence  of  the  \\long-tail\"  distribution\r\nof object shape (Fig. 17); many object instances can\r\nbe modeled with a few shape con\fgurations, but there\r\nexists  of  long  tail  of  unusual  shapes. Examples  from\r\nthe long tail may be di\u000ecult to observe in any \fnite\r\ntraining dataset, suggesting that extrapolation is crucial\r\nfor recognizing these cases. Once the representation for\r\nsharing and extrapolation is accurately speci\fed, fairly\r\nlittle training data is needed. Indeed, our analysis shows\r\nthat one can train a state-of-the-art face detector (Zhu\r\nand Ramanan, 2012) with 50 face images. Relation to Exemplar SVMs:In the setting of\r\nobject  detection,  we  were  not  able  to  see  signi\fcant\r\nperformance improvements due to our non-parametric\r\ncompositional mixtures. However, EDPMs may be use-\r\nful for other tasks.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"sMlOt81tYMwRMAy9fI51I5lExZz3UgRAtgNlRdD6wVs="},"5ad00868-9b09-4205-99d3-8e5768832210":{"id_":"5ad00868-9b09-4205-99d3-8e5768832210","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"HMrezKae41Y6N/oUnON8WPC0IgBFgn+O1c4Ra0esPFs="},"PREVIOUS":{"nodeId":"0411d0e8-0946-4c24-b78f-5d525dca1525","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"sMlOt81tYMwRMAy9fI51I5lExZz3UgRAtgNlRdD6wVs="}},"text":"However, EDPMs may be use-\r\nful for other tasks. Speci\fcally, they share an attractive\r\nproperty of exemplar SVMs (ESVMs) (Malisiewicz et al,\r\n2011): each detection can be a\u000eliated with its closest\r\nmatching training example (given by the mixture in-\r\ndex), allowing us to transfer annotations from a training\r\nexample to the test instance. (Malisiewicz et al, 2011)\r\nargue that non-parametric label transfer is an e\u000bective","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"7fLeBkoRdIbdU9oZu2Dk5JHGg8pj+GPvcY3hhsXcMf8="},"443bca5e-05cb-431e-823e-f122bff44309":{"id_":"443bca5e-05cb-431e-823e-f122bff44309","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"0FVZ86I1JeHwcKUAvCZalRMTavvNxeHY65Foaibw5fA="},"NEXT":{"nodeId":"62f48b94-6765-4b1a-b31f-eb455dd2ceb3","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"XfRgl4W6MlurkUH2UxzxGyIpFbP4hwgLYdRqn01tKGA="}},"text":"Do We Need More Training Data?13\r\nFig. 14We visualize detections using our exemplar DPM\r\n(EDPM) model. As opposed to existing exemplar-based meth-\r\nods (Malisiewicz et al, 2011), our model shared parameters\r\nbetween  exemplars  (and  so  is  faster  to  evaluate)  and  can\r\ngeneralize to unseen shape con\fgurations. Moreover, EDPMs\r\nreturns corresponding landmarks between an exemplar and a\r\ndetected instance (and hence an associated set of landmark\r\ndeformation vectors), visualized on the top row of faces. way of transferring associative knowledge, such as 3D\r\npose, segmentation masks, attribute labels, etc. How-\r\never, unlike ESVMs, EDPMs share computation among\r\nthe exemplars (and so are faster), can generalize to un-\r\nseen con\fgurations (since they can extrapolate to new\r\nshapes), and also report a part deformation \feld associ-\r\nated with each detection (which maybe useful to warp\r\ntraining labels to better match the detected instance). We show example detections (and their matching exem-\r\nplars) in Fig. 14. 5 Related Work\r\nWe view our study as complementary to other meta-\r\nanalysis of the object recognition problem, such as stud-\r\nies of the dependence of performance on the number of\r\nobject categories (Deng et al, 2010), visual properties\r\n(Hoiem et al, 2012), dataset collection bias (Torralba\r\nFaces\r\nFig. 15We compare the performance of mixtures models\r\nwith EPMs and latent/supervised DPMs for the task of face\r\ndetection. A single rigid template (K= 1) tuned for frontal\r\nfaces outperforms the one tuned for all faces (as shown in\r\nFig. 9). Mixture models boost performance to 76%, approach-\r\ning the performance of a latent DPM (79%). The EPM shares\r\nsupervised part parameters across rigid templates, boosting\r\nperformance to 85%. The supervised DPM (91%) shares pa-\r\nrameters but also implicitly scores additional templates not\r\nseen during training. Bus\r\nFig. 16We compare the performance of mixture models with\r\nlatent EPMs, EDPMs, and DPMs for bus detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"1zf3F7vm/deh9Qk8Ttnh+bDwgPBf591k+gAbidF+Vgk="},"62f48b94-6765-4b1a-b31f-eb455dd2ceb3":{"id_":"62f48b94-6765-4b1a-b31f-eb455dd2ceb3","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"0FVZ86I1JeHwcKUAvCZalRMTavvNxeHY65Foaibw5fA="},"PREVIOUS":{"nodeId":"443bca5e-05cb-431e-823e-f122bff44309","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"1zf3F7vm/deh9Qk8Ttnh+bDwgPBf591k+gAbidF+Vgk="}},"text":"In the\r\nlatent setting, EPMs signi\fcantly outperform the rigid mix-\r\ntures of template and match the performance of the standard\r\nlatent DPMs. and  Efros, 2011),  and  component-speci\fc  analysis  of\r\nrecognition pipelines (Parikh and Zitnick, 2011). Object detection:Our  analysis  is  focused  on\r\ntemplate-based approaches to recognition, as such meth-\r\nods are currently competitive on challenging recognition\r\nproblems such as PASCAL. However, it behooves us to\r\nrecognize the large body of alternate approaches includ-\r\ning hierarchical or \\deep\" feature learning (Krizhevsky\r\net al, 2012), local feature analysis (Tuytelaars and Miko-","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"XfRgl4W6MlurkUH2UxzxGyIpFbP4hwgLYdRqn01tKGA="},"4c726372-b14a-474b-88bf-02d9a1809472":{"id_":"4c726372-b14a-474b-88bf-02d9a1809472","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"DUBy5IxPLWAL/3SaAX1/51bZUmc/s/luIHUhOto6Wh8="},"NEXT":{"nodeId":"a30c7764-768e-4795-ac42-f071861de4b5","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"NvjJ3MAAtL2xfhk9jYoLkGINGlf/nnK5cXWfHabSjsQ="}},"text":"14Xiangxin Zhu et al. (a) Bus\r\n(b) Face\r\nFig. 17We plot the number of distinct shape patterns in\r\nour training set of buses and faces. Each training example is\r\n\\binned\" into a discrete shape by quantizing a vector of part\r\nlocations. The above histograms count the number of examples\r\nthat fall into a particular shape bin. In both cases, the number\r\nof occurrences seems to follow a long-tail distribution: a small\r\nnumber of patterns are common, while there are a huge number\r\nof  rare  cases. Interestingly,  there  are  less  than  500  unique\r\nbus con\fgurations observed in our PASCAL-10X dataset of\r\n2000  training  examples. This  suggests  that  one  can  build\r\nan exemplar part model (EPM) from the \\right\" set of 500\r\ntraining examples and still perform similarly to a DPM trained\r\non the full dataset (Fig. 16). lajczyk,  2008),  kernel  methods  (Vedaldi  et  al,  2009),\r\nand decision trees (Bosch et al, 2007), to name a few. Such methods may produce di\u000berent dependencies on\r\nperformance as a function of dataset size due to inher-\r\nent di\u000berences in model architectures. We hypothesize\r\nthat our conclusions regarding parameter sharing and\r\nextrapolation may generally hold for other architectures. Non-parametric models in vision:Most relevant\r\nto our analysis is work on data-driven models for recog-\r\nnition. Non-parametric scene models have been used\r\nfor scene completion (Hays and Efros, 2007), geoloca-\r\ntion (Hays and Efros, 2008). Exemplar-based methods\r\nhave  also  been  used  for  scene-labeling  through  label\r\ntransfer (Liu et al, 2011; Tighe and Lazebnik, 2010). Other examples include nearest-neighbor methods for\r\nlow-resolution image analysis (Torralba et al, 2008) and\r\nimage classi\fcation (Zhang et al, 2006; Boiman et al,\r\n2008). The closest approach to us is (Malisiewicz et al,\r\n2011), who learn exemplar templates for object detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CSn7OkXkLR1CdF2KP/5OevwTrN49tXDl8QXXXblaY0A="},"a30c7764-768e-4795-ac42-f071861de4b5":{"id_":"a30c7764-768e-4795-ac42-f071861de4b5","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"DUBy5IxPLWAL/3SaAX1/51bZUmc/s/luIHUhOto6Wh8="},"PREVIOUS":{"nodeId":"4c726372-b14a-474b-88bf-02d9a1809472","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"CSn7OkXkLR1CdF2KP/5OevwTrN49tXDl8QXXXblaY0A="},"NEXT":{"nodeId":"284c2630-84d6-4575-b80f-38a050c78b8a","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"+TJfKjVlLRqsZyDF1WXVBnQAIkgqIa5o0+imsSWjj8I="}},"text":"Our analysis suggests that it is crucial to share infor-\r\nmation between exemplars and extrapolate to unseen\r\ntemplates by re-composing parts to new con\fgurations. Scalable nearest-neighbors:We   demonstrate\r\nthat  compositional  part  models  are  one  method  for\r\ne\u000ecient  nearest-neighbor  computations. Prior  work\r\nhas  explored  approximate  methods  such  as  hashing\r\n(Shakhnarovich et al, 2003, 2005) and kd-trees (Muja\r\nand Lowe, 2009; Beis and Lowe, 1997). Our analysis\r\nshows that one can view parts as tools for exact and\r\ne\u000ecient indexing into an exponentially-large set of tem-\r\nplates. This suggests an alternative perspective of parts\r\nas computational entities rather than semantic ones. 6 Conclusion\r\nWe have performed an extensive analysis of the current\r\ndominant paradigm for object detection using HOG fea-\r\nture templates. We speci\fcally focused on performance\r\nas a function of the amount of training data, and intro-\r\nduced several non-parametric models to diagnose the\r\nstate of a\u000bairs. To scale current systems to larger datasets, we \fnd\r\nthat one must get certain \\details\" correct. Speci\fcally,\r\n(a) cross-validation of regularization parameters is mun-\r\ndane but crucial, (b) current discriminative classi\fcation\r\nmachinery is overly sensitive to noisy data, suggesting\r\nthat (c) manual cleanup and supervision or more clever\r\nlatent  optimization  during  learning  may  play  an  im-\r\nportant role for designing high-performance detection\r\nsystems. We also demonstrate that HOG templates have\r\na relatively small e\u000bective capacity; one can train ac-\r\ncurate HOG templates with 100-200 positive examples\r\n(rather than thousands of examples as is typically done\r\n(Dalal and Triggs, 2005)). From  a  broader  perspective,  an  emerging  idea  in\r\nour community is that object detection might be solved\r\nwith simple models backed with massive training sets. Our experiments suggest a slightly re\fned view. Given\r\nthe size of existing datasets, it appears that the cur-\r\nrent state-of-the-art will need signi\fcant additional data\r\n(perhaps exponentially larger sets) to continue produc-\r\ning consistent improvements in performance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NvjJ3MAAtL2xfhk9jYoLkGINGlf/nnK5cXWfHabSjsQ="},"284c2630-84d6-4575-b80f-38a050c78b8a":{"id_":"284c2630-84d6-4575-b80f-38a050c78b8a","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"DUBy5IxPLWAL/3SaAX1/51bZUmc/s/luIHUhOto6Wh8="},"PREVIOUS":{"nodeId":"a30c7764-768e-4795-ac42-f071861de4b5","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"NvjJ3MAAtL2xfhk9jYoLkGINGlf/nnK5cXWfHabSjsQ="}},"text":"We found\r\nthat larger gains were possible by enforcing richer con-\r\nstraints within the model, often through non-parametric\r\ncompositional representations that could make better\r\nuse of additional data. In some sense, we need \\better\r\nmodels\" to make better use of \\big data\". Another common hypothesis is that we should fo-\r\ncus on developing better features, not better learning\r\nalgorithms. While  HOG  is  certainly  limited,  we  still\r\nsee substantial performance gains without any change\r\nin the features themselves or the class of discriminant\r\nfunctions. Instead, the strategic issues appear to be pa-\r\nrameter sharing, compositionality, and non-parametric","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+TJfKjVlLRqsZyDF1WXVBnQAIkgqIa5o0+imsSWjj8I="},"a94a70c4-eaf5-449e-a900-49ff582632fc":{"id_":"a94a70c4-eaf5-449e-a900-49ff582632fc","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"JfAPQ+Wi5rvn/c/hwcz5POUU3cQT7j4QSaBXYvuDWrg="},"NEXT":{"nodeId":"4606d2a4-a63e-4680-ab56-2e2964b91115","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"js5GBThO0BPqw2FfXD6jA5t8nSpw/qhZfakuC+cF6Ac="}},"text":"Do We Need More Training Data?15\r\nencodings. Establishing and using accurate, clean corre-\r\nspondence among training examples (e.g., that specify\r\nthat certain examples belong to the same sub-category,\r\nor that certain spatial regions correspond to the same\r\npart) and developing non-parametric compositional ap-\r\nproaches that implicitly make use of augmented training\r\nsets appear the most promising directions. References\r\nBeis JS, Lowe DG (1997) Shape indexing using approx-\r\nimate nearest-neighbour search in high-dimensional\r\nspaces. In: Computer Vision and Pattern Recognition,\r\n1997. Proceedings., 1997 IEEE Computer Society Con-\r\nference on, IEEE, pp 1000{1006\r\nBoiman O, Shechtman E, Irani M (2008) In defense of\r\nnearest-neighbor based image classi\fcation. In: Com-\r\nputer Vision and Pattern Recognition, 2008. CVPR\r\n2008. IEEE Conference on, IEEE, pp 1{8\r\nBosch A, Zisserman A, Muoz X (2007) Image classi\f-\r\ncation using random forests and ferns. In: Computer\r\nVision, 2007. ICCV 2007. IEEE 11th International\r\nConference on, IEEE, pp 1{8\r\nBourdev L, Malik J (2009) Poselets: Body part detec-\r\ntors  trained  using  3d  human  pose  annotations. In:\r\nInternational Conference on Computer Vision\r\nChang C, Lin C (2011) LIBSVM: A library for support\r\nvector machines. ACM Transactions on Intelligent Sys-\r\ntems and Technology 2:27:1{27:27, software available\r\nathttp://www.csie.ntu.edu.tw/~cjlin/libsvm\r\nDalal N, Triggs B (2005) Histograms of oriented gradi-\r\nents for human detection. In: CVPR 2005. Deng  J,  Berg  A,  Li  K,  Fei-Fei  L  (2010)  What  Does\r\nClassifying More Than 10,000 Image Categories Tell\r\nUs?","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"j2wG3/oa2i9dcqJg7O/kfiP+IXKonfWeO2JDRXeHDsk="},"4606d2a4-a63e-4680-ab56-2e2964b91115":{"id_":"4606d2a4-a63e-4680-ab56-2e2964b91115","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"JfAPQ+Wi5rvn/c/hwcz5POUU3cQT7j4QSaBXYvuDWrg="},"PREVIOUS":{"nodeId":"a94a70c4-eaf5-449e-a900-49ff582632fc","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"j2wG3/oa2i9dcqJg7O/kfiP+IXKonfWeO2JDRXeHDsk="},"NEXT":{"nodeId":"88e7857e-5625-46f0-8b45-8a86079f1c86","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"RspFmZe2qp2ZT++R24BdPE7a2RUVWjoqJgcfAMQGY80="}},"text":"In: International Conference on Computer Vision\r\nDivvala SK, Efros AA, Hebert M (2012) How impor-\r\ntant  are  deformable  parts  in  the  deformable  parts\r\nmodel? In: European Conference on Computer Vision\r\n(ECCV), Parts and Attributes Workshop\r\nEveringham M, Van Gool L, Williams C, Winn J, Zis-\r\nserman A (2010) The PASCAL visual object classes\r\n(VOC) challenge. International Journal of Computer\r\nVision 88(2):303{338\r\nFelzenszwalb P, Huttenlocher D (2012) Distance trans-\r\nforms  of  sampled  functions. Theory  of  Computing\r\n8(19)\r\nFelzenszwalb P, Girshick R, McAllester D, Ramanan D\r\n(2010) Object detection with discriminatively trained\r\npart-based models. IEEE TPAMI\r\nGross R, Matthews I, Cohn J, Kanade T, Baker S (2010)\r\nMulti-pie. Image and Vision Computing\r\nHalevy A, Norvig P, Pereira F (2009) The unreason-\r\nable e\u000bectiveness of data. Intelligent Systems, IEEE\r\n24(2):8{12\r\nHays J, Efros A (2007) Scene completion using millions\r\nof photographs. In: ACM Transactions on Graphics\r\n(TOG), ACM, vol 26, p 4\r\nHays J, Efros AA (2008) Im2gps: estimating geographic\r\ninformation from a single image. In: Computer Vision\r\nand Pattern Recognition, 2008. CVPR 2008. IEEE\r\nConference on, IEEE, pp 1{8\r\nHoiem D, Chodpathumwan Y, Dai Q (2012) Diagnosing\r\nerror in object detectors. In: Computer Vision  ECCV\r\n2012, Springer Berlin Heidelberg, vol 7574, pp 340{353\r\nKrizhevsky A, Sutskever I, Hinton G (2012) Imagenet\r\nclassi\fcation with deep convolutional neural networks. In: Advances in Neural Information Processing Sys-\r\ntems 25, pp 1106{1114\r\nLiu C, Yuen J, Torralba A (2011) Nonparametric scene\r\nparsing via label transfer.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"js5GBThO0BPqw2FfXD6jA5t8nSpw/qhZfakuC+cF6Ac="},"88e7857e-5625-46f0-8b45-8a86079f1c86":{"id_":"88e7857e-5625-46f0-8b45-8a86079f1c86","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"JfAPQ+Wi5rvn/c/hwcz5POUU3cQT7j4QSaBXYvuDWrg="},"PREVIOUS":{"nodeId":"4606d2a4-a63e-4680-ab56-2e2964b91115","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"js5GBThO0BPqw2FfXD6jA5t8nSpw/qhZfakuC+cF6Ac="}},"text":"Pattern Analysis and Ma-\r\nchine Intelligence, IEEE Transactions on 33(12):2368{\r\n2382\r\nMalisiewicz T, Gupta A, Efros A (2011) Ensemble of\r\nexemplar-svms for object detection and beyond. In:\r\nInternational Conference on Computer Vision, IEEE,\r\npp 89{96\r\nMcAllester DA (1999) Some pac-bayesian theorems. Ma-\r\nchine Learning 37(3):355{363\r\nMuja  M,  Lowe  DG  (2009)  Fast  approximate  nearest\r\nneighbors with automatic algorithm con\fguration. In:\r\nInternational Conference on Computer Vision Theory\r\nand Applications (VISSAPP09), pp 331{340\r\nParikh D, Zitnick C (2011) Finding the weakest link in\r\nperson detectors. In: Computer Vision and Pattern\r\nRecognition, IEEE, pp 1425{1432\r\nPlatt J (1999)  Probabilistic  outputs for support  vec-\r\ntor machines and comparisons to regularized likeli-\r\nhood methods. In: ADVANCES IN LARGE MARGIN\r\nCLASSIFIERS, MIT Press, pp 61{74\r\nShakhnarovich G, Viola P, Darrell T (2003) Fast pose\r\nestimation with parameter-sensitive hashing. In: Com-\r\nputer Vision, 2003. Proceedings. Ninth IEEE Interna-\r\ntional Conference on, IEEE, pp 750{757\r\nShakhnarovich G, Darrell T, Indyk P (2005) Nearest-\r\nneighbor methods in learning and vision: theory and\r\npractice, vol 3. MIT press Cambridge\r\nTighe J, Lazebnik S (2010) Superparsing: scalable non-\r\nparametric image parsing with superpixels. In: Com-\r\nputer Vision{ECCV 2010, Springer, pp 352{365\r\nTorralba A, Efros A (2011) Unbiased look at dataset\r\nbias. In: Computer Vision and Pattern Recognition,\r\nIEEE, pp 1521{1528\r\nTorralba A, Fergus R, Freeman WT (2008) 80 million\r\ntiny images: A large data set for nonparametric object","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"RspFmZe2qp2ZT++R24BdPE7a2RUVWjoqJgcfAMQGY80="},"dde23d27-9dba-46ce-adbf-93b9cbc304b7":{"id_":"dde23d27-9dba-46ce-adbf-93b9cbc304b7","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Do_We_Need_More_Training_Data.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Do_We_Need_More_Training_Data.pdf","file_name":"Do_We_Need_More_Training_Data.pdf"},"hash":"MRjNV5s7Z6vtKZaF3M/WQxkj18rdG17QXImYswLDlkc="}},"text":"16Xiangxin Zhu et al. and scene recognition. Pattern Analysis and Machine\r\nIntelligence, IEEE Transactions on 30(11):1958{1970\r\nTuytelaars  T,  Mikolajczyk  K  (2008)  Local  invariant\r\nfeature detectors: a survey. Foundations and TrendsR\r\r\nin Computer Graphics and Vision 3(3):177{280\r\nVedaldi A, Gulshan V, Varma M, Zisserman A (2009)\r\nMultiple kernels for object detection. In: Computer\r\nVision, 2009 IEEE 12th International Conference on,\r\nIEEE, pp 606{613\r\nWu Y, Liu Y (2007) Robust truncated hinge loss support\r\nvector machines. Journal of the American Statistical\r\nAssociation 102(479):974{983\r\nZhang  H,  Berg  AC,  Maire  M,  Malik  J  (2006)  Svm-\r\nknn: Discriminative nearest neighbor classi\fcation for\r\nvisual category recognition. In: Computer Vision and\r\nPattern Recognition, 2006 IEEE Computer Society\r\nConference on, IEEE, vol 2, pp 2126{2136\r\nZhu X, Ramanan D (2012) Face detection, pose esti-\r\nmation,  and  landmark  localization  in  the  wild. In:\r\nComputer Vision and Pattern Recognition","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"x+C/sg6tiVAj7/dX/pR17LCuVQ0C7Z0kwEZy10IUFSY="},"2dbf54b8-8511-43f5-ad0d-8a41de6bae46":{"id_":"2dbf54b8-8511-43f5-ad0d-8a41de6bae46","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"y6Y1qs48uFmVsWsy22qnVozT4S6bJlMY7/rTiwdjF4M="}},"text":"Error Analysis of Object Detection Models \r\nWith Interactive User Interfaces \r\n \r\n \r\nby \r\nDayeon Oh \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nA Final Project \r\n \r\n \r\nsubmitted to \r\n \r\n \r\nOregon State University \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\n \r\nin partial fulfillment of \r\nthe requirements for the \r\ndegree of \r\n \r\n \r\nMaster of Science \r\n \r\n \r\n \r\n \r\n \r\nPresented June 9th \r\nCommencement 2022","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/eNEnV4MUsSncy5HduLAcgc3Mi2UmdltKqWLF+nmYEY="},"0efd40d2-99a8-446c-aab9-9212235899f8":{"id_":"0efd40d2-99a8-446c-aab9-9212235899f8","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"UCkzH+7mAKOBcl6NzDTHI6mTQHa2GvATZ70HnCVyKp8="}},"text":"AN ABSTRACT OF THE PROJECT OF \r\n \r\nDayeon  Oh for  the  degree  of Master  of Science in Computer  science presented  on \r\nJune 9th, 2022. Title:  Error Analysis of Object Detection Models with Interactive User Interfaces. Object detection models are being widely used in many applications, such as \r\nautonomous driving, construction management, and cancer detection. Evaluating the \r\nperformance of the object detection model is more complicated than other computer \r\nvision models such as image classification models. Most of the images have several \r\nobjects to be detected, and the types of detections and their errors can be categorized \r\nin several different ways (e.g., classification error, localization error). To address this \r\nchallenge, we design and develop an interactive tool that helps users evaluate and \r\nanalyze results from object detection models. We first categorize detected and ground-\r\ntruth bounding boxes into 7 and 11 types based on the literature. We enable users to \r\nanalyze object detection results based on this categorization at three different levels: \r\nsummary level, class and detection type level, and image level. This enables users to \r\nanalyze a large number of and a variety of model errors from a summarized overview \r\ninto individual images. From the summary level, users can explore the overall \r\ndetection results such as average precision, the number of detected labels and ground \r\ntruth labels, and the number of each detection type. At the class and detection level, \r\nusers can see more detailed information about a certain class and detection type and \r\nimages that correspond to it. At the image level, users can click each image to get a \r\ndetailed analysis. We developed an interactive user interface that implements this idea \r\nwith a driving dataset, trained with a state-of-the-art object detection model. We also \r\npresent a usage scenario of how users can use our tool for inspecting errors from the \r\ntrained object detection model. Being able to easily browse object detection results \r\nand their images with a certain class and a detection type, our interface helps users to \r\nwork with object detection models more effectively in their research. Corresponding e-mail address: ohda@oregonstate.edu","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"nq1vwg3xGcg6CGv1SrFCHaW03/Va+2xWCvGOQdEHRfs="},"08cd9e85-9387-4429-a275-2869cf08a0fa":{"id_":"08cd9e85-9387-4429-a275-2869cf08a0fa","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"MeDYRHKsyPUU6FJKH+r+xnH/k8UAFoDnlX9lskwVqIg="}},"text":"©Copyright by Dayeon Oh  \r\nDefense Date (June 9th, 2022)  \r\nAll Rights Reserved","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Rt9nDd00cAgh8gQJKvBWFEHdtwpMNi6Ull/zHwT+Adw="},"da9acf49-a160-47e5-8193-f330274d4108":{"id_":"da9acf49-a160-47e5-8193-f330274d4108","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"GAAQaj/LYM3iDyvbKen2LRHlsRt0t0wKpOPg9btubwY="}},"text":"TABLE OF CONTENTS \r\n \r\n                Page \r\n1 Introduction  ................................................................................................................. 1 \r\n2 Related works............................................................................................................... 4 \r\n3 Methods and Design  ................................................................................................... 7 \r\n3.1 Tool development................................................................................................. 7 \r\n3.1.1 Summary view panel  ........................................................................................ 7 \r\n3.1.2. Chart view panel ........................................................................................ 14 \r\n3.1.3 Image view panel  ...................................................................................... 17 \r\n3.2 Post-processing  ................................................................................................. 18 \r\n4 Use cases .................................................................................................................... 21 \r\n4.1 Dataset and Model.............................................................................................. 21 \r\n4.2 User scenarios .................................................................................................... 21 \r\n4.2.1 Finding similarities between certain errors  .................................................... 22 \r\n4.2.2 Finding example images for specific conditions ............................................. 23 \r\n5 Discussion and Future work....................................................................................... 25 \r\nBibliography ................................................................................................................. 26","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"IU9cNj+ZcMrx7NNrj7V6kAi6YhdvrobE/HAbq5F3kwE="},"ad005b2b-09db-44fb-a9dc-f3780fc2f46b":{"id_":"ad005b2b-09db-44fb-a9dc-f3780fc2f46b","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"QslbS0du1pfiTj+nZlLvts4CtsPOo/6opMU2G3b/d78="}},"text":"LIST OF FIGURES  \r\n \r\nFigure                                                                                                                             Page \r\n1. The object detection error explorer. ................................................................................ 6 \r\n2. Ground truth result summary table from the summary view panel ................................ 6 \r\n3. Summary view panel....................................................................................................... 8 \r\n4. Detected label types ...................................................................................................... 10 \r\n5. Ground-truth label types ............................................................................................... 13 \r\n6. Image view panel .......................................................................................................... 14 \r\n7. Image modal window panel .......................................................................................... 16 \r\n8. Chart images for user scenarios .................................................................................... 19","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"J2nKvjDylgaLFzcXEAlmbwkmoMn7uBHnalt+Vugzk4s="},"25f35939-23a7-44d1-9359-372478da85a3":{"id_":"25f35939-23a7-44d1-9359-372478da85a3","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"C4fsjk7KVu/Haz49j30qx+x2pn+u0ZzUYrA4tV3bmDA="}},"text":"Error Analysis of Object Detection Models  \r\nwith Interactive User Interfaces \r\n1 Introduction \r\nRecent advancements in deep learning and hardware computing power have increased the \r\ncapabilities of computer vision (CV), a scientific field that defines how machines \r\nunderstand the meaning of images and videos like humans do. In general, CV tasks \r\nconsist of image classification, object detection, facial recognition, and image \r\nsegmentation. Among the CV tasks above, object detection, a task that works to identify \r\nand locate objects within an image or video has been one of the successful applications of \r\nCV in many fields such as robotics [8], transportation (e.g., autonomous driving [5]), and \r\neven in a medical [6] and construction area [7]. To use the object detection model more confidently, the interpretability of a model with \r\nits performance is particularly important because the end-users can assess how well the \r\nmodel performs on a specific object detection task. Moreover, the users can make more \r\ncritical decisions based on the results of the object detection model only when they \r\ninterpret and rely on the model. Also, if the model is interpretable enough, the end-users \r\nreadily understand the reasoning behind the predictions and decisions made by the model \r\neven without domain knowledge of computer science. The interpretability of the model \r\ncan be improved when end-users recognize and correct predictions (i.e., object detection) \r\nthat are incorrect. One key metric of the improved interpretability is its error debugging \r\nand intuitive diagnosis for the model’s detections and predictions. The users may need to \r\ndebug any errors made from the model by evaluating and analyzing its prediction \r\nperformance together with quantitative result tables of the model. Evaluating the performance of the object detection model is particularly challenging \r\ncompared to other CV tasks such as image classification. It is because the object \r\ndetection results can be categorized into many types, not just as correct and incorrect \r\nunlike other computer vision models such as image classification models. In further, \r\nseveral detected bounding boxes can be detected correctly but their detected class can be \r\nincorrect, and several bounding boxes are detected incorrectly but their detected class can","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NIQKvGJil2qejjDQMtMYqEV2Z+iEKCoe4TRUr0WzT7M="},"dfd3b528-3045-42c8-8d4a-153eb5eadeb8":{"id_":"dfd3b528-3045-42c8-8d4a-153eb5eadeb8","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"p7PTxsiRyfIJG3aVHiG5jhhheSnZj8/1lS00NXFACGk="}},"text":"2 \r\n \r\n \r\nbe correct. As a result, an effective performance evaluation method for object detection is \r\nneeded in our body of knowledge. Meanwhile, to evaluate the performance of the object \r\ndetection model, we may use various evaluation metrics such as mean average precision \r\n(mAP) and harmonic mean of precision and recall (F1-score), but they are not explaining \r\ndetailed information about the errors. Therefore, as mentioned above we can specify the detection errors into sub-categories to \r\nunderstand the errors of detection results in more detail. In addition, sorting out images \r\nthat contain particular labels, classes, or error types, and analyzing errors can be more \r\ncomplicated compared to other computer vision models. This issue is particularly \r\nimportant for non-computer science experts because it would be much harder for them to \r\nunderstand the model’s performance with their limited expertise and experiences. Previous research suggested various solution tools such as Voxel 51 [4], TIDE [1], and \r\nHoiem et al [2] to address the problems above. Their framework provides the interface to \r\nlook for the detection result of each image or provide chart information of the errors. Also, these works have categorized the incorrect detection results into several different \r\ngroups to provide users with error information that was difficult to understand with mAP \r\nor correct and incorrect results alone. However, previous research works still have \r\nlimitations as follows:  \r\n1. Focusing only on showing bounding boxes of each image or summarizing \r\ndetection errors causes a lack of information about the opposite. 2. Categorized error types do not include all the detected errors. This paper presents the object detection error analyzer [Figure 1], which is an \r\ninterpretability tool that supports the evaluation and analysis of the result of the object \r\ndetection model. Our tool will help users to work with the object detection model more \r\nefficiently by providing three different level approaches to the object detection results, \r\nwhich are summary level, class and detection type level, and image level. We designed it","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6pC4MhTTyZj0nc9dN3uEpJO+wgLQeicQj14QGz9LY2k="},"977cd463-5def-4f6f-88ac-64f794b6d454":{"id_":"977cd463-5def-4f6f-88ac-64f794b6d454","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"NLBvTGb/XeaI8K/lNp/ngneM7Bki0u05gniyQAGs9Sk="}},"text":"3 \r\n \r\n \r\ninto three different levels to provide adequate information, from the overall to detailed \r\nresults of the model, to match the users’ needs. Also, we expanded six different detection \r\nerrors created from the TIDE [1] into seven different categories including correctly \r\nclassified bounding boxes and other remaining incorrect bounding boxes which were not \r\nincluded in the former six detection errors. In addition, we also categorized the ground \r\ntruth bounding based on the detected bounding boxes and their detection status to show a \r\nrelationship between detected and ground-truth bounding boxes. First, at the summary level, we designed a table and distribution charts that contain the \r\noverall and summarized results of the object detection model. From the table, users will \r\nbe able to see basic information such as numbers and detected and ground truth bounding \r\nboxes, average precisions, and the number of labels belonging to each detection type \r\nbased on the classes. Users can also look for the distribution of the size of bounding \r\nboxes, IOU, and detection score of all detected labels through distribution charts. Second, \r\nin the class and detection type level, based on the types that we classified the detection \r\ntypes and classes users can look for the corresponding images, distribution chart results \r\nof bounding boxes, IOU, and detection score. Third, at the image level, we designed a \r\nformat that users can analyze all the detected and ground truth bounding boxes to get \r\nmore detailed information about each selected image. With these features, this tool can support end-users to check the performance of the \r\nobject detection model from overall summary to small detail. Also, it can help them to \r\nmake better decisions for their purposes with reliability and accountability by effectively \r\ncombining interpretability in the object detection model. Furthermore, from a technical \r\npoint of view, this tool provides visualization of the performance and presents a \r\nsystematic analysis method. This paper will first describe the dataset, models, and post-\r\nprocessing methods, and then will describe the tool of design, and walk through some \r\nuser scenarios.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"BxAxRoBE+mqKNEchLZuGBYj7TTMYHJROsCgrOh0ydX0="},"d8fe3430-8d03-473e-bffe-417e63921e00":{"id_":"d8fe3430-8d03-473e-bffe-417e63921e00","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"cfkE1yU96Ao5MH6ZTweZzFZWrYKr6WWk0KhW5JdBCuM="}},"text":"4 \r\n \r\n \r\n2 Related Works \r\nAcademy-wise, mAP score has been traditionally used as an indicator to measure the \r\nperformance of object detection models, despite its drawback that it does not provide \r\ninsights into the errors. Therefore, many researchers have made efforts to come up with \r\nbetter tools to measure the performance of the object detection models, especially in \r\nmany cases by providing analytical information about the errors. Hoeim et al [2], and \r\nTIDE [1], created their own labels to categorize the object detection errors. Hoeim et al \r\n[2], showed which aspects influence the object detection and categorized the false-\r\npositive errors into four different categories: localization error, confusion with similar \r\nobjects, confusion with dissimilar objects, and confusion with the background. They \r\nshowed the results of the false-positive errors of each class in the form of a pie chart and \r\na bar chart. TIDE [1] categorized the errors into 6 different categories: classification \r\nerror, localization error, classification and localization error, duplication error, \r\nbackground error, and missed detected error. They also used pie charts and bar charts to \r\nshow the distribution of errors in each class and showed charts for different object \r\ndetection models to compare performances. They compared different models and \r\ndifferent datasets to evaluate the performance. In contrast, we have extended their 6 \r\ndifferent error categories to categorize the object detection results. Borji et al [3], introduced a new evaluation method called upper bound average precision \r\n(UAP). They calculated the UAP of each class, and also compared about 15 different \r\nobject detection models, and evaluate and visualize the performance with graphs and \r\ntables. They showed the performance of different models for each class by using different \r\nicons and displaying them in the same bar, using the y-coordinate to illustrate the \r\nperformance. Google open image dataset [12] created a browsing tool for images. It is not \r\nspecific for the object detection models, but users can select filters such as class, train, or \r\nvalidation data to browse corresponding images. It allows users to browse individual \r\nimages with objects and their bounding boxes and classes. Voxel 51 [4] introduced an","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"uprDjjPVa54mqBUQLLZ2pc9ld/qtB+//TjusuKZHro8="},"5a707c87-14bc-4ae7-bb39-506b6f9ee47e":{"id_":"5a707c87-14bc-4ae7-bb39-506b6f9ee47e","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"/Kn19ZdFcno81syr8RQDYdAhBZZ4ZqxD2leLLLph3Ko="}},"text":"5 \r\n \r\n \r\ninteractive tool to explore and browse the object detection model. They helped users to \r\ninvestigate the object detection results with the filtering functions such as labels, \r\ncategories, and detection scores. It is possible to create a summary result that shows \r\nprecision, recall, f-1 score, and support for each class. However, it has a shortcoming in \r\nthat users have to write a python code to use this tool. Hoeim et al [2], TIDE [1], and Borji et al [3] showed the performance of object detection \r\nmodels by summarizing errors into the types they have created. Voxel 51 [4] and Google \r\nopen image dataset [12] displayed the images that users have filtered. When evaluating \r\nthe performance of object detection models and debugging them, it is important for users \r\nto intuitively understand how the objects were detected when they look at the results. Because of this reason, many researchers have tried to categorize and show errors with \r\nsimple and compact visualizations. However, 4 different false-positive error types and 6 \r\nerror types that previous research has created do not contain all the detection results \r\nwhich limit the explanation of the results. In addition, showing summary results with pie \r\ncharts and bar charts is easy to check the percentage of each error type, but it is difficult \r\nto immediately check in detail which images and under what circumstances each error \r\noccurs. In this regard, we create an interactive tool that displays a summary, charts, and \r\nimages all together to provide a simultaneous exploration of the results with example \r\nimages. Also, we create new categories that include all the detected errors to allow users \r\nto browse all the objects and images based on the selected category. In this way, users \r\ncan recognize the similarities among the objects that belong to the same category and \r\ncheck the overall performance of object detection models.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"T0zVG9cbj0LsmCi7LEXuG4FjZD/SfkkdavK37DxayYs="},"f4cb7f0e-d75b-481f-8e33-fcf62ce64d8f":{"id_":"f4cb7f0e-d75b-481f-8e33-fcf62ce64d8f","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"a4NOv/AYgM54Ka8CYF5Va+i1FaEnmO5yJN6ClT+/p/A="}},"text":"6 \r\n \r\n \r\n \r\nFigure 2. Ground truth result summary table from the summary view panel. Figure 1. The Object Detection Error Explorer. Summary view panel (A), Chart view \r\npanel (B), and Image view panel (C).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CgdC0/5LM0bpm5TpoJkEGH3FdeSMGDhiOGBtKUui56w="},"470a9825-31bf-43b9-8604-02e04e11abf2":{"id_":"470a9825-31bf-43b9-8604-02e04e11abf2","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"DJOecUUPPME0vNfPvFBoWGAZfR19XzFD/Sn7xWR/7jI="}},"text":"7 \r\n \r\n \r\n3 Methods \r\nThe object detection result analyzer is an interactive tool that supports the evaluation and \r\nanalysis of the results of the object detection model. The objective of our research is to \r\ndevelop a tool that enables debugging object detection models and verifies the features of \r\nthe tool. For this purpose, we design the tool as a one-page web application with the \r\nfeatures to analyze and evaluate the object detection results. In addition, we use a realistic \r\nuser scenario to verify the function of the tool. We design three different panels to debug \r\nthe object detection results by categorizing the object detection ground truths and \r\ndetection labels, displaying distribution charts of different values such as IoU, detection \r\nscore, and bounding box size. Finally, we display images with the bounding boxes to let \r\nusers explore and interpret the object detection results easily. 3.1 Tool development \r\nThis section describes how we design the tool to implement the features to accomplish \r\nour research goal. Object detection result explorer is a one-page web application that \r\nprovides three different views to analyze the object detection results: summary level table \r\nview, chart view, and image view. 3.1.1 Summary view panel \r\nWe design a summary result table [Figure 1. A] to display basic information about the \r\nobject detection results. It contains the number of detected labels and ground truth labels \r\nof each class and their average precision values. We design a detected result summary \r\ntable and ground truth summary table to check how many labels are in each class at a \r\nglance. One of the main information is the number of detected labels of each detected \r\ncategory by each class. We categorize the detection result and ground truth values in \r\nseveral different categories based on the literature so that end-users can easily explore the \r\nobject detection results of their interests.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"cT/oBhYRU8ZQ0i5NpWP6XBliGsGW9hML7Vytj5uc1uY="},"9be4c487-aa46-4661-8b15-a60ae02125fc":{"id_":"9be4c487-aa46-4661-8b15-a60ae02125fc","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"Dde/jNo+lIyuTfV1Cczr80f1gwXKdgBCJqB56Uv/SWQ="}},"text":"8 \r\n \r\n \r\nThe most frequently used evaluation metric for the object detection model is the mean \r\naverage precision (mAP). This measurement can show the accuracy of the model’s \r\nperformance with a simple numeric number. However, it is difficult to conduct a detailed \r\nanalysis of the object detection model’s results with this single result alone because it \r\ndoes not show any information about the incorrectly detected labels. Therefore, we \r\ncategorize the detected labels into 7 different categories. In addition, to provide insights \r\nregarding the relationships between detected labels and ground truth labels, we categorize \r\nground truth labels into 11 different categories. Figure 3. If users click the cell, the chart panel and image view panel update \r\nautomatically with corresponding values.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"M8Qh9M7ONux58rD0J8+JJp/Ojcg6uEV349/q9qUJQAE="},"2cea6c07-43c3-4ac4-8da1-25cf37ac4028":{"id_":"2cea6c07-43c3-4ac4-8da1-25cf37ac4028","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"/V1X799ZtkKAzrkm1skQ/bC6mVc0bV5qw66iGS7LNv0="}},"text":"9 \r\n \r\n \r\n \r\nDetected label types \r\nWe create 7 different detected labels to categorize every detected label based on the 6 \r\nerror types from TIDE [1]. In accordance with the TIDE [1], the authors categorized the \r\nincorrectly detected labels into 6 different types: classification error, localization error, \r\nclassification and localization error, duplicate detection error, background error, and \r\nmissed ground truth error. However, several incorrectly detected labels do not belong to \r\nany of the categories. Therefore, we have revised the categories into 7 different types and \r\nincluded correctly detected labels to categorize all the detected labels. The following \r\ndetected types (see fig.4) are calculated based on the two different IoU thresholds: \r\nforeground IoU threshold (tf) 0.5 and background threshold (tb) 0.1 [1]. 1. Correctly detected: largest IoU and IoU >= tf for ground truth bounding box and \r\ndetected class are correct. 2. Classification error: max IoU >= tf for ground truth bounding box but the detected \r\nclass is incorrect. 3. Localization error: tb <= max IoU <= tf for ground truth bounding box, and the \r\ndetected class is correct. 4. Classification and Localization error: tb <= max IoU <=tf for ground truth \r\nbounding box, and the detected class is incorrect. 5. Duplicate detection error: max IoU >= tf for ground truth bounding box but there \r\nis another bounding box with a higher IoU score exist. 6. Background error: max IoU <= tb for ground truth bounding box. 7. Other Incorrect errors: other detected bounding box that does not belong to the \r\nabove 6 types. From the existing categorization methods from the TIDE [1], incorrectly detected labels \r\nwith the characteristic of type 7 from our category and correctly detected labels did not \r\nhave their type. We categorize all the labels into one of the categories to let users easily \r\nexplore all the images based on certain detected types and classes. In the post-processing \r\nstep, we calculate the max IoU score for every detected label. For each detected label, we","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KCrjPKSQH4N2mSLlculp3bS8RC6Q3ossNbvIsGz/xnw="},"c00919a2-8f50-4bcb-86a9-7591b6d34751":{"id_":"c00919a2-8f50-4bcb-86a9-7591b6d34751","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"mLtdmg6KA38EKeVO6k0PDFGHqlf8yciDbDhmYhO4quI="}},"text":"10 \r\n \r\n \r\ncategorize it into one of the 7 detected categories based on its IoU score and its predicted \r\nclass. Figure 4. Detected label types \r\n \r\nGround-truth label types \r\nWe also create 11 different ground-truth label types (see fig.5) to categorize all the \r\nground-truth labels based on the relationship with detected labels. When analyzing the \r\nresults of the object detection model, exploring both the ground truth labels and the \r\ndetected labels together may provide more diverse perspectives than when analyzing only \r\ndetected labels. If both detected and ground truth bounding boxes are shown together, \r\nend-users can analyze the performance more accurately when the ground truth’s \r\nrelationship with detected boxes is shown than when there is just basic location \r\ninformation on ground truth. For example, there should be cases where a certain ground \r\ntruth object is properly detected only once and another one is done several times. In that \r\nsituation, by accumulating and analyzing the images of such cases, end-users can have a \r\nbetter understanding of the commonalities and features, which is more useful than the \r\nplainly expressed ground truth.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EsqzWgrN6CQhBdFVULlEEXr/khAI+8uywWIfRzQwNZ4="},"aa08b1c1-2826-4606-add4-656b76e62f61":{"id_":"aa08b1c1-2826-4606-add4-656b76e62f61","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"5kKNr9v0v004Lxwiy/ZxJWffyzXRRj/7HQOaXQqtjMo="}},"text":"11 \r\n \r\n \r\n1. One match + Correct label: Matches with only one detected label and the \r\nmatching detected label is type correct. 2. One match + classification error: Matches with only one detected label and the \r\nmatching detected label is classification error. 3. One match + localization error: Matches with only one detected label and the \r\nmatching detected label is localization error. 4. One match + classification, localization error: Matches with only one detected \r\nlabel, and the matching detected label is classification + localization error. 5. One Match + incorrect error: Matches with only one detected label and the \r\nmatching detected label is another incorrect error. 6. Matches + Correct and duplicate errors: Matches with more than one label and all \r\nthe detected labels are detected with the correct category  \r\n . All the detected labels are correct and duplicated errors, detected \r\ncategories match with ground truth labels, and all IoU >= tf \r\n7. Matches + Correct and classification errors: Matches with more than one label \r\nand all the detected labels have IoU >= tf but include correct and incorrect labels. 8. Matches + Correct and localization errors:  Matches with more than one label and \r\nall the detected labels are detected with the correct category. . Including detected labels with IoU >= tf and tb <= IoU < tf. 9. Matches + Include correct: Matches with more than one label and include at least \r\none correct label. 10. Matches + Incorrect: Matches with more than one label but all the detected labels \r\nare incorrect. 11. Missed detected error: No match with the detected label. Type 11 in the ground truth category was one of the 6 types of errors made from TIDE \r\n[1], which is an undetected ground truth not already covered by classification and \r\nlocalization error (false negative). TIDE [1] put this error into their 6 types of errors, but \r\nwe put this false-negative labels into one of the ground truth categories because we","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"7/jEYfpBLZMmoIajtwHZ8zlMToW0CS/Hw+DbfADLjns="},"5f51e9a7-6ac1-4087-9e9e-7da646d09dc4":{"id_":"5f51e9a7-6ac1-4087-9e9e-7da646d09dc4","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"uE11D+gp7V3Raq9o2KJbVLKM0hPgmot3i/1cTkvplr8="}},"text":"12 \r\n \r\n \r\ncategorize all the ground truth labels and separated them with the categories of detected \r\nlabels. We use the matching index between detected labels and ground truth labels that \r\nwe calculate through the post-processing steps to categorize the ground truth labels. Based on the created categories and classes, users can click the cell of detection \r\ncategories and class to see corresponding images to the cell from the detection summary \r\ntable. When the users hover over each cell of the table, the letters on the selected cell turn \r\ninto bold to notify the cell that the users are hovering over. The background color of the \r\ncell will also change to a darker color when the users click the cell, and the chart view \r\nand image view will update automatically based on the class and detection type that users \r\nselect. [Figure 3] The cell with highlighted color will change when the users click a \r\ndifferent cell, and the chart and image will automatically update. If the users double click \r\nthe highlighted cell, the color of the cell goes back to the default color, and the chart and \r\nimage view will also go back to the default view. Next to the result summary title, there is a toggle switch button that changes between \r\ndetection result and ground truth result. The ground truth summary table shows the \r\nnumber of each ground truth category for each class. (See fig.2) In the class column, \r\nthere is a color block next to the name of each class. Each color block shows the color of \r\nthe bounding box depicted on the image view panel.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9wYpo0CRneU/YAZwb8Suf1nHzxQeriMRM0Cl8EIvhK0="},"682f054a-ca2a-4c88-bd6c-534b5e04c83e":{"id_":"682f054a-ca2a-4c88-bd6c-534b5e04c83e","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"CFBBIeaS3vTlX7ox9qOhmHz8zl6RCVwdIYBpTPzxpXU="}},"text":"13 \r\n \r\n \r\n \r\n \r\nFigure 5. Ground-truth label types","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gevQs8EfWxSxzQSgj3llGJlq+J8jcOqx+QAyvNWDV6E="},"9c0cc5e4-ddf0-4aa2-8459-1ea9a802d659":{"id_":"9c0cc5e4-ddf0-4aa2-8459-1ea9a802d659","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"4eeoNBG1k3s0XLc8RGjCjw3706inNQySYeRHxtR89AI="}},"text":"14 \r\n \r\n \r\n \r\nFigure 6. When users click one of the bars from the chart view panel, the image view \r\npanel updates automatically with corresponding images. 3.1.2 Chart view panel  \r\nA chart view panel is located on the bottom left of the tool (see Figure 1. B). It shows \r\nthree different bar charts: the distribution of the size of bounding boxes of detection \r\nlabels, the distribution of the IoU score of detection labels, and the distribution of the \r\ndetection score. We select these three aspects to see correlations between different \r\ndetection categories. On the default page [Figure 1], the chart panel shows three \r\ndistribution bar charts for the whole detected labels, and the color of the default bar chart \r\nis light blue. The bounding box bar chart has 10 different labels, from the size 10 to \r\n100,000, the detection score distribution bar chart has 9 different labels, from 0.2 to 1.0, \r\nand the IoU score chart has 10 different labels, from 0.0 to 0.9. The labels are created \r\nbased on the minimum and maximum values of each chart value. When the users click one of the class and detection cells from the summary table, the \r\nchart view panel gets updated based on the detection labels belonging to the selected \r\nclass and detection. The color of the bar chart for the selected class and detection type is","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KhS7HVt9SP58k+AhXdymdruZIbTKiII/oLli19A/HNU="},"9bf1c6a2-cd22-4914-8ee3-3b5399c5e96b":{"id_":"9bf1c6a2-cd22-4914-8ee3-3b5399c5e96b","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"pq2rBtF2S+eeOJouK+XUuWqI5x/KTib2q+d7WQ7L0+I="}},"text":"15 \r\n \r\n \r\nlight yellow. We use different colors to differentiate between the default distribution \r\nchart and the selected class and detection chart. In the selected class and detection chart, \r\nusers can select one of the bars to browse corresponding images in the image view panel, \r\nand the color of the selected bar turns darker yellow. The users can reverse their choice by \r\ndouble-clicking the selected bar, and the image view panel shows the whole selected and \r\ndetection images, and the selected bar will go back to light yellow. The default three distribution charts are the bounding box size chart and IoU score chart. However, there is a variation depending on the detection category. The classification \r\nerror category is when the detected bounding box has a higher IoU score than the IoU \r\nthreshold, but the detected category is incorrect. It is important to know how the detected \r\ncategory is incorrectly detected in this case. Therefore, we add one more distribution \r\nchart called the missed detected class chart which shows how the detected labels of the \r\nselected class are incorrectly classified. The labels in the missed detected class chart are \r\nthe names of the classes that detection labels incorrectly classified, so the number and \r\nnames of the label are different depending on the selected class. The users can also click \r\none of the bars to see corresponding images on the image view panel. (see fig.4) In \r\naddition, the background detection error category is when the detection result detected \r\nthe background. Therefore, there is no IoU distribution chart for this case because there \r\nare no matching bounding boxes between the ground truth bounding box and the detected \r\nbounding box.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"7XrMluX03iLJ+LINqM1r7ZmfgP4UNibAS2hMdZr9T9k="},"f1c7c845-e5fb-4a0f-b30b-e110dadfd894":{"id_":"f1c7c845-e5fb-4a0f-b30b-e110dadfd894","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"LXLnogcwA7qnbB2MNs7Q1ua7zZitgNIMH3ttxbseuxI="}},"text":"16 \r\n \r\n \r\n \r\nFigure 7. The modal window for each image for more detailed information.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FKFsfexxEGRu06OSAq71DP3IKWB2A5LAFvOA7BW2T/E="},"a811ea6f-c29e-49f5-b5c2-07a78bf1178f":{"id_":"a811ea6f-c29e-49f5-b5c2-07a78bf1178f","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"XAgu16uael8EEW31oMBAvDr4Ffc9q3XHsqYENcSlfco="}},"text":"17 \r\n \r\n \r\n \r\n3.1.3 Image view panel  \r\nThe image view panel is located on the right side of the tool (see Figure 1. C). The \r\ndefault image view shows the first 500 images from the BDD 100k dataset [9]. We use \r\n13 different colors to represent each class, and the color is shown in the summary table. To represent the detection result and ground truth result, we use a solid rectangle and \r\ndotted rectangle, so the users can see both detected bounding boxes and ground truth \r\nbounding boxes of each image at once. Also, we use a pop-up window for each image to \r\nshow all the detected bounding boxes and ground truth bounding boxes on a bigger \r\nscreen. When the users select one of the cells in the detection summary table, the image view \r\npanel shows the corresponding images. For each class and detected type, we find the \r\nimage index that contains the selected class and detected type. Then, we calculate the \r\nimage index and the number of detected labels that belong to the selected class and \r\ndetected type and sort the list of image indexes based on the number of detected labels. Therefore, in the image view panel, the order of the image is the number of detected \r\nlabels that belong to the selected class and detection type. The selected image view only \r\nshows detected and ground truth bounding boxes that belong to the selected class and \r\ndetection type. Users can hover over the detected bounding box to see the ground truth \r\nclass, predicted class, and detection score. At the selected image view, users can click each image for more detailed information. The modal window pops up when users click any image. As we can see in fig 5, there is a \r\nlist of classes with ground truth and detected checkboxes and a list of detected error \r\ntypes, where users can check to see corresponding bounding boxes. When users first open \r\na modal window, the selected class and error type from the table summary is selected as a \r\ndefault, and users can toggle checkboxes to see other bounding box information.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rra3qacLdc0bGmKLmpSm+kSP0UTEqgcXxybonKPH3XQ="},"499f71a5-72b7-4f83-a68c-36cab3252048":{"id_":"499f71a5-72b7-4f83-a68c-36cab3252048","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"RFKGw7ayFytZI16FhFb3fWEaFjTY/4lzWd3hnGN4YCM="}},"text":"18 \r\n \r\n \r\n3.2 Post-processing  \r\nFrom the detection result of the object detection model, several post-production steps are \r\nrequired to have an adequate data structure for our tool. First, before categorizing the \r\ndetected and ground truth labels. For each image, we compare every detected bounding \r\nbox with ground truth bounding boxes to find a matching ground-truth bounding box for \r\neach detected bounding box. From every detected bounding box from one image, we \r\ncalculate all the Intersection over Union (IoU) between a detected bounding box and all \r\nthe ground truth bounding boxes from the same image. Then extract the max IoU value \r\nfrom the list of calculated IoU scores and its corresponding ground-truth label id. In this \r\nway, it is easy to check the detected and its corresponding ground-truth label. When \r\ncalculating the IoU, we set the default IoU value as -1, and if the maximum IoU is -1, \r\nthen it means there is no match between the ground-truth bounding box and the detected \r\nbounding box. In this case, we left the matching ground-truth id field and max IoU value \r\nfield empty. After we find the max IoU value and its matching ground truth label for \r\nevery detected bounding box, we use the well-known object detection post-processing \r\nalgorithm non-maximum suppression (NMS) to reduce the overlapping detected \r\nbounding boxes. NMS algorithm uses the detection score and IoU value to calculate \r\nwhich labels to remove. It designates one label with the highest detection score from the \r\ninput list as a standard and compares the IoU score with the pivot label. If the IoU score \r\nbetween the pivot bounding box and another bounding box is higher than the IoU \r\nthreshold parameter (0.5), it deletes the labels to remove overlapping bounding boxes. After we apply the NMS algorithm and reduce multiple overlapping labels, we calculate \r\nthe mean average precision (mAP) for each class. Average precision (AP) is a widely \r\nused evaluation metric to check the accuracy of object detection models, which computes \r\nthe average precision and recall curve. To calculate the precision value, we need to \r\ncompute true positive values over the summation of true positives and false positives, and \r\nto calculate the recall value, we need to compute the true positive values over the \r\nsummation of true positives and false negatives. In the object detection model, we","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5gNAxbqkMiohCJwhCrzgXhXClvvDRiszmv0ktsf7NGw="},"01ff7575-669d-4e89-955e-ae32ba2660e4":{"id_":"01ff7575-669d-4e89-955e-ae32ba2660e4","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"i37Kr99KclhnpH8KtEMhyr1ahe+wm3XFD2imcuVxIkE="}},"text":"19 \r\n \r\n \r\nconsider one detection as a true positive when the predicted class is the same as the \r\nground-truth class and also the IoU value is larger than the IoU threshold. A false positive \r\nis either when the predicted class is different from the ground-truth class or the IoU value \r\nis smaller than the IoU threshold. A false negative value will be the undetected ground-\r\ntruth bounding box, and a false positive does not apply to an object detection model. We \r\nuse 10 different IoU thresholds, from 0.2 to 0.7, to calculate the true positive and false \r\npositive, and calculate the mAP score for each class. a) Car localization error bounding box distribution chart \r\n \r\n \r\nb) Pedestrian localization error class distribution chart \r\n \r\nc) Car localization error detection score distribution chart","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"e0dUg/73t+V5i8gORJU1SvpotMZxtHVT9N9hTos52lQ="},"a6c9616b-a803-47a0-9045-e3c0ba954da2":{"id_":"a6c9616b-a803-47a0-9045-e3c0ba954da2","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"Kt9uFsnIL0GsizRUnVx3dfrc4eg4pGR731uBoAEBzMw="}},"text":"20 \r\n \r\n \r\n \r\nd) traffic light classification error bounding box distribution chart \r\n \r\n \r\n \r\ne) traffic light classification error detection score distribution chart \r\n \r\n \r\nf) traffic light classification error IoU distribution chart \r\nFigure 8. Chart images for user scenarios","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"3kKZtGn9jXPG4eHID8SMVqDHcAe3cH3lWu0RxqNeDTs="},"bd7097a1-3877-4a4d-8d28-d8f1bab7de38":{"id_":"bd7097a1-3877-4a4d-8d28-d8f1bab7de38","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"odJ1gOwXmt6OAu8fi8iGE0/ptSKR2OSSxAXtVe4xrOk="}},"text":"21 \r\n \r\n \r\n4 Use cases \r\nIn this section, we want to verify the functionality of the tool through a user scenario \r\ncase. For the use case scenario, we will create a persona called Amy as a potential user of \r\nour tool. We assume that she has basic background knowledge about machine learning. She wants to use this tool to answer the questions that she has had in her autonomous \r\ndriving research. 4.1 Dataset and model \r\nFor the experiment, we have chosen the BDD 100K driving dataset [9], a well-known \r\nobject detection dataset, which is also known to have a very large-scale dataset with more \r\nthan 10 object categories. There are a couple of experiment objectives to lead us to select \r\nthis dataset: Firstly, we hope to investigate how classes with similar properties are \r\ndetected/differentiated and to produce detection results from a minimum of 7 to 8 classes \r\nas well as from distinct objects. Given this dataset has 13 classes, we can perform this \r\nexperiment objective without hurdles. Secondly, we want to test whether external factors \r\ninfluence detection results. Given this dataset has several external properties such as \r\nbrightness and darkness (daylight and night-time), and occluded or blurred, it enables us \r\nto carry out the second experiment objective. We have chosen Faster-RCNN \r\nResnet50_v1 [10, 11] pre-trained model to train and test the dataset. We fine-tuned it \r\nwith the BDD 100k dataset. The total mAP value with COCO 2017 dataset was 29.3, \r\nand the total mAP fine-tuned with BDD 100K train dataset was 25.3. Classes like other \r\nperson, other vehicle, trailer, and the train had only 2 to 15 ground truth labels and \r\ndetected labels are usually zero. In addition, the value of the mAP calculated without the \r\n4 classes was 36.6, which was enough for the experiment. 4.2 User scenario \r\nWe create a scenario where our persona uses our tool to answer her research questions. She will investigate the cause of errors while interpreting the results of the object","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DOPCqmeu3E/n3jZ2fTuWfFtEhHdt6TksbA6fFKN43b0="},"1b0a6177-a7e1-4d46-b046-ade8c2cf5531":{"id_":"1b0a6177-a7e1-4d46-b046-ade8c2cf5531","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"U4nYhX6uyEtx+G0mNB19tbGOFIjGakMf/kB48lWj/F8="}},"text":"22 \r\n \r\n \r\ndetection models. Frequently occurring classification and localization errors are her main \r\nfocus. 4.2.1 Finding similarities between certain errors \r\nClassification errors. Amy first looks for the summary view panel to view the overall performance of the \r\nFaster-RCNN [10, 11] model. She finds that there are many classification errors among \r\nthe classes, so she wants to look for any common characteristics in those images or \r\nobjects. First, she clicks the ‘car, classification error’ cell from figure 1. A, to see how the \r\nobject ‘cars’ are incorrectly classified as other classes. In the image view panel, she \r\nchecks images that have detected labels mis-detected as cars. Then from the chart view \r\npanel, she checks the classification error distribution chart, and she finds that many trucks \r\nand buses are mis-detected as cars. After looking at the images from the image view \r\npanel she first finds that many labels are incorrectly detected as cars when their ground \r\ntruth labels are trucks, mostly the cases are when the objects are white vans or very small. She also finds that many of the mis-detected objects are placed in the dark or shadowed \r\nplaces. Second, she clicks on the ‘bicycle, classification error’. From the chart distribution panel, \r\nshe clicks bars of the large size bounding boxes from the bounding box size distribution \r\ncharts to see corresponding images and finds the similarity that many objects are also \r\nplaced in the night scene or shadowed. Third, she clicks the ‘traffic light, classification \r\nerror’ cell to see any common characteristics among the images. Front the chart view \r\npanel, she looks at the bounding box and detection score distribution chart and finds that \r\nmost of the bounding boxes have small sizes and the detection score is quite low. Also, \r\nthe classification error distribution chart shows that many traffic signs are mis-detected as \r\na traffic lights. Then she looks at the image view panel to browse corresponding images \r\nand finds that most of the objects that are incorrectly detected are commonly from night \r\nscene images and the objects are rather small or shadowed.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4cxjXNxVnYncozRVhkK8VyT8/CaKisJmVgUUxoIWZts="},"c6dfa17e-d9bc-450f-80f0-67fbac213059":{"id_":"c6dfa17e-d9bc-450f-80f0-67fbac213059","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"1TxM0Ud7f7nIvroB/LKeLEF2IVra1p2E5rzYgODaSBI="}},"text":"23 \r\n \r\n \r\n \r\nLocalization errors \r\nAmy then clicks the ‘car, localization error’ cell to see when and how the localization \r\nerrors occur. From the chart panel view, in fig 6. a, she finds that the size of the bounding \r\nboxes is small, and detection scores are distributed in the low values (see fig 6. c). Then \r\nfrom the image panel view, she finds that localization errors are closely located so she \r\nclicks the image to popup the modal window to have a close look at them. After clicking \r\nground truth and localization error together there, she can compare the sizes of the \r\nground truth bounding box and detected bounding box in a bigger window in more detail. After browsing several images, she finds that many localization errors frequently \r\noccurred when the objects were stuck together, and small or occluded objects tended to \r\nhave more localization errors than clear and big objects. She also finds that there are \r\nseveral objects with higher bounding boxes that have higher detection scores, and most of \r\nthe objects with localization errors are small. After that, she clicks the ‘pedestrian, \r\nlocalization error’ cell to see the characteristics of the corresponding images. After \r\nbrowsing the chart panel view, she finds that ‘pedestrian, localization error’ has a similar \r\ndistribution to ‘car, localization error’ (see figure 6. C and fig 6. h) Then she looks at the \r\nimage panel view and discovers that similar to the previous findings, many objects are \r\nstuck together, and the sizes are very small. 4.2.2 Finding example images for specific conditions \r\nMis-detection between pedestrians and vehicles and finding ground truth errors \r\nAmy wants to explore this object detection model to answer the question of whether there \r\nare cases where pedestrians are mis-detected as vehicles due to the concerns that \r\nconfusing pedestrians and vehicles are quite detrimental in autonomous driving. Then, \r\nshe first clicks the “car, classification error’ cell, then looks at the classification error \r\ndistribution chart, and clicks ‘pedestrian’ in the bar chart. After looking at several \r\nexample images, she realizes that the pedestrian objects are placed right next to the cars","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GAq2H7/DYSPB3Go9yT8QIPONwqTbQ/kqCPR8QeWPxHc="},"7889168a-2e9d-4acb-9c20-8c5f8a5c980e":{"id_":"7889168a-2e9d-4acb-9c20-8c5f8a5c980e","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_29","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"CLSKs7yqZCy83EVEXqWdvN7PIg2fc9u45/v/1LNj/Vk="}},"text":"24 \r\n \r\n \r\nand the objects are very small. She next clicks the “pedestrian, classification error” cell, \r\nand looks for classification errors as “car” (see fig 6. b), to see the objects which are \r\ndetected as pedestrians, but the ground truth is cars. However, she finds out some objects \r\nfrom the examples were detected correctly. So, she clicks one of the images to look for \r\nmore details and double-check the ground truth, detected bounding boxes, and finds that \r\nthe ground truth is wrong. Missed to detect pedestrians and vehicles \r\nAmy also pursues answers on the question of when pedestrians or vehicles are missed to \r\nbe detected due to the concerns about the safety in that situation by using the object \r\ndetection model. detected properly? ” To see the undetected objects, she first switches the \r\ndetection summary table to the ground truth summary table by clicking the toggle button. Then, Amy selects “pedestrian, undetected”, and “car, undetected” cells to see common \r\ncharacteristics of the undetected objects. From the chart view panel of “pedestrian, \r\nundetected”, she finds that the size of the bounding boxes is very small and checks the \r\ncorresponding images. Most of the undetected labels were on the sidewalks or the road, \r\nand there were densely placed in the images. In the case of the chart view panel from the \r\n“car, undetected” cell, she also finds that the size of the bounding boxes is also very small \r\nin this case and checks the corresponding images. She clicks one of the corresponding \r\nimages to open the modal window and compares the correctly classified ‘car’ and missed \r\ndetected ‘car’. In this case, lots of undetected ‘car’ objects were on the opposite side of \r\nthe roads, outside of the roads, and they were very small. Therefore, based on those \r\nfindings, Amy concludes that the missed detected ground truth labels are not critical to \r\nsafety in driving, but she could run more training to improve the model.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GEM9koTCaE5iYZ0OKmKobzSicrz9YUbkHKBmH9jQpXw="},"027bd69f-6095-453c-89ef-2708d8824b06":{"id_":"027bd69f-6095-453c-89ef-2708d8824b06","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"PUSs6abvAU5hKE6p/JBGLzlBxYBenNBxTPdQQEuK32Y="}},"text":"25 \r\n \r\n \r\n5 Discussion and Future work \r\nThe objective of our research is to develop an interactive tool that helps evaluate and \r\nanalyze the results of the object detection model. We believe that our tool can help non-\r\ncomputer science experts to evaluate the performance of the models and answer their \r\nquestions by finding common characteristics in certain errors by browsing the images of \r\ntheir interests. Based on the analysis of the object detection results obtained from the tool, \r\nusers can decide to use this model for their research or not or decide to train further to get \r\nhigher accuracy. In addition, they may understand in which cases the model performs \r\npoorly. Despite our efforts herein, there are limitations in this work. This tool needs to \r\nenhance explainability and interpretability in order for users to use the object detection \r\nmodel with more ease and confidence. Specifically, our current work has not provided \r\nclear answers to the question of ‘why’ this model detected objects correctly or \r\nincorrectly. Instead, by showing the images and the results, the tool helps users to infer \r\nthe black box of the model to a certain degree. Therefore, in future works, we need to \r\nimprove the explainability by showing the images of users’ interests such as using a grad-\r\ncam to show how a model detects an object. Future works may also include exploring \r\nchart values and images, intersecting with the ground truth and detected object categories, \r\nmaking it possible to browse more detailed and specific conditions.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"qXTw2g2IdRzituoTRnVEo4EavoTJEUEhA9x1s6agp5A="},"db5294e9-0416-4e1e-8489-539433d4a846":{"id_":"db5294e9-0416-4e1e-8489-539433d4a846","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"owyVAaPQMMNOZDZq7f7iV3AP0Ubl9aqVCgMvsWM4eUw="},"NEXT":{"nodeId":"101c6465-fd0d-4f8e-88b0-733792c62bc4","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"PfYcav9D3TF8yG/FklTLsMcm0nrKBhDO7E1+UrEwgCI="}},"text":"26 \r\n \r\n \r\nBibliography \r\n[1] Boyla D., Foley S., Hays J., Hoffman J.: TIDE: A General Toolbox for Identifying \r\nObject Detection Errors. In: ECCV (2020)  \r\n[2] Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In: \r\nECCV (2012) \r\n[3] Borji, A., Iranmanesh, S.M.: Empirical upper-bound in object detection and more. arXiv:1911.12451 (2019) \r\n[4] Voxel 51. https://voxel51.com/. [5] S. H. Naghavi, C. Avaznia and H. Talebi, \"Integrated real-time object detection for \r\nself-driving vehicles,\" 2017 10th Iranian Conference on Machine Vision and Image \r\nProcessing (MVIP), 2017, pp. 154-158, doi: 10.1109/IranianMVIP.2017.8342340. [6] M. Vas and A. Dessai, \"Lung cancer detection system using lung CT image \r\nprocessing,\" 2017 International Conference on Computing, Communication, Control and \r\nAutomation (ICCUBEA), 2017, pp. 1-5, doi: 10.1109/ICCUBEA.2017.8463851. [7] C. -C. Liu and J. J. -C. Ying, \"DeepSafety: A Deep Learning Framework for Unsafe \r\nBehaviors Detection of Steel Activity in Construction Projects,\" 2020 International \r\nComputer Symposium (ICS), 2020, pp. 135-140, doi: 10.1109/ICS51289.2020.00036. [8] M. Jain et al., \"Object Detection and Gesture Control of Four-Wheel Mobile Robot,\" \r\n2019 International Conference on Communication and Electronics Systems (ICCES), \r\n2019, pp. 303-308, doi: 10.1109/ICCES45898.2019.9002323. [9] Fisher et al.: BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask \r\nLearning.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"OZ2hGCH8mPvZFzQFhychiSuOBsnlwWP8QzzYqwdYQCE="},"101c6465-fd0d-4f8e-88b0-733792c62bc4":{"id_":"101c6465-fd0d-4f8e-88b0-733792c62bc4","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"owyVAaPQMMNOZDZq7f7iV3AP0Ubl9aqVCgMvsWM4eUw="},"PREVIOUS":{"nodeId":"db5294e9-0416-4e1e-8489-539433d4a846","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"OZ2hGCH8mPvZFzQFhychiSuOBsnlwWP8QzzYqwdYQCE="}},"text":"arXiv: 1805.04687 \r\n[10] Ren S., He K., Girshick., Sun J.: Faster R-CNN: Towards Real-Time Object \r\nDetection with Region Proposal Networks. arXiv: 1506.04197 \r\n[11] TF2.0 Faster R-CNN with Resent-50 V1 Object detection model: \r\nhttps://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1  \r\n[12] Google Open Images Dataset V6 \r\nhttps://storage.googleapis.com/openimages/web/index.html","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"PfYcav9D3TF8yG/FklTLsMcm0nrKBhDO7E1+UrEwgCI="},"87b92d05-9cb0-4522-baf4-6a2b75ef3858":{"id_":"87b92d05-9cb0-4522-baf4-6a2b75ef3858","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf","file_name":"Error_Analysis_of_Object_Detection_Models_With_Interactive_User_Interfaces.pdf"},"hash":"EyrEK3WIWMXpSmFLZjb306+FIZDZYYUlxZOaISVqoFk="}},"text":"27","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"r/rvkFc9vuR8ZSKMQrrZ5VCcyFsXxzPDwpaMRP4R9ZM="},"4bdc20fc-15f9-47e3-a67b-7ae335ddb9d1":{"id_":"4bdc20fc-15f9-47e3-a67b-7ae335ddb9d1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Fs8clIhdrLdwD72JgyO3hjdWtqlBrgdqlvCLnsHR2Wg="},"NEXT":{"nodeId":"d4949c5d-c6f8-46a1-920a-6e0ed3c3a103","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gDMtu0KM9oNBcSaIxMWOP5gxnKgqRWQD5AFqzoOETfI="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW1\r\nImbalance Problems in Object Detection: A\r\nReview\r\nKemal Oksuzy, Baris Can Cam, Sinan Kalkanz, and Emre Akbasz\r\nAbstract—In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in\r\na systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and\r\npresent a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the\r\nexisting imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our\r\nreview up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our\r\nproblem-based taxonomy. Researchers can track newer studies on this webpage available at:\r\nhttps://github.com/kemaloksuz/ObjectDetectionImbalance. F\r\n1    INTRODUCTION\r\nObject detection is the simultaneous estimation of categories\r\nand locations of object instances in a given image. It is a fun-\r\ndamental problem in computer vision with many important\r\napplications in e.g. surveillance [1], [2], autonomous driving\r\n[3], [4], medical decision making [5], [6], and many problems\r\nin robotics [7], [8], [9], [10], [11], [12]. Since  the  time  when  object  detection  (OD)  was  cast  as\r\na machine learning problem, the first generation OD meth-\r\nods relied on hand-crafted features and linear, max-margin\r\nclassifiers. The  most  successful  and  representative  method\r\nin this generation was the Deformable Parts Model (DPM)\r\n[13]. After the extremely influential work by Krizhevsky et\r\nal. in 2012 [14], deep learning (or deep neural networks) has\r\nstarted  to  dominate  various  problems  in  computer  vision\r\nand  OD  was  no  exception. The  current  generation  OD\r\nmethods  are  all  based  on  deep  learning  where  both  the\r\nhand-crafted features and linear classifiers of the first gener-\r\nation methods have been replaced by deep neural networks.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"RXoMd6jRM8VZfkNF9AeI93LZ3LRcR+iSsKN69B3LwqY="},"d4949c5d-c6f8-46a1-920a-6e0ed3c3a103":{"id_":"d4949c5d-c6f8-46a1-920a-6e0ed3c3a103","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Fs8clIhdrLdwD72JgyO3hjdWtqlBrgdqlvCLnsHR2Wg="},"PREVIOUS":{"nodeId":"4bdc20fc-15f9-47e3-a67b-7ae335ddb9d1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"RXoMd6jRM8VZfkNF9AeI93LZ3LRcR+iSsKN69B3LwqY="},"NEXT":{"nodeId":"e58db602-5a28-4559-9d68-73c01d805acb","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YD7DVwsdSngpleLGlXsRx/VLSXzBgbw6y7Nvq/xLMUs="}},"text":"This replacement has brought significant improvements in\r\nperformance:  On  a  widely  used  OD  benchmark  dataset\r\n(PASCAL  VOC),  while  the  DPM  [13]  achieved  0.34  mean\r\naverage-precision  (mAP),  current  deep  learning  based  OD\r\nmodels achieve around 0.80 mAP [15]. In the last five years, although the major driving force of\r\nprogress in  OD has  been the  incorporation of  deep neural\r\nnetworks [16], [17], [18], [19], [20], [21], [22], [23], imbalance\r\nproblems in OD at several levels have also received signif-\r\nicant  attention  [24],  [25],  [26],  [27],  [28],  [29],  [30]. Anim-\r\nbalance  problemwith respect to an input property occurs\r\nwhen  the  distribution  regarding  that  property  affects  the\r\nperformance. When  not  addressed,  an  imbalance  problem\r\nhas adverse effects on the final detection performance. For\r\nexample,  the  most  commonly  known  imbalance  problem\r\nin  OD  is  the  foreground-to-background  imbalance  which\r\nAll authors are at the Dept. of Computer Engineering, Middle East Techni-cal  University  (METU),  Ankara,  Turkey. E-mail:{kemal.oksuz@metu.edu.tr,\r\ncan.cam@metu.edu.tr, skalkan@metu.edu.tr, emre@ceng.metu.edu.tr}y\r\nCorresponding author.z\r\nEqual contribution for senior authorship. manifests itself in the extreme inequality between the num-\r\nber  of  positive  examples  versus  the  number  of  negatives. In  a  given  image,  while  there  are  typically  a  few  positive\r\nexamples,  one  can  extract  millions  of  negative  examples. If  not  addressed,  this  imbalance  greatly  impairs  detection\r\naccuracy. In  this  paper,  we  review  the  deep-learning-era  object\r\ndetection  literature  and  identify  eight  different  imbalance\r\nproblems.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gDMtu0KM9oNBcSaIxMWOP5gxnKgqRWQD5AFqzoOETfI="},"e58db602-5a28-4559-9d68-73c01d805acb":{"id_":"e58db602-5a28-4559-9d68-73c01d805acb","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Fs8clIhdrLdwD72JgyO3hjdWtqlBrgdqlvCLnsHR2Wg="},"PREVIOUS":{"nodeId":"d4949c5d-c6f8-46a1-920a-6e0ed3c3a103","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gDMtu0KM9oNBcSaIxMWOP5gxnKgqRWQD5AFqzoOETfI="}},"text":"We  group  these  problems  in  a  taxonomy  with\r\nfour  main  types:  class  imbalance,  scale  imbalance,  spatial\r\nimbalance  and  objective imbalance  (Table  1).Class  imbal-\r\nanceoccurs  when  there  is  significant  inequality  among\r\nthe  number  of  examples  pertaining  to  different  classes. While  the  classical  example  of  this  is  the  foreground-to-\r\nbackground  imbalance,  there  is  also  imbalance  among  the\r\nforeground (positive) classes.Scale imbalanceoccurs when\r\nthe  objects  have  various  scales  and  different  numbers  of\r\nexamples  pertaining  to  different  scales.Spatial  imbalance\r\nrefers to a set of factors related to spatial properties of the\r\nbounding  boxes  such  as  regression  penalty,  location  and\r\nIoU. Finally,objective  imbalanceoccurs  when  there  are\r\nmultiple loss functions to minimize, as is often the case in\r\nOD (e.g. classification and regression losses). 1.1    Scope and Aim\r\nImbalance  problems  in  general  have  a  large  scope  in  ma-\r\nchine  learning,  computer  vision  and  pattern  recognition. We  limit  the  focus  of  this  paper  to  imbalance  problems  in\r\nobject detection. Since the current state-of-the-art is shaped\r\nby  deep  learning  based  approaches,  the  problems  and  ap-\r\nproaches  that  we  discuss  in  this  paper  are  related  to  deep\r\nobject detectors. Although we restrict our attention to object\r\ndetection  in  still  images,  we  provide  brief  discussions  on\r\nsimilarities and differences of imbalance problems in other\r\ndomains. We believe that these discussions would provide\r\ninsights  on  future  research  directions  for  object  detection\r\nresearchers. Presenting  a  comprehensive  background  for  object  de-\r\ntection is not among the goals of this paper; however, some\r\narXiv:1909.00169v3  [cs.CV]  11 Mar 2020","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"YD7DVwsdSngpleLGlXsRx/VLSXzBgbw6y7Nvq/xLMUs="},"97f5962d-bd29-467d-98d9-ed9491c69f99":{"id_":"97f5962d-bd29-467d-98d9-ed9491c69f99","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"LREEO2m3a4A047WSoKxzTPFnS3NUvMnLSXWQVkuNzRw="},"NEXT":{"nodeId":"8d666df8-617c-46ac-aa27-7ff053a57e2a","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0N9ShDIEB2q0EG/OKTq7TEBpxYS3/RLDOKb1dYORdRw="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW2\r\nTABLE 1: Imbalance problems reviewed in this paper. We state that an imbalance problem with respect to an input property\r\noccurs when the distribution regarding that property affects the performance. The first column shows the major imbalance\r\ncategories. For each Imbalance problem given in the middle column, the last column shows the associated input property\r\nconcerning the definition of the imbalance problem. TypeImbalance ProblemRelated Input Property\r\nClassForeground-Background Class Imbalance (§4.1)The numbers of input bounding boxes pertain-ing to different classes\r\nForeground-Foreground Class Imbalance (§4.2)\r\nScaleObject/box-level Scale Imbalance (§5.1)The scales of input and ground-truth boundingboxes\r\nFeature-level Imbalance (§5.2)Contribution of the feature layer from different\r\nabstraction levels of the backbone network (i.e. high and low level)\r\nSpatial\r\nImbalance in Regression Loss (§6.1)Contribution of the individual examples to the\r\nregression loss\r\nIoU Distribution Imbalance (§6.2)IoU  distribution  of  positive  input  bounding\r\nboxes\r\nObject Location Imbalance (§6.3)Locations of the objects throughout the image\r\nObjectiveObjective Imbalance (§7)Contribution  of  different  tasks  (i.e. classifica-\r\ntion, regression) to the overall loss\r\nbackground  knowledge  on  object  detection  is  required  to\r\nmake the most out of this paper. For a thorough background\r\non  the  subject,  we  refer  the  readers  to  the  recent,  compre-\r\nhensive object detection surveys [31], [32], [33]. We provide\r\nonly a brief background on state-of-the-art object detection\r\nin Section 2.1. Our  main  aim  in  this  paper  is  to  present  and  discuss\r\nimbalance problems in object detection comprehensively. In\r\norder to do that,\r\n1)     We  identify  and  define  imbalance  problems  and\r\npropose a taxonomy for studying the problems and\r\ntheir solutions. 2)     We present a critical literature review for the exist-\r\ning studies with a motivation to unify them in a sys-\r\ntematic manner.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+vYl2RSq2Amjg/LHiq0ISdMD6kLm3ZiBVADr9W1T9tg="},"8d666df8-617c-46ac-aa27-7ff053a57e2a":{"id_":"8d666df8-617c-46ac-aa27-7ff053a57e2a","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"LREEO2m3a4A047WSoKxzTPFnS3NUvMnLSXWQVkuNzRw="},"PREVIOUS":{"nodeId":"97f5962d-bd29-467d-98d9-ed9491c69f99","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+vYl2RSq2Amjg/LHiq0ISdMD6kLm3ZiBVADr9W1T9tg="},"NEXT":{"nodeId":"cccb2271-7393-42d5-9b09-1bf225f1d38c","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"lLhwEtxPh1b0va/x5x7J01vOMyNVk9GgRgmBkpAslXk="}},"text":"The general outline of our review\r\nincludes  a  definition  of  the  problems,  a  summary\r\nof  the  main  approaches,  an  in-depth  coverage  of\r\nthe  specific  solutions,  and  comparative  summaries\r\nof the solutions. 3)     We present and discuss open issues at the problem-\r\nlevel and in general. 4)     We also reserved a section for imbalance problems\r\nfound in domains other than object detection. This\r\nsection is generated with meticulous examination of\r\nmethods considering their adaptability to the object\r\ndetection pipeline. 5)     Finally, we provide an accompanying webpage1as\r\na living repository of papers addressing imbalance\r\nproblems,  organized  based  on  our  problem-based\r\ntaxonomy. This  webpage  will  be  continuously  up-\r\ndated with new studies. 1.2    Comparison with Previous Reviews\r\nRecent object detection surveys [31], [32], [33] aim to present\r\nadvances  in  deep  learning  based  generic  object  detection. To  this  end,  these  surveys  propose  a  taxonomy  for  object\r\n1. https://github.com/kemaloksuz/ObjectDetectionImbalance\r\ndetection methods, and present a detailed analysis of some\r\ncornerstone methods that have had high impact. They also\r\nprovide  discussions  on  popular  datasets  and  evaluation\r\nmetrics. From  the  imbalance  point  of  view,  these  surveys\r\nonly  consider  the  class  imbalance  problem  with  a  limited\r\nprovision. Additionally, Zou et al. [32] provide a review for\r\nmethods that handle scale imbalance. Unlike these surveys,\r\nhere  we  focus  on  a  classification  of  imbalance  problems\r\nrelated  to  object  detection  and  present  a  comprehensive\r\nreview of methods that handle these imbalance problems. There  are  also  surveys  on  category  specific  object  de-\r\ntection  (e.g. pedestrian  detection,  vehicle  detection,  face\r\ndetection)  [34],  [35],  [36],  [37]. Although  Zehang  Sun  et\r\nal. [34]  and  Dollar  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0N9ShDIEB2q0EG/OKTq7TEBpxYS3/RLDOKb1dYORdRw="},"cccb2271-7393-42d5-9b09-1bf225f1d38c":{"id_":"cccb2271-7393-42d5-9b09-1bf225f1d38c","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"LREEO2m3a4A047WSoKxzTPFnS3NUvMnLSXWQVkuNzRw="},"PREVIOUS":{"nodeId":"8d666df8-617c-46ac-aa27-7ff053a57e2a","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0N9ShDIEB2q0EG/OKTq7TEBpxYS3/RLDOKb1dYORdRw="}},"text":"[34]  and  Dollar  et  al. [35]  cover  the  methods  proposed\r\nbefore  the  current  deep  learning  era,  they  are  beneficial\r\nfrom  the  imbalance  point  of  view  since  they  present  a\r\ncomprehensive  analysis  of  feature  extraction  methods  that\r\nhandle  scale  imbalance. Zafeiriou  et  al. [36]  and  Yin  et  al. [38]  propose  comparative  analyses  of  non-deep  and  deep\r\nmethods. Litjens  et  al. [39]  discuss  applications  of  various\r\ndeep neural network based methodsi.e. classification, detec-\r\ntion,  segmentationto  medical  image  analysis. They  present\r\nchallenges  with  their  possible  solutions  which  include  a\r\nlimited  exploration  of  the  class  imbalance  problem. These\r\ncategory  specific  object  detector  reviews  focus  on  a  single\r\nclass  and  do  not  consider  the  imbalance  problems  in  a\r\ncomprehensive  manner  from  the  generic  object  detection\r\nperspective. Another set of relevant work includes the studies specif-\r\nically for imbalance problems in machine learning [40], [41],\r\n[42], [43]. These studies are limited to the foreground class\r\nimbalance  problem  in  our  context  (i.e. there  is  no  back-\r\nground  class). Generally,  they  cover  dataset-level  methods\r\nsuch  as  undersampling  and  oversampling,  and  algorithm-\r\nlevel  methods  including  feature  selection,  kernel  modifi-\r\ncations  and  weighted  approaches. We  identify  three  main\r\ndifferences  of  our  work  compared  to  such  studies. Firstly,\r\nthe main scope of  such work is the classification problem,","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"lLhwEtxPh1b0va/x5x7J01vOMyNVk9GgRgmBkpAslXk="},"7360f7b8-d21d-4b94-b922-45ba41625428":{"id_":"7360f7b8-d21d-4b94-b922-45ba41625428","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rwkCGKH9LNQRw2uMsaate/JDXGXZxcUVLMNwjHh7zck="},"NEXT":{"nodeId":"9d25e04a-ec45-4bba-84d9-e9269034eee8","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gSlxjG/5k5I03fAAmBcA/WNk5qSRsEZB+bVDyjSDe60="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW3\r\nwhich  is  still  relevant  for  object  detection;  however,  object\r\ndetection  also  has  a  “search”  aspect,  in  addition  to  the\r\nrecognition  aspect,  which  brings  in  the  background  (i.e. negative)  class  into  the  picture. Secondly,  except  Johnson\r\net  al. [43],  they  consider  machine  learning  approaches  in\r\ngeneral  without  any  special  focus  on  deep  learning  based\r\nmethods. Finally,  and  more  importantly,  these  works  only\r\nconsider foreground class imbalance problem, which is only\r\none  of  eight  different  imbalance  problems  that  we  present\r\nand discuss here (Table 1). 1.3    A Guide to Reading This Review\r\nThe paper is organized as follows. Section 2 provides a brief\r\nbackground on object detection, and the list of frequently-\r\nused terms and notation used throughout the paper. Section\r\n3 presents our taxonomy of imbalance problems. Sections 4-\r\n7 then cover each imbalance problem in detail, with a critical\r\nreview of the proposed solutions and include open issues for\r\neach imbalance problem. Each section dedicated to a specific\r\nimbalance problem is designed to be self-readable, contain-\r\ning  definitions  and  a  review  of  the  proposed  methods. In\r\norder  to  provide  a  more  general  perspective,  in  Section  8,\r\nwe present the solutions addressing imbalance in other but\r\nclosely related domains. Section 9 discusses open issues that\r\nare  relevant  to  all  imbalance  problems. Finally,  Section  10\r\nconcludes the paper. Readers who are familiar with the current state-of-the-art\r\nobject detection methods can directly jump to Section 3 and\r\nuse Figure 1 to navigate both the imbalance problems and\r\nthe sections dedicated to the different problems according to\r\nthe taxonomy.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NHISBedGrOhaUtv5f44Rz2JLG2wjF6S3XcBxZmPTpd0="},"9d25e04a-ec45-4bba-84d9-e9269034eee8":{"id_":"9d25e04a-ec45-4bba-84d9-e9269034eee8","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rwkCGKH9LNQRw2uMsaate/JDXGXZxcUVLMNwjHh7zck="},"PREVIOUS":{"nodeId":"7360f7b8-d21d-4b94-b922-45ba41625428","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"NHISBedGrOhaUtv5f44Rz2JLG2wjF6S3XcBxZmPTpd0="},"NEXT":{"nodeId":"835aa95a-5b8d-4f48-92bc-af33075e7ba7","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CfxkEJXX2nAzViypuKFVTW+mfCfVEq8R8YGY9pUYejE="}},"text":"For readers who lack a background in state-\r\nof-the-art  object  detection,  we  recommend  starting  with\r\nSection  2.1,  and  if  this  brief  background  is  not  sufficient,\r\nwe refer the reader to the more in-depth reviews mentioned\r\nin Section 1.1. 2    BACKGROUND, DEFINITIONS ANDNOTATION\r\nIn  the  following,  we  first  provide  a  brief  background  on\r\nstate-of-the-art  object  detection  methods,  and  then  present\r\nthe definitions and notations used throughout the paper. 2.1    State of the Art in Object Detection\r\nToday  there  are  two  major  approaches  to  object  detection:\r\ntop-down  and  bottom-up. Although  both  the  top-down\r\nand bottom-up approaches were popular prior to the deep\r\nlearning era, today the majority of the object detection meth-\r\nods follow the top-down approach; the bottom-up methods\r\nhave been proposed relatively recently. The main difference\r\nbetween  the  top-down  and  bottom-up  approaches  is  that,\r\nin  the  top-down  approach,  holistic  object  hypotheses  (i.e.,\r\nanchors,  regions-of-interests/proposals)  are  generated  and\r\nevaluated  early  in  the  detection  pipeline,  whereas  in  the\r\nbottom-up  approach,  holistic  objects  emerge  by  grouping\r\nsub-object entities like keypoints or parts, later in the pro-\r\ncessing pipeline. Methods  following  the  top-down  approach  are  catego-\r\nrized  into  two:  two-stage  and  one-stage  methods. Two-\r\nstage methods [16], [17], [18], [21] aim to decrease the large\r\nnumber of negative examples resulting from the predefined,\r\nTABLE 2: Frequently used notations in the paper.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gSlxjG/5k5I03fAAmBcA/WNk5qSRsEZB+bVDyjSDe60="},"835aa95a-5b8d-4f48-92bc-af33075e7ba7":{"id_":"835aa95a-5b8d-4f48-92bc-af33075e7ba7","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rwkCGKH9LNQRw2uMsaate/JDXGXZxcUVLMNwjHh7zck="},"PREVIOUS":{"nodeId":"9d25e04a-ec45-4bba-84d9-e9269034eee8","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gSlxjG/5k5I03fAAmBcA/WNk5qSRsEZB+bVDyjSDe60="},"NEXT":{"nodeId":"48741c64-9bf7-473d-9aaa-154cfe639867","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"o+NVqpao04hFSZWDHbcKcdwKrLzsXIANqgEyJI0YEsw="}},"text":"SymbolDomainDenotes\r\nBSee.Def.A Bounding box\r\nCA set ofintegersSet of Class labels in a dataset\r\njCijjCij 2Z+Number of examples for theithclass in a dataset\r\nCii2Z+Backbone feature layer at depthi\r\nI(P)I(P)2 f0;1gIndicator function.1if predicatePis true, else0\r\nP ii2Z+\r\nPyramidal feature layercorresponding to\r\nith backbonefeature layer\r\npipi2[0;1]Confidence score ofith class (i.e.output of classifier)\r\npsps2[0;1]Confidence score of the groundtruth class\r\np0p02[0;1]Confidence score of backgroundclass\r\nuu2CClass label of a ground truth\r\n^x^x2RInput of the regression loss\r\ndense  sliding  windows,  called  anchors,  to  a  manageable\r\nsize  by  using  a  proposal  mechanism  [21],  [44],  [45]  which\r\ndetermines the regions where the objects most likely appear,\r\ncalled  Region  of  Interests  (RoIs). These  RoIs  are  further\r\nprocessed by a detection network which outputs the object\r\ndetection results in the form of bounding boxes and associ-\r\nated  object-category  probabilities. Finally,  the  non-maxima\r\nsuppression  (NMS)  method  is  applied  on  the  object  de-\r\ntection results to eliminate duplicate or highly-overlapping\r\nresults. NMS is a universal post-processing step used by all\r\nstate-of-the-art object detectors. One-stage  top-down  methods,  including  SSD  Variants\r\n[19], [46], YOLO variants [15], [20], [47] and RetinaNet [22],\r\nare  designed  to  predict  the  detection  results  directly  from\r\nanchors  –  without  any  proposal  elimination  stage  –  after\r\nextracting the features from the input image. We present a\r\ntypical one-stage object detection pipeline in Figure 1(a). The\r\npipeline starts with feeding the input image to the feature\r\nextraction  network,  which  is  usually  a  deep  convolutional\r\nneural  network.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CfxkEJXX2nAzViypuKFVTW+mfCfVEq8R8YGY9pUYejE="},"48741c64-9bf7-473d-9aaa-154cfe639867":{"id_":"48741c64-9bf7-473d-9aaa-154cfe639867","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rwkCGKH9LNQRw2uMsaate/JDXGXZxcUVLMNwjHh7zck="},"PREVIOUS":{"nodeId":"835aa95a-5b8d-4f48-92bc-af33075e7ba7","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CfxkEJXX2nAzViypuKFVTW+mfCfVEq8R8YGY9pUYejE="}},"text":"A  dense  set  of  object  hypotheses  (called\r\nanchors) are produced, which are then sampled and labeled\r\nby  matching  them  to  ground-truth  boxes. Finally,  labeled\r\nanchors (whose features are obtained from the output of the\r\nfeature extraction network) are fed to the classification and\r\nregression  networks  for  training. In  a  two-stage  method,\r\nobject  proposals  (or  regions-of-interest)  are  first  generated\r\nusing anchors by a separate network (hence, the two stages). On the other hand, bottom-up object detection methods\r\n[23],  [48],  [49]  first  predict  important  key-points  (e.g. cor-\r\nners, centers, etc. ) on objects and then group them to form\r\nwhole object instances by using a grouping method such as\r\nassociative embedding [50] and brute force search [49]. 2.2    Frequently Used Terms and Notation\r\nTable  2  presents  the  notation  used  throughout  the  paper,\r\nand below is a list of frequently used terms.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"o+NVqpao04hFSZWDHbcKcdwKrLzsXIANqgEyJI0YEsw="},"77509b57-0002-4479-b392-1332854723ed":{"id_":"77509b57-0002-4479-b392-1332854723ed","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YNF342AlTfMmkppelZsYny+DlzBD7Fk/wse8f8ylwO4="},"NEXT":{"nodeId":"9c8246ed-466d-4d57-8dee-bc78717aa03b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YccgMCiB5qpS3sQVb4ADZGx3xylExHC6tRkzlOeHQNM="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW4\r\nBB Matching & Labeling\r\nAnchor/RoISet\r\nGT Set\r\nFeature ExtractionDetectionBB Matching, Labeling and Sampling\r\nBlue GT\r\nBlack GT\r\nPositive\r\nNegative\r\nSampling\r\nGround Truth Scales\r\n2-Scale Imbalance(§5)\r\nLoss Values of Tasks\r\n4-ObjectiveImbalance(§7)1-Class Imbalance(§4)\r\nClass 1Class 2\r\n3-SpatialImbalance(§6)\r\nIoUof Positive Input BB0.60.50.70.80.9\r\n# of \r\nExamples\r\nDetections & Loss Value\r\nClassificationRegression\r\nBBs\r\ntoTrain\r\nLabeled\r\nBBs\r\n(a)\r\n(b)\r\nExample Numbers\r\nBlack GTBlue GT\r\nFeature Extraction \r\nNetwork\r\nFig. 1:(a)The common training pipeline of a generic detection network. The pipeline has 3 phases (i.e. feature extraction,\r\ndetection  and  BB  matching,  labeling  and  sampling)  represented  by  different  background  colors.(b)Illustration  of  an\r\nexample  imbalance  problem  from  each  category  for  object  detection  through  the  training  pipeline. Background  colors\r\nspecify at which phase an imbalance problem occurs. Feature  Extraction  Network/Backbone:This  is  the  part  of\r\nthe object detection pipeline from the input image until the\r\ndetection network. Classification  Network/Classifier:This  is  the  part  of  the\r\nobject detection pipeline from the features extracted by the\r\nbackbone to the classification result, which is indicated by a\r\nconfidence score. Regression   Network/Regressor:This  is  the  part  of  the\r\nobject detection pipeline from the features extracted by the\r\nbackbone  to  the  regression  output,  which  is  indicated  by\r\ntwo bounding box coordinates each of which consisting of\r\nan x-axis and y-axis values. Detection  Network/Detector:It  is  the  part  of  the  object\r\ndetection pipeline including both classifier and regressor.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"pVJ/aFyjcE5opXtXfVaRQBBNF04bBN3XE8R6kvrbLyQ="},"9c8246ed-466d-4d57-8dee-bc78717aa03b":{"id_":"9c8246ed-466d-4d57-8dee-bc78717aa03b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YNF342AlTfMmkppelZsYny+DlzBD7Fk/wse8f8ylwO4="},"PREVIOUS":{"nodeId":"77509b57-0002-4479-b392-1332854723ed","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"pVJ/aFyjcE5opXtXfVaRQBBNF04bBN3XE8R6kvrbLyQ="},"NEXT":{"nodeId":"0eb1ee46-010e-4c95-905f-d1b04c60c964","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"O0mQOdvvKwS4CQQusA3YPMI8uQ5ovM8NamxNeXTXrhs="}},"text":"Region  Proposal  Network  (RPN):It is the part of the two\r\nstage  object  detection  pipeline  from  the  features  extracted\r\nby the backbone to the generated proposals, which also have\r\nconfidence scores and bounding box coordinates. Bounding  Box:A  rectangle  on  the  image  limiting  certain\r\nfeatures. Formally,[x1;y1;x2;y2]determine a bounding box\r\nwith top-left corner(x1;y1)and bottom-right corner(x2;y2)\r\nsatisfyingx2> x1andy2> y1. Anchor:The set of pre defined bounding boxes on which the\r\nRPN in two stage object detectors and detection network in\r\none stage detectors are applied. Region  of  Interest  (RoI)/Proposal:The  set  of  bounding\r\nboxes generated by a proposal mechanism such as RPN on\r\nwhich the detection network is applied on two state object\r\ndetectors. Input Bounding Box:Sampled anchor or RoI the detection\r\nnetwork or RPN is trained with. Ground Truth:It is tuple(B;u)such thatBis the bounding\r\nbox  anduis  theclass  labelwhereu2CandCis  the\r\nenumeration of the classes in the dataset. Detection:It is a tuple(\u0016B;p)such that\u0016Bis the bounding\r\nbox andpis the vector over the confidence scores for each\r\nclass2and bounding box. Intersection  Over  Union:For a ground truth boxBand a\r\ndetection  box\u0016B,  we  can  formally  define  Intersection  over\r\nUnion(IoU) [51], [52], denoted byIoU(B;\u0016B), as\r\nIoU(B;\u0016B) =A(B\\\r\n\u0016B)\r\nA(B[\u0016B);(1)\r\nsuch thatA(B)is the area of a bounding boxB. Under-represented Class:The class which has less samples\r\nin a dataset or mini batch during training in the context of\r\nclass imbalance. Over-represented Class:The class which has more samples\r\nin a dataset or mini batch during training in the context of\r\nclass imbalance. Backbone Features:The set of features obtained during the\r\napplication of the backbone network. Pyramidal  Features/Feature  Pyramid:The  set  of  features\r\nobtained by applying some transformations to the backbone\r\nfeatures.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"YccgMCiB5qpS3sQVb4ADZGx3xylExHC6tRkzlOeHQNM="},"0eb1ee46-010e-4c95-905f-d1b04c60c964":{"id_":"0eb1ee46-010e-4c95-905f-d1b04c60c964","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YNF342AlTfMmkppelZsYny+DlzBD7Fk/wse8f8ylwO4="},"PREVIOUS":{"nodeId":"9c8246ed-466d-4d57-8dee-bc78717aa03b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YccgMCiB5qpS3sQVb4ADZGx3xylExHC6tRkzlOeHQNM="}},"text":"Regression  Objective  Input:Some  methods  make  predic-\r\ntions  in  the  log  domain  by  applying  some  transformation\r\nwhich  can  also  differ  from  method  to  method  (compare\r\ntransformation in Fast R-CNN [17] and in KL loss [53] for\r\nSmooth L1 Loss), while some methods directly predict the\r\nbounding  box  coordinates  [23]. For  the  sake  of  clarity,  we\r\nuse^xto denote the regression loss input for any method. 2. We use class and category interchangeably in this paper.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"O0mQOdvvKwS4CQQusA3YPMI8uQ5ovM8NamxNeXTXrhs="},"2380e6ec-89df-4453-a077-681982b05efb":{"id_":"2380e6ec-89df-4453-a077-681982b05efb","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vvIJVXU6a5nU5KtHlL9A8vz2uGPwqXfirnRyuNIUqlA="},"NEXT":{"nodeId":"5bd8a340-bc6c-4cd3-80b2-7a022a3cb8c3","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rrIwmoXJ0x/nllyaTYAxT3vaLaPK3Yd5dj5A0+C7bzo="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW5\r\nMethods forImbalance Problems\r\nSpatial Imbalance(§6)Imbalance inRegression Task\r\n(§6.1)\r\nIoUDistribution\r\nImbalance(§6.2)\r\nObjectLocation\r\nImbalance(§6.3)\r\nScale Imbalance(§5)\r\nObject/box-level Imbalance\r\n(§5.1)\r\nFeature-levelImbalance\r\n(§5.2)\r\nObjective Imbalance(§7)\r\nClass Imbalance(§4)\r\nFg-Bg ClassImbalance\r\n(§4.1)\r\nFg-Fg ClassImbalance\r\n(§4.2)\r\n\u000fTask Weighting\u000fClassification Aware\r\nRegression Loss [30]\r\n\u000fGuided Loss [54]\r\n\u000f1.Hard Sampling Methods\r\n\u000fA.Random Sampling\u000fB.Hard Example Mining\r\n–Bootstraping [55]–SSD [19]\r\n–Online Hard Example Mining [24]–IoU-based Sampling [29]\r\n\u000fC.Limit Search Space–Two-stage Object Detectors\r\n–IoU-lower Bound [17]–Objectness Prior [56]\r\n–Negative Anchor Filtering [57]–Objectness Module [58]\r\n\u000f2.Soft Sampling Methods\r\n\u000fFocal Loss [22]\u000fGradient Harmonizing Mechanism [59]\r\n\u000fPrime Sample Attention [30]\u000f3.Sampling-Free Methods\r\n\u000fResidual Objectness [60]\u000fNo Sampling Heuristics [54]\r\n\u000fAP Loss [61]\u000fDR Loss [62]\r\n\u000f4.Generative Methods\r\n\u000fAdversarial Faster-RCNN [63]\u000fTask Aware Data Synthesis [64]\r\n\u000fPSIS [65]\u000fpRoI Generator [66]\r\n\u000fSee generative methods forfg-bg class imb. \u000fFine-tuning Long TailDistribution for Obj.Det.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9sxMZOY6vD47lgdgaHWrN+PtD1wSk3QuOlDWH5SaRfI="},"5bd8a340-bc6c-4cd3-80b2-7a022a3cb8c3":{"id_":"5bd8a340-bc6c-4cd3-80b2-7a022a3cb8c3","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vvIJVXU6a5nU5KtHlL9A8vz2uGPwqXfirnRyuNIUqlA="},"PREVIOUS":{"nodeId":"2380e6ec-89df-4453-a077-681982b05efb","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"9sxMZOY6vD47lgdgaHWrN+PtD1wSk3QuOlDWH5SaRfI="},"NEXT":{"nodeId":"fcec85c5-904a-4047-be0e-46a039de4aa6","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"wBOzd+RQAbPUMGI0ygKPpH+XzR8WVA6itNg8BZVkQr8="}},"text":"\u000fFine-tuning Long TailDistribution for Obj.Det. [25]\r\n\u000fOFB Sampling [66]\r\n\u000fGuided Anchoring [67]\u000fFree Anchor [68]\r\n\u000f1.Methods Predicting from the Feature Hierarchy ofBackbone Features\r\n\u000fScale-dependent Pooling [69]\u000fSSD [19]\r\n\u000fMulti Scale CNN [70]\u000fScale Aware Fast R-CNN [71]\r\n\u000f2.Methods Based on Feature Pyramids\r\n\u000fFPN [26]\u000fSee feature-level imbalance methods\r\n\u000f3.Methods Based on Image Pyramids\r\n\u000fSNIP [27]\u000fSNIPER [28]\r\n\u000f4.Methods Combining Image and Feature Pyramids\r\n\u000fEfficient Featurized Image Pyramids [72]\u000fEnriched Feature Guided Refinement Network [58]\r\n\u000fSuper Resolution for Small Objects [73]\u000fScale Aware Trident Network [74]\r\n\u000f1.Methods Using Pyramidal Features as a Basis\r\n\u000fPANet [75]\u000fLibra FPN [29]\r\n\u000f2.Methods Using Backbone Features as a Basis\r\n\u000fSTDN [76]\u000fParallel-FPN [77]\r\n\u000fDeep Feature Pyramid Reconf. [78]\u000fZoom Out-and-In [79]\r\n\u000fMulti-level FPN [80]\u000fNAS-FPN [81]\r\n\u000fAuto-FPN [82]\r\n\u000f1.Lpnorm based\r\n\u000fSmooth L1 [17]\u000fBalanced L1 [29]\r\n\u000fKL Loss [53]\u000fGradient Harmonizing\r\nMechanism [59]\r\n\u000f2.IoU based\r\n\u000fIoU Loss [83]\u000fBounded IoU Loss [84]\r\n\u000fGIoU Loss [85]\u000fDistance IoU Loss [86]\r\n\u000fComplete IoU Loss [86]\r\n\u000fCascade R-CNN [87]\u000fHSD [88]\r\n\u000fIoU-uniform R-CNN [89]\u000fpRoI Generator [66]\r\nFig. 2: Problem based categorization of the methods used for imbalance problems. Note that a work may appear at multiple\r\nlocations if it addresses multiple imbalance problems – e.g.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rrIwmoXJ0x/nllyaTYAxT3vaLaPK3Yd5dj5A0+C7bzo="},"fcec85c5-904a-4047-be0e-46a039de4aa6":{"id_":"fcec85c5-904a-4047-be0e-46a039de4aa6","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vvIJVXU6a5nU5KtHlL9A8vz2uGPwqXfirnRyuNIUqlA="},"PREVIOUS":{"nodeId":"5bd8a340-bc6c-4cd3-80b2-7a022a3cb8c3","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rrIwmoXJ0x/nllyaTYAxT3vaLaPK3Yd5dj5A0+C7bzo="}},"text":"Note that a work may appear at multiple\r\nlocations if it addresses multiple imbalance problems – e.g. Libra R-CNN [29]\r\n3    A  TAXONOMY  OF  THEIMBALANCEPROBLEMS\r\nAND THEIRSOLUTIONS INOBJECTDETECTION\r\nIn  Section  1,  we  defined  the  problem  of  imbalance  as\r\nthe  occurrence  of  a  distributional  bias  regarding  an  input\r\nproperty  in  the  object  detection  training  pipeline. Several\r\ndifferent types of such imbalance can be observed at various\r\nstages of the common object detection pipeline (Figure 1). To\r\nstudy these problems in a systematic manner, we propose a\r\ntaxonomy based on the related input property. We  identify  eight  different  imbalance  problems,  which\r\nwe group into four main categories: class imbalance, scale\r\nimbalance, spatial imbalance and objective imbalance. Table\r\n1 presents the complete taxonomy along with a brief defi-\r\nnition  for  each  problem. In  Figure  2,  we  present  the  same\r\ntaxonomy along with a list of proposed solutions for each\r\nproblem. Finally, in Figure 1, we illustrate a generic object\r\ndetection pipeline where each phase is annotated with their\r\ntypically  observed  imbalance  problems. In  the  following,\r\nwe  elaborate  on  the  brief  definitions  provided  earlier,  and\r\nillustrate the typical phases where each imbalance problem\r\noccurs. Class  imbalance(Section  4;  blue  branch  in  Figure  2)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wBOzd+RQAbPUMGI0ygKPpH+XzR8WVA6itNg8BZVkQr8="},"c8056d48-5c64-481c-9b9d-5bf984cb3914":{"id_":"c8056d48-5c64-481c-9b9d-5bf984cb3914","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"uYNHiveBePU7cWExvQiC+Cjxt9tCJT5jEDn5sxCjfwo="},"NEXT":{"nodeId":"9b5cfcb5-201c-4283-a22a-74994d7e1777","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gBJkwQ47R2rL84hjczhTC3k1XrR+z83zOTMGJaAsOjY="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW6\r\noccurs when a certain class is over-represented. This prob-\r\nlem  can  manifest  itself  as  either  “foreground-background\r\nimbalance,”  where  the  background  instances  significantly\r\noutnumber  the  positive  ones;  or  “foreground-foreground\r\nimbalance,” where typically a small fraction of classes dom-\r\ninate the dataset (as observed from the long-tail distribution\r\nin Figure 5). The class imbalance is usually handled at the\r\n“sampling” stage in the object detection pipeline (Figure 1). Scale  imbalance(Section  5;  green  branch  in  Figure  2)\r\nis observed when object instances have various scales and\r\ndifferent number of examples pertaining to different scales. This  problem  is  a  natural  outcome  of  the  fact  that  objects\r\nhave different dimensions in nature. Scale also could cause\r\nimbalance  at  the  feature-level  (typically  handled  in  the\r\n“feature extraction” phase in Figure 1), where contribution\r\nfrom  different  abstraction  layers  (i.e. high  and  low  levels)\r\nare not balanced. Scale imbalance problem suggests that a\r\nsingle scale of visual processing is not sufficient for detect-\r\ning  objects  at  different  scales. However,  as  we  will  see  in\r\nSection 5, proposed methods fall short in addressing scale\r\nimbalance,  especially  for  small  objects,  even  when  small\r\nobjects are surprisingly abundant in a dataset. Spatial  imbalance(Section  6;  orange  branch  in  Figure\r\n2)  refers  to  a  set  of  factors  related  to  spatial  properties  of\r\nthe bounding boxes. Owing to these spatial properties, we\r\nidentify three sub-types of spatial imbalance: (i) “imbalance\r\nin regression loss” is about the contributions of individual\r\nexamples  to  the  regression  loss  and  naturally  the  problem\r\nis  related  to  loss  function  design  (ii)  “IoU  distribution  im-\r\nbalance” is related to the biases in the distribution of IoUs\r\n(among ground-truth boxes vs.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"RcFSX8oswYffDK0SH86Vd/YIdjDXri06HkFxsyS7TzU="},"9b5cfcb5-201c-4283-a22a-74994d7e1777":{"id_":"9b5cfcb5-201c-4283-a22a-74994d7e1777","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"uYNHiveBePU7cWExvQiC+Cjxt9tCJT5jEDn5sxCjfwo="},"PREVIOUS":{"nodeId":"c8056d48-5c64-481c-9b9d-5bf984cb3914","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"RcFSX8oswYffDK0SH86Vd/YIdjDXri06HkFxsyS7TzU="},"NEXT":{"nodeId":"cdec28b8-1da9-4b52-a4c8-746fc52354d2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"mKIXzLO7r0wfHKAoCyHqJhz9tOFbVFMMX8J7O7pYSAo="}},"text":"anchors or detected boxes),\r\nand typically observed at the bounding-box matching and\r\nlabeling phase in the object detection pipeline (Figure 1) (iii)\r\n“object location imbalance” is about the location distribution\r\nof object instances in an image, which is related to both the\r\ndesign  of  the  anchors  and  the  sampled  subset  to  train  the\r\ndetection network. Finally,objective  imbalance(Section  7;  purple  branch\r\nin Figure 2) occurs when there are multiple objectives (loss\r\nfunctions) to minimize (each for a specific task, e.g. classifi-\r\ncation and box-regression). Since different objectives might\r\nbe  incompatible  in  terms  of  their  ranges  as  well  as  their\r\noptimum  solutions,  it  is  essential  to  develop  a  balanced\r\nstrategy  that  can  find  a  solution  acceptable  in  terms  of  all\r\nobjectives. Figure  2  gives  an  overall  picture  of  the  attention  that\r\ndifferent  types  of  imbalance  problems  have  received  from\r\nthe  research  community. For  example,  while  there  are  nu-\r\nmerous  methods  devised  for  the  foreground-background\r\nclass imbalance problem, the objective imbalance and object\r\nlocation imbalance problems are examples of the problems\r\nthat  received  relatively  little  attention. However,  recently\r\nthere  have  been  a  rapidly  increasing  interest  (Figure  3)\r\nin  these  imbalance  problems  as  well,  which  necessitates  a\r\nstructured view and perspective on the problems as well as\r\nthe solutions, as proposed in this paper. Note that some imbalance problems are directly owing\r\nto the data whereas some are the by-products of the specific\r\nmethods used. For example, class imbalance, object location\r\nimbalance etc. are the natural outcomes of the distribution\r\nof  classes  in  the  real  world. On  the  other  hand,  objective\r\n201520162017201820190\r\n2\r\n4\r\n6\r\n8\r\n10Class Imbalance §4Scale Imbalance §5\r\nSpatial Imbalance §6\r\nObjective Imbalance §7\r\nFig. 3: Number of papers per imbalance problem category\r\nthrough years.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gBJkwQ47R2rL84hjczhTC3k1XrR+z83zOTMGJaAsOjY="},"cdec28b8-1da9-4b52-a4c8-746fc52354d2":{"id_":"cdec28b8-1da9-4b52-a4c8-746fc52354d2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"uYNHiveBePU7cWExvQiC+Cjxt9tCJT5jEDn5sxCjfwo="},"PREVIOUS":{"nodeId":"9b5cfcb5-201c-4283-a22a-74994d7e1777","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gBJkwQ47R2rL84hjczhTC3k1XrR+z83zOTMGJaAsOjY="},"NEXT":{"nodeId":"154de7b9-b4d6-4cbe-ab82-29dd35da1f92","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"LajmuUtTo7tLcunqdsMhmflWTH1uhXv81bBC6TmegIE="}},"text":"3: Number of papers per imbalance problem category\r\nthrough years. imbalance, feature-level imbalance and imbalance in regres-\r\nsion  loss  are  owing  to  the  selected  methods  and  possibly\r\navoidable with a different set of methods; for example, it is\r\npossible to avoid IoU distribution imbalance altogether by\r\nfollowing a bottom-up approach where, typically, IoU is not\r\na labeling criterion (e.g. [23], [49]). 4    IMBALANCE1: CLASSIMBALANCE\r\nClass   imbalance   is   observed   when   a   class   is   over-\r\nrepresented,   having   more   examples   than   others   in   the\r\ndataset. This  can  occur  in  two  different  ways  from  the\r\nobject detection perspective: foreground-background imbal-\r\nance and foreground-foreground imbalance. Figure  4  illustrates  the  presence  of  class  imbalance. To\r\ngenerate  the  figure,  we  apply  the  default  set  of  anchors\r\nfrom  RetinaNet  [22]  on  the  MS-COCO  dataset3[90]  and\r\ncalculated  the  frequencies  for  the  cases  where  the  IoU  of\r\nan  anchor  with  a  ground  truth  bounding  box  exceeds0:5\r\nand  where  it  is  less  than0:4(i.e. it  is  a  background  box)\r\nfollowing  the  labeling  rule  of  RetinaNet  [22]. When  an\r\nanchor  overlaps  with  a  foreground  class  (with  IoU>0.5),\r\nwe  kept  a  count  for  each  class  separately  and  normalized\r\nthe resulting frequencies with the number of images in the\r\ndataset. These two types of class imbalance have different char-\r\nacteristics  and  have  been  addressed  using  different  types\r\nof  solutions. Therefore,  in  the  following,  we  will  cover\r\nthem  separately. However,  some  solutions  (e.g. generative\r\nmodeling) could be employed for both problem types.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"mKIXzLO7r0wfHKAoCyHqJhz9tOFbVFMMX8J7O7pYSAo="},"154de7b9-b4d6-4cbe-ab82-29dd35da1f92":{"id_":"154de7b9-b4d6-4cbe-ab82-29dd35da1f92","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"uYNHiveBePU7cWExvQiC+Cjxt9tCJT5jEDn5sxCjfwo="},"PREVIOUS":{"nodeId":"cdec28b8-1da9-4b52-a4c8-746fc52354d2","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"mKIXzLO7r0wfHKAoCyHqJhz9tOFbVFMMX8J7O7pYSAo="}},"text":"generative\r\nmodeling) could be employed for both problem types. 4.1    Foreground-Background Class Imbalance\r\nDefinition.In  foreground-background  class  imbalance,  the\r\nover-represented  and  under-represented  classes  are  back-\r\nground  and  foreground  classes  respectively. This  type  of\r\nproblem  is  inevitable  because  most  bounding  boxes  are\r\nlabeled as background (i.e. negative) class by the bounding\r\nbox  matching  and  labeling  module  as  illustrated  in  Fig-\r\nure 4(a). Foreground-background imbalance problem occurs\r\nduring  training  and  it  does  not  depend  on  the  number  of\r\nexamples per class in the dataset since they do not include\r\nany annotation on background. 3. Throughout the text, unless otherwise specified, “COCO”, “PAS-CAL”, and “Open Images” respectively correspond to the Pascal VOC\r\n2012 trainval [51], MS-COCO 2017 train [90], Open Images v5 training(subset  with  bounding  boxes)  [91]  and  Objects365  train  [92]  dataset\r\npartitions. And, when unspecified, the backbone is ResNet-50 [93].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"LajmuUtTo7tLcunqdsMhmflWTH1uhXv81bBC6TmegIE="},"d9c9f9ec-696a-4128-8992-9e2fbb2be521":{"id_":"d9c9f9ec-696a-4128-8992-9e2fbb2be521","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Pd6/hBcD3MUsqn3ctoTG4h7OYuX1upvTFFyDzPqzWtQ="},"NEXT":{"nodeId":"8411ffae-8ded-465c-8cb6-b43d5c2e0a00","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"xJ3bPMGqibuw3ENni/c5B8s/rQsUzqOZl5vchaFUi0A="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW7Number of Anchors for Bg and Fg\r\n166827.50\r\n   163.04\r\nBackground               ForegroundClasses0\r\n0.5\r\n1\r\n1.5\r\n2\r\nNumber of Anchors\r\n105\r\n(a)\r\nNumber of Anchors for Fg Classes\r\n0             10             20             30             40             50             60             70             80Foreground Classes0\r\n10\r\n20\r\n30\r\n40\r\n50\r\nNumber of Anchors\r\n(b)\r\nFig. 4: Illustration of the class imbalance problems. The numbers of RetinaNet [22] anchors on MS-COCO [90] are plotted\r\nfor foreground-background(a), and foreground classes(b). The values are normalized with the total number of images in\r\nthe dataset. The figures depict severe imbalance towards some classes. Solutions.We  can  group  the  solutions  for  the  foreground-\r\nbackground  class  imbalance  into  four:  (i)  hard  sampling\r\nmethods,  (ii)  soft  sampling  methods,  (iii)  sampling-free\r\nmethods and (iv) generative methods. Each set of methods\r\nare explained in detail in the subsections below. In sampling methods, the contribution (wi) of a bound-\r\ning box (BBi) to the loss function is adjusted as follows:\r\nwiCE(ps);(2)\r\nwhereCE()is  the  cross-entropy  loss. Hard  and  soft  sam-\r\npling  approaches  differ  on  the  possible  values  ofwi. For\r\nthe  hard  sampling  approaches,wi2 f0;1g,  thus  a  BB  is\r\neither selected or discarded. For soft sampling approaches,\r\nwi2[0;1], i.e. the contribution of a sample is adjusted with\r\na weight and each BB is somehow included in training. 4.1.1  Hard Sampling Methods\r\nHard sampling is a commonly-used method for addressing\r\nimbalance in object detection. It restrictswito be binary; i.e.,\r\n0 or 1.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HR93UqHhaAsiR3HkhrTRGWON5AaTeNK8r/XhWRpdFIw="},"8411ffae-8ded-465c-8cb6-b43d5c2e0a00":{"id_":"8411ffae-8ded-465c-8cb6-b43d5c2e0a00","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Pd6/hBcD3MUsqn3ctoTG4h7OYuX1upvTFFyDzPqzWtQ="},"PREVIOUS":{"nodeId":"d9c9f9ec-696a-4128-8992-9e2fbb2be521","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"HR93UqHhaAsiR3HkhrTRGWON5AaTeNK8r/XhWRpdFIw="},"NEXT":{"nodeId":"356f2073-66ef-4bff-92d8-0656cf568589","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6FnCUeZb1PfFF9iAkFi7gWehi3liM2EYzU/uKiruSS0="}},"text":"It restrictswito be binary; i.e.,\r\n0 or 1. In other words, it addresses imbalance by selecting\r\na  subset  of  positive  and  negative  examples  (with  desired\r\nquantities)  from  a  given  set  of  labeled  BBs. This  selection\r\nis performed using heuristic methods and the non-selected\r\nexamples  are  ignored  for  the  current  iteration. Therefore,\r\neach  sampled  example  contributes  equally  to  the  loss  (i.e. wi=  1)  and  the  non-selected  examples  (wi=  0)  have  no\r\ncontribution  to  the  training  for  the  current  iteration. See\r\nTable 3 for a summary of the main approaches. A  straightforward  hard-sampling  method  israndom\r\nsampling. Despite  its  simplicity,  it  is  employed  in  R-CNN\r\nfamily  of  detectors  [16],  [21]  where,  for  training  RPN,  128\r\npositive  examples  are  sampled  uniformly  at  random  (out\r\nof  all  positive  examples)  and  128  negative  anchors  are\r\nsampled in a similar fashion; and 16 positive examples and\r\n48  negative  RoIs  are  sampled  uniformly  from  each  image\r\nin  the  batch  at  random  from  within  their  respective  sets,\r\nfor  training  the  detection  network  [17]. In  any  case,  if  the\r\nnumber  of  positive  input  bounding  boxes  is  less  than  the\r\ndesired  values,  the  mini-batch  is  padded  with  randomly\r\nsampled negatives. On the other hand, it has been reported\r\nthat  other  sampling  strategies  may  perform  better  when  a\r\nproperty  of  an  input  box  such  as  its  loss  value  or  IoU  is\r\ntaken into account [24], [29], [30].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"xJ3bPMGqibuw3ENni/c5B8s/rQsUzqOZl5vchaFUi0A="},"356f2073-66ef-4bff-92d8-0656cf568589":{"id_":"356f2073-66ef-4bff-92d8-0656cf568589","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Pd6/hBcD3MUsqn3ctoTG4h7OYuX1upvTFFyDzPqzWtQ="},"PREVIOUS":{"nodeId":"8411ffae-8ded-465c-8cb6-b43d5c2e0a00","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"xJ3bPMGqibuw3ENni/c5B8s/rQsUzqOZl5vchaFUi0A="},"NEXT":{"nodeId":"5d4ebb6d-272f-41da-abef-2f260f7acf88","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZSJ7GOaTohiaxPbPJ214EeCNLuV1IK9EDfZx2GRwCU8="}},"text":"The  first  set  of  approaches  to  consider  a  property  of\r\nthe  sampled  examples,  rather  than  random  sampling,  is\r\ntheHard-example  mining  methods4. These  methods  rely\r\non  the  hypothesis  that  training  a  detector  more  with  hard\r\nexamples  (i.e. examples  with  high  losses)  leads  to  better\r\nperformance. The origins of this hypothesis go back to the\r\nbootstrappingidea in the early works on face detection [55],\r\n[94],  [95],  human  detection  [96]  and  object  detection  [13]. The idea is based on training an initial model using a subset\r\nof negative examples, then using the negative examples on\r\nwhich the classifier fails (i.e. hard examples), a new classifier\r\nis trained. Multiple classifiers are obtained by applying the\r\nsame  procedure  iteratively. Currently  deep-learning-based\r\nmethods also adopt some versions of the hard example min-\r\ning in order to provide more useful examples by using the\r\nloss values of the examples. The first deep object detector to\r\nuse hard examples in the training was Single-Shot Detector\r\n[19],  which  chooses  only  the  negative  examples  incurring\r\nthe  highest  loss  values. A  more  systematic  approach  con-\r\nsidering  the  loss  values  of  positive  and  negative  samples\r\nis  proposed  inOnline  Hard  Example  Mining(OHEM)  [24]. However, OHEM needs additional memory and causes the\r\ntraining  speed  to  decrease. Considering  the  efficiency  and\r\nmemory  problems  of  OHEM,IoU-based  sampling[29]  was\r\nproposed  to  associate  the  hardness  of  the  examples  with\r\ntheir  IoUs  and  to  use  a  sampling  method  again  for  only\r\nnegative examples rather than computing the loss function\r\nfor the entire set.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6FnCUeZb1PfFF9iAkFi7gWehi3liM2EYzU/uKiruSS0="},"5d4ebb6d-272f-41da-abef-2f260f7acf88":{"id_":"5d4ebb6d-272f-41da-abef-2f260f7acf88","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Pd6/hBcD3MUsqn3ctoTG4h7OYuX1upvTFFyDzPqzWtQ="},"PREVIOUS":{"nodeId":"356f2073-66ef-4bff-92d8-0656cf568589","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6FnCUeZb1PfFF9iAkFi7gWehi3liM2EYzU/uKiruSS0="}},"text":"In the IoU-based sampling, the IoU interval\r\nfor the negative samples is divided intoKbins and equal\r\nnumber of negative examples are sampled randomly within\r\neach  bin  to  promote  the  samples  with  higher  IoUs,  which\r\nare expected to have higher loss values. To  improve  mining  performance,  several  studies  pro-\r\nposed  tolimit  the  search  spacein  order  to  make  hard\r\nexamples  easy  to  mine.Two-stage  object  detectors[18],  [21]\r\nare  among  these  methods  since  they  aim  to  find  the  most\r\nprobable bounding boxes (i.e. RoIs) given anchors, and then\r\nchoose  top  N  RoIs  with  the  highest  objectness  scores,  to\r\nwhich  an  additional  sampling  method  is  applied. Fast  R-\r\nCNN  [17]  sets  thelower  bound  of  IoUof  the  negative  RoIs\r\n4. In this paper, we adopt the boldface font whenever we introducean  approach  involving  a  set  of  different  methods,  and  the  method\r\nnames themselves are in italic.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZSJ7GOaTohiaxPbPJ214EeCNLuV1IK9EDfZx2GRwCU8="},"db4d987e-fa8a-4be2-a86b-fba5883487db":{"id_":"db4d987e-fa8a-4be2-a86b-fba5883487db","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"T9ebeMItZTUs4STXUQXzb/uayy7+k4FDkZQ+a5tPwq4="},"NEXT":{"nodeId":"e09c1912-b9af-4c16-9bd4-561dda2388d7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vOj6dp+R+kFAjQ2ItHIShOr02A1rO82a3Vdt2LgbiSM="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW8\r\nto0:1rather  than0for  promoting  hard  negatives  and\r\nthen applies random sampling. Kong et al. [56] proposed a\r\nmethod that learnsobjectness priorsin an end-to-end setting\r\nin  order  to  have  a  guidance  on  where  to  search  for  the\r\nobjects. All  of  the  positive  examples  having  an  objectness\r\nprior larger than a threshold are used during training, while\r\nthe  negative  examples  are  selected  such  that  the  desired\r\nbalance (i.e.1  :  3) is  preserved between  positive and neg-\r\native  classes. Zhang  et  al. [57]  proposed  determining  the\r\nconfidence  scores  of  anchors  with  the  anchor  refinement\r\nmodule in a one-stage detection pipeline and again adopted\r\na  threshold  to  eliminate  the  easy  negative  anchors. The\r\nauthors  coined  their  approach  asnegative  anchor  filtering. Nie et al. [58] used a cascaded-detection scheme in the SSD\r\npipeline  which  includes  anObjectness  Modulebefore  each\r\nprediction  module. These  objectness  modules  are  binary\r\nclassifiers to filter out the easy anchors. 4.1.2  Soft Sampling Methods\r\nSoft sampling adjusts the contribution (wi) of each example\r\nby its relative importance to the training process. This way,\r\nunlike hard sampling, no sample is discarded and the whole\r\ndataset is utilized for updating the parameters. See Table 3\r\nfor a summary of the main approaches. A  straightforward  approach  is  to  use  constant  coeffi-\r\ncients  for  both  the  foreground  and  background  classes. YOLO  [20],  having  less  number  of  anchors  compared  to\r\nother  one-stage  methods  such  as  SSD  [19]  and  RetinaNet\r\n[22], is a straightforward example for soft sampling in which\r\nthe loss values of the examples from the background class\r\nare halved (i.e.wi= 0:5).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Ue+0NINOZITGvtzkUCUXIoU5oTF08RTPATSNugsNlDw="},"e09c1912-b9af-4c16-9bd4-561dda2388d7":{"id_":"e09c1912-b9af-4c16-9bd4-561dda2388d7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"T9ebeMItZTUs4STXUQXzb/uayy7+k4FDkZQ+a5tPwq4="},"PREVIOUS":{"nodeId":"db4d987e-fa8a-4be2-a86b-fba5883487db","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Ue+0NINOZITGvtzkUCUXIoU5oTF08RTPATSNugsNlDw="},"NEXT":{"nodeId":"d39cab8c-9bed-4bf8-b0f9-b16afdda54d8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZGw8jgDJ5ySCkvWfXYXZuF+V8NC+sdI2meTFuGTBrBE="}},"text":"Focal Loss[22] is the pioneer example that dynamically\r\nassigns more weight to the hard examples as follows:\r\nwi= (1\u0000ps)\r;(3)\r\nwherepsis the estimated probability for the ground-truth\r\nclass. Since  lowerpsimplies  a  larger  error,  Equation  (3)\r\npromotes  harder  examples. Note  that  when\r=  0,  focal\r\nloss degenerates to vanilla cross entropy loss and Lin et al. [22]  showed  that\r=  2ensures  a  good  trade-off  between\r\nhard and easy examples for their architecture. Similar  to  focal  loss  [22],Gradient  Harmonizing  Mech-\r\nanism(GHM)  [59]  suppresses  gradients  originating  from\r\neasy positives and negatives. The authors first observed that\r\nthere are too many samples with small gradient norm, only\r\nlimited number of samples with medium gradient norm and\r\nsignificantly  large  amount  of  samples  with  large  gradient\r\nnorm. Unlike focal loss, GHM is a counting-based approach\r\nwhich counts the number of examples with similar gradient\r\nnorm and penalizes the loss of a sample if there are many\r\nsamples with similar gradients as follows:\r\nwi=1G(BB\r\ni)=m\r\n;(4)\r\nwhereG(BBi)is the count of samples whose gradient norm\r\nis close to the gradient norm ofBBi; andmis the number\r\nof  input  bounding  boxes  in  the  batch. In  this  sense,  the\r\nGHM  method  implicitly  assumes  that  easy  examples  are\r\nthose with too many similar gradients. Different from other\r\nmethods, GHM is shown to be useful not only for classifica-\r\ntion task but also for the regression task. In addition, since\r\nthe purpose is to balance the gradients within each task, this\r\nmethod is also relevant to the “imbalance in regression loss”\r\ndiscussed in Section 6.1. Different  from  the  latter  soft  sampling  methods,PrIme\r\nSample Attention(PISA) [30] assigns weights to positive and\r\nnegative  examples  based  on  different  criteria.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"vOj6dp+R+kFAjQ2ItHIShOr02A1rO82a3Vdt2LgbiSM="},"d39cab8c-9bed-4bf8-b0f9-b16afdda54d8":{"id_":"d39cab8c-9bed-4bf8-b0f9-b16afdda54d8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"T9ebeMItZTUs4STXUQXzb/uayy7+k4FDkZQ+a5tPwq4="},"PREVIOUS":{"nodeId":"e09c1912-b9af-4c16-9bd4-561dda2388d7","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vOj6dp+R+kFAjQ2ItHIShOr02A1rO82a3Vdt2LgbiSM="},"NEXT":{"nodeId":"4c943646-7d3e-47d9-996b-e96a5dee8552","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"mphgAlZsvWae2JEI0EW+pNVXM3UXqhR816v9K6S1RTM="}},"text":"While  the\r\npositive  ones  with  higher  IoUs  are  favored,  the  negatives\r\nwith  larger  foreground  classification  scores  are  promoted. More  specifically,  PISA  first  ranks  the  examples  for  each\r\nclass based on their desired property (IoU or classification\r\nscore) and calculates a normalized rank,\u0016ri, for each example\r\nias follows:\r\n\u0016ri=Nmax\u0000riN\r\nmax\r\n;(5)\r\nwhereri(0\u0014ri\u0014Nj\u00001) is the rank of theith example and\r\nNmaxis the maximum number of examples over all classes\r\nin  the  batch. Based  on  the  normalized  rank,  the  weight  of\r\neach example is defined as:\r\nwi= ((1\u0000\f)\u0016ri+\f)\r;(6)\r\nwhere\fadjusts  the  contribution  of  the  normalized  rank\r\nand  hence,  the  minimum  sample  weight;  and\ris  the\r\nmodulating  factor  again. These  parameters  are  validated\r\nfor  positives  and  negatives  independently. Note  that  the\r\nbalancing  strategy  in  Equations  (5)  and  (6)  increases  the\r\ncontribution of the samples with high IoUs for positives and\r\nhigh scores for negatives to the loss. 4.1.3  Sampling-Free Methods\r\nRecently,  alternative  methods  emerged  in  order  to  avoid\r\nthe  aforementioned  hand-crafted  sampling  heuristics  and\r\ntherefore, decrease the number of hyperparameters during\r\ntraining. For  this  purpose,  Chen  et  al. [60]  added  an  ob-\r\njectness branch to the detection network in order to predict\r\nResidual Objectnessscores. While this new branch tackles the\r\nforeground-background imbalance, the classification branch\r\nhandles  only  the  positive  classes. During  inference,  clas-\r\nsification  scores  are  obtained  by  multiplying  classification\r\nand  objectness  branch  outputs. The  authors  showed  that\r\nsuch  a  cascaded  pipeline  improves  the  performance. This\r\narchitecture is trained using vanilla cross entropy loss.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZGw8jgDJ5ySCkvWfXYXZuF+V8NC+sdI2meTFuGTBrBE="},"4c943646-7d3e-47d9-996b-e96a5dee8552":{"id_":"4c943646-7d3e-47d9-996b-e96a5dee8552","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"T9ebeMItZTUs4STXUQXzb/uayy7+k4FDkZQ+a5tPwq4="},"PREVIOUS":{"nodeId":"d39cab8c-9bed-4bf8-b0f9-b16afdda54d8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZGw8jgDJ5ySCkvWfXYXZuF+V8NC+sdI2meTFuGTBrBE="}},"text":"This\r\narchitecture is trained using vanilla cross entropy loss. Another  recent  approach  [54]  suggests  that  if  the  hy-\r\nperparameters  are  set  appropriately,  not  only  the  detector\r\ncan  be  trained  without  any  sampling  mechanism  but  also\r\nthe performance can be improved. Accordingly, the authors\r\npropose  methods  for  setting  the  initialization  bias,  loss\r\nweighting (see Section 7) and class-adaptive threshold, and\r\nin such a way they trained the network using vanilla cross\r\nentropy loss achieving better performance. Finally,  an  alternative  method  is  to  directly  model  the\r\nfinal  performance  measure  and  weigh  examples  based  on\r\nthis model. This approach is adopted byAP Loss[61] which\r\nformulates the classification part of the loss as a ranking task\r\n(see alsoDR Loss[62] which also uses a ranking method to\r\ndefine  a  classification  loss  based  on  Hinge  Loss)  and  uses\r\naverage precision (AP) as the loss function for this task. In\r\nthis method, the confidence scores are first transformed as\r\nxij=\u0000(pi\u0000pj)such thatpiis the confidence score of the\r\nith bounding box. Then, based on the transformed values,","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"mphgAlZsvWae2JEI0EW+pNVXM3UXqhR816v9K6S1RTM="},"9aa12ff9-2254-454c-9699-bd6ad0c99210":{"id_":"9aa12ff9-2254-454c-9699-bd6ad0c99210","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jb9wggyEbH9l3t6SUVeZyMbVjbx5uykfhbFMfvf9zQY="},"NEXT":{"nodeId":"f2445676-34dd-4284-b3ed-e217dd044e71","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Hm3S2IRP3zApOk6cQGUVK/uF/4b3FF2YW6CcDubh5Ic="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW9\r\nTABLE 3: A toy example depicting the selection methods of common hard and soft sampling methods. One positive and\r\ntwo negative examples are to be chosen from six bounding boxes (drawn at top-right). The properties are the basis for\r\nthe  sampling  methods.psis  the  predicted  ground  truth  probability  (i.e. positive  class  probability  for  positive  BBs,  and\r\nbackground probability for negative BBs). If we set a property or hyperparameter for this example, it is shown in the table. For soft sampling methods, the numbers are the weights of each box (i.e.wi). We assumed one foreground class for PISA. Legend&MethodConsideredPropertyIntuitionAddit.Params.Positive ExamplesNegative Examples\r\nBoxes\r\nNegative BB\r\nPositive BB\r\nGround Truth\r\n3Selected BB7Discarded BB\r\nN/AN/AN/A\r\nps=0.8Loss = 0.22\r\nIoU = 0.95\r\nps=0.2Loss = 1.61\r\nIoU = 0.55\r\nps=0.1Loss = 2.30\r\nIoU = 0.35\r\nps=0.3Loss = 1.20\r\nIoU = 0.15\r\nps=0.6Loss = 0.51\r\nIoU = 0.06\r\nps=1.0Loss = 0.00\r\nIoU = 0.00\r\nHard Sampling\r\nRandom Sampling\u0000Samples uniformlyamong boxes\u0000Random Sample\u00021Random Sample\u00022\r\nSSD [19]LossChooses neg. with maxloss\u0000Random Sample\u000213377\r\nOHEM [24]LossChooses pos.&neg. withmax loss\u0000733377\r\nIoU-Based Sampling [29]IoUSamples equal number ofnegatives from IoU binsK=2Does  not  specify  a  crite-rion for positivesRandomSample\u00021Random Sample\u00021\r\nIoU Lower Bound [17]IoU=0.05Discards neg.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"no0cXAUKFArgN37tESRWWq7JB0gNzuds0BX1AD/P6/0="},"f2445676-34dd-4284-b3ed-e217dd044e71":{"id_":"f2445676-34dd-4284-b3ed-e217dd044e71","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jb9wggyEbH9l3t6SUVeZyMbVjbx5uykfhbFMfvf9zQY="},"PREVIOUS":{"nodeId":"9aa12ff9-2254-454c-9699-bd6ad0c99210","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"no0cXAUKFArgN37tESRWWq7JB0gNzuds0BX1AD/P6/0="},"NEXT":{"nodeId":"f7578c17-3f2e-46d7-ba1e-2975365f554d","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"eiQSSapDakWkXg8ePAXT0BXBxIxnNEaJVGUO+UciI/o="}},"text":"having IoUless than lower bound\u0000Does  not  specify  a  crite-rion for positivesRandom Sample\u000227\r\nObjectness Prior [56]\u0000Learns to predictobjectness priors for pos.\u0000Chooses   all   with   priorlarger than threshold.The  negatives  are  sampled  randomly  by  preserving  1:3ratio with positives. So, fixed batch size is not considered. Negative   Anchor   Filter-ing [57]ps= 0:9Discards negs. less thanthreshold\u0000Does  not  specify  a  crite-rion for positivesUses  OHEM  for  the  remaining  negatives.In this case, the first two boxes are selected7\r\nSoft Sampling\r\nFocal Loss [22]psPromotes examples withlarger loss values\r= 10.20.80.90.70.40.0\r\nGradient      HarmonizingMechanism [59]psPromotes hard examplesby suppressing effects of\r\noutliers\u000f= 0:40.660.660.600.661.20.60\r\nPrime  Sample  Attention[30]IoU,pPromotes examples withlarger IoUs for positivesand larger fg scores for\r\nnegatives (i.e. 1-pshere)\r\n\r= 1\f= 01.000.751.000.750.500.25\r\nthe primary terms of the AP loss are computed as (presented\r\nhere in a simplified form):\r\nUij=I(xij>0)1 +P\r\nk2P[N\r\nI(xik>0);(7)\r\nwhereP;Nare  the  set  of  positive  and  negative  labeled\r\nexamples respectively. Note thatUijis zero ifpi< pjand\r\na positive value less than one otherwise. With this quantity,\r\nAP loss is defined as follows:\r\nLAP=1jPjX\r\ni;j\r\nUijyij;(8)\r\nwhereyijis  the  ranking  label,  set  to1only  if  theith  box\r\nis a foreground bounding box andjth box is a background\r\nbounding box.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Hm3S2IRP3zApOk6cQGUVK/uF/4b3FF2YW6CcDubh5Ic="},"f7578c17-3f2e-46d7-ba1e-2975365f554d":{"id_":"f7578c17-3f2e-46d7-ba1e-2975365f554d","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jb9wggyEbH9l3t6SUVeZyMbVjbx5uykfhbFMfvf9zQY="},"PREVIOUS":{"nodeId":"f2445676-34dd-4284-b3ed-e217dd044e71","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Hm3S2IRP3zApOk6cQGUVK/uF/4b3FF2YW6CcDubh5Ic="},"NEXT":{"nodeId":"3fb8ec15-3a20-4c37-b5bc-64cb97d2c7b2","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"232iY/a2VJ5QgaU1RuU39Htz2tp1fk3+4Mx4N+jHOSc="}},"text":"To make this differentiable, the authors propose a novel\r\nerror-driven  update  rule,  which  performs  better  than  the\r\nprevious  works  aiming  to  include  average  precision  as\r\na  training  objective  [97],  [98]. However,  this  optimization\r\nalgorithm is beyond the scope of our work. 4.1.4  Generative Methods\r\nUnlike sampling-based and sampling-free methods, genera-\r\ntive methods address imbalance by directly producing and\r\ninjecting artificial samples into the training dataset. Table 4\r\npresents a summary of the main approaches. One approach is to use generative adversarial networks\r\n(GANs). A  merit  of  GANs  is  that  they  adapt  themselves\r\nto generate harder examples during training since the loss\r\nvalues of these networks are directly based on the classifica-\r\ntion accuracy of the generated examples in the final detec-\r\ntion. An example is theAdversarial-Fast-RCNNmodel [63],\r\nwhich generates hard examples with occlusion and various\r\ndeformations. In this method, the generative manipulation\r\nis directly performed at the feature-level, by taking the fixed\r\nsize feature maps after RoI standardization layers (i.e. RoI\r\npooling  [17]). For  this  purpose,  they  proposed  two  net-\r\nworks: (i) adversarial spatial dropout network for occluded\r\nfeature  map  generation,  and  (ii)  adversarial  spatial  trans-\r\nformer  network  for  deformed  (transformed)  feature  map\r\ngeneration. These two networks are placed sequentially in\r\nthe  network  design  in  order  to  provide  harder  examples\r\nand they are integrated into the conventional object training\r\npipeline in an end-to-end manner. Alternatively, artificial images can be produced to aug-\r\nment  the  dataset  [64],  [65],  [99],  [100]  by  generating  com-\r\nposite  images  in  which  multiple  crops  and/or  images  are\r\nblended. A straightforward approach is to randomly place\r\ncropped objects onto images as done by Dwibedi et al. [99]. However,  the  produced  images  may  look  unrealistic.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"eiQSSapDakWkXg8ePAXT0BXBxIxnNEaJVGUO+UciI/o="},"3fb8ec15-3a20-4c37-b5bc-64cb97d2c7b2":{"id_":"3fb8ec15-3a20-4c37-b5bc-64cb97d2c7b2","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jb9wggyEbH9l3t6SUVeZyMbVjbx5uykfhbFMfvf9zQY="},"PREVIOUS":{"nodeId":"f7578c17-3f2e-46d7-ba1e-2975365f554d","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"eiQSSapDakWkXg8ePAXT0BXBxIxnNEaJVGUO+UciI/o="}},"text":"[99]. However,  the  produced  images  may  look  unrealistic. This\r\nproblem  is  alleviated  by  determining  where  to  paste  and\r\nthe size of the pasted region according to the visual context","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"232iY/a2VJ5QgaU1RuU39Htz2tp1fk3+4Mx4N+jHOSc="},"4061a2a7-3a7c-4f01-ab0b-7ed7d16e2447":{"id_":"4061a2a7-3a7c-4f01-ab0b-7ed7d16e2447","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zfOe4c5fVAhjeT5EREcoKXajIe2cdQZijb6c5ML8CzE="},"NEXT":{"nodeId":"4fbeabd8-d54c-4752-bc6e-c7a77b2d2c1f","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZpTnq8wFv9epBmTH5ypp2PM4lfB89pRq7oJju5WO1+U="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW10\r\n[100]. In a similar vein, the objects can be swapped between\r\nimages:Progressive and Selective Instance-Switching(PSIS) [65]\r\nswaps single objects belonging to the same class between a\r\npair of images considering also the scales and shapes of the\r\ncandidate instances. Producing images by swapping objects\r\nof  low-performing  classes  improves  detection  quality. For\r\nthis reason, they use the performance ranking of the classes\r\nwhile  determining  the  objects  to  swap  and  the  number  of\r\nimages to generate. A more prominent approach is to use GANs to generate\r\nimages rather than copying existing objects: an example is\r\nTask Aware Data Synthesis[64] which uses three competing\r\nnetworks  to  generate  hard  examples:  a  synthesizer,  a  dis-\r\ncriminator  and  a  target  network  where  the  synthesizer  is\r\nexpected to fool both the discriminator and the target net-\r\nwork by yielding high quality synthetic hard images. Given\r\nan  image  and  a  foreground  object  mask,  the  synthesizer\r\naims  to  place  the  foreground  object  mask  onto  the  image\r\nto  produce  realistic  hard  examples. The  discriminator  is\r\nadopted in order to enforce the synthesizer towards realistic\r\ncomposite images. The target network is an object detector,\r\ninitially pretrained to have a baseline performance. Instead  of  generating  images,  thePositive  RoI  (pRoI)\r\nGenerator[66]  generates  a  set  of  positive  RoIs  with  given\r\nIoU, BB relative spatial and foreground class distributions. The approach relies on a bounding box generator that is able\r\nto generate bounding boxes (i.e. positive example) with the\r\ndesired IoU with a given bounding box (i.e. ground truth). Noting that the IoU of an input BB is related to its hardness\r\n[29],  the  pRoI  generator  is  a  basis  for  simulating,  thereby\r\nanalyzing, hard sampling methods (Section 4.1.1).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wvQfrLqrYP2P4J6OojgLwM2epl8DlAWrBi76jI3P4rg="},"4fbeabd8-d54c-4752-bc6e-c7a77b2d2c1f":{"id_":"4fbeabd8-d54c-4752-bc6e-c7a77b2d2c1f","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zfOe4c5fVAhjeT5EREcoKXajIe2cdQZijb6c5ML8CzE="},"PREVIOUS":{"nodeId":"4061a2a7-3a7c-4f01-ab0b-7ed7d16e2447","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"wvQfrLqrYP2P4J6OojgLwM2epl8DlAWrBi76jI3P4rg="},"NEXT":{"nodeId":"ea520b57-6e7e-4346-8525-787197452e4c","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6HSE+2wieRUmT28qwoW4u8mThNe51yDZGxx7wdTdZII="}},"text":"4.2    Foreground-Foreground Class Imbalance\r\nDefinition.In  foreground-foreground  class  imbalance,  the\r\nover-represented and the under-represented classes are both\r\nforeground  classes. This  imbalance  among  the  foreground\r\nclasses  has  not  attracted  as  much  interest  as  foreground-\r\nbackground  imbalance. We  discuss  this  problem  in  two\r\ncategories according to their origin: (i) dataset-level, and (ii)\r\nbatch-level. 4.2.1  Foreground-Foreground Imbalance Owing to the\r\nDataset\r\nDefinition.Objects  exist  at  different  frequencies  in  nature,\r\nand  therefore,  there  is  naturally  an  imbalance  among  the\r\nobject classes in datasets – see Figure 5(a) which shows that\r\nthe  datasets  suffer  from  significant  gap  in  class  examples. For this reason, overfitting in favor of the over-represented\r\nclasses  may  be  inevitable  for  naive  approaches  on  such\r\ndatasets. Solutions.Owing  to  the  fact  that  some  of  the  generative\r\nmethods  are  able  to  generate  new  images  or  bounding\r\nboxes  (see  Section  4.1.4)  that  cannot  be  obtained  by  the\r\nconventional  training  pipeline,  these  methods  can  also  be\r\nadopted to alleviate the foreground-foreground class imbal-\r\nance problem. Another   method   addressing   this   imbalance   problem\r\nfrom   the   object   detection   perspective   is   proposed   by\r\nOuyang  et  al. [25]. Their  method  offinetuning  long-tail\r\ndistribution for object detection(here, “long tail” corresponds\r\nto  under-represented  classes)  provides  an  analysis  on  the\r\neffects of this level to the training process and uses cluster-\r\ning based on visual similarity. In their analysis, two factors\r\naffecting  the  training  are  identified:  (i)  the  accuracy  of  the\r\nprediction, and (ii) the number of examples.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZpTnq8wFv9epBmTH5ypp2PM4lfB89pRq7oJju5WO1+U="},"ea520b57-6e7e-4346-8525-787197452e4c":{"id_":"ea520b57-6e7e-4346-8525-787197452e4c","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zfOe4c5fVAhjeT5EREcoKXajIe2cdQZijb6c5ML8CzE="},"PREVIOUS":{"nodeId":"4fbeabd8-d54c-4752-bc6e-c7a77b2d2c1f","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZpTnq8wFv9epBmTH5ypp2PM4lfB89pRq7oJju5WO1+U="},"NEXT":{"nodeId":"a40a1327-810b-4eff-86e6-6b8e0ce3dae6","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"A7fxMOXkCooYLmMaCeYQ0YZ0FiyNszJgBjtKMGw/VII="}},"text":"Based on this\r\nobservation, they handcrafted a similarity measure among\r\nclasses  based  on  the  inner  product  of  the  features  of  the\r\nlast layer of the pretrained backbone network (i.e. GoogLe\r\nNet [101]), and grouped the classes hierarchically in order\r\nto compensate for dataset-level foreground class imbalance. For  each  node  in  the  designed  hierarchy  tree,  a  classifier\r\nis  learned  based  on  the  confidence  scores  of  the  classifier. The  leaves  of  the  tree  are  basically  an  SVM  classifier  that\r\ndetermines the final detection for the given input BB. 4.2.2  Foreground-Foreground Imbalance Owing to the\r\nBatch\r\nDefinition.The  distribution  of  classes  in  a  batch  may  be\r\nuneven,  introducing  a  bias  in  learning. To  illustrate  the\r\nbatch-level foreground imbalance, Figure 4(b) provides the\r\nmean number of anchors per class on the MS COCO dataset\r\n[90]. A random sampling approach is expected to allocate an\r\nunbalanced number of positive examples in favor of the one\r\nwith more anchors, which may lead the model to be biased\r\nin favor of the over-represented class during training. Also\r\nsee Figure 5(b) and (c), which display that the numbers of\r\nobjects and classes in an image vary significantly. Solutions.A  solution  for  this  problem,Online  Foreground\r\nBalanced  (OFB)  samplingby  Oksuz  et  a. [66],  shows  that\r\nthe foreground-foreground class imbalance problem can be\r\nalleviated  at  the  batch  level  by  assigning  probabilities  to\r\neach bounding box to be sampled, so that the distribution\r\nof  different  classes  within  a  batch  is  uniform. In  other\r\nwords, the approach aims to promote the classes with lower\r\nnumber  of  positive  examples  during  sampling. While  the\r\nmethod  is  efficient,  the  performance  improvement  is  not\r\nsignificant.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6HSE+2wieRUmT28qwoW4u8mThNe51yDZGxx7wdTdZII="},"a40a1327-810b-4eff-86e6-6b8e0ce3dae6":{"id_":"a40a1327-810b-4eff-86e6-6b8e0ce3dae6","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zfOe4c5fVAhjeT5EREcoKXajIe2cdQZijb6c5ML8CzE="},"PREVIOUS":{"nodeId":"ea520b57-6e7e-4346-8525-787197452e4c","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6HSE+2wieRUmT28qwoW4u8mThNe51yDZGxx7wdTdZII="}},"text":"Moreover,  the  authors  have  not  provided  an\r\nanalysis  on  whether  or  not  batch-level  imbalance  causes\r\na  bias  in  learning,  therefore,  we  discuss  this  as  an  open\r\nproblem in Section 4.4. 4.3    Comparative Summary\r\nIn early deep object detectors, hard sampling methods were\r\ncommonly  used  to  ensure  balanced  training. OHEM  [24]\r\nwas  one  of  the  most  effective  methods  with  a  relative\r\nimprovement of14:7%over the Fast R-CNN baseline (with\r\nVGG-16 backbone) on MS COCO 2015. On the other hand,\r\nrecent  methods  aim  to  use  all  examples  either  by  soft\r\nsampling  or  without  sampling. Among  the  seven  papers\r\ninvestigated under these categories, six of them have been\r\npublished  during  the  last  year. The  pioneering  Focal  Loss\r\nmethod [22] achieves9:3%relative improvement compared\r\nto the baseline alpha-balanced cross entropy on Retina-Net\r\n(i.e. from31:1to34:0mAP5). AP Loss [61], with a relative\r\nimprovement of3:9%over Focal Loss (i.e. from33:9to35:0\r\nmAP), is also promising. Sampling methods for positive examples are not many. One approach, prime sample attention (PISA) [30], reported\r\n5. Unless  otherwise  stated,  MS  COCO  2017  minival  results  are  re-ported using the COCO-style mAP.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"A7fxMOXkCooYLmMaCeYQ0YZ0FiyNszJgBjtKMGw/VII="},"6209f235-3334-4c2e-b709-4b354ef40d4a":{"id_":"6209f235-3334-4c2e-b709-4b354ef40d4a","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gTPWbvd+8twOP1s+CHK9CnD9Y4cqU2chodQtB1QzdBM="},"NEXT":{"nodeId":"cce778c3-764f-4342-b832-d841bf42fe97","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"56tYTDyYzQUGFDz80fzfTZx9yjs2kl1sXhZ3/pH3axk="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW11\r\nTABLE 4: Comparison of major generative methods addressing class imbalance. Generative MethodGenerates\r\nAdversarial-Fast-RCNN [63]Occluded and spatially transformed features during RoI pooling in order\r\nto make the examples harder\r\nTask Aware Data Synthesis [64]Images with hard examples in a GAN setting in which given foreground\r\nmask is to be placed onto the given image by the generator. PSIS [65]Images  by  switching  the  instances  between  existing  images  considering\r\nthe performance of the class during training\r\npRoI Generator [66]Positive RoIs (i.e. BBs) following desired IoU, spatial and foreground class\r\ndistributions\r\n100101102Class sorted position100\r\n101\r\n102\r\n103\r\n104\r\n105\r\n106\r\nNumber of boxes\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(a)\r\n20406080Number of objects per image100\r\n101\r\n102\r\n103\r\n104\r\n105\r\n106\r\nNumber of images\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(b)\r\n102030Number of distinct classes per image100\r\n101\r\n102\r\n103\r\n104\r\n105\r\n106\r\nNumber of images\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(c)\r\nFig. 5: Some statistics of common datasets (training sets). For readability, theyaxes are in logarithmic scale.(a)The total\r\nnumber of examples from each class.(b)The number of images vs. the number of examples.(c)The number of images vs. the number of classes. (This figure is inspired from Kuznetsova et al. [91] but calculated and drawn from scratch). a  relative  improvement  of4:4%(from36:4to38:0mAP\r\nwhen applied to positives only) and showed that sampling\r\nalso  matters  for  positives. We  notice  several  crucial  points\r\nregarding prime sampling.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Uxn38OCnpDxnZj2XqMGatjjhvtM1UKLqpLEqki8hOHQ="},"cce778c3-764f-4342-b832-d841bf42fe97":{"id_":"cce778c3-764f-4342-b832-d841bf42fe97","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gTPWbvd+8twOP1s+CHK9CnD9Y4cqU2chodQtB1QzdBM="},"PREVIOUS":{"nodeId":"6209f235-3334-4c2e-b709-4b354ef40d4a","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Uxn38OCnpDxnZj2XqMGatjjhvtM1UKLqpLEqki8hOHQ="},"NEXT":{"nodeId":"78e7bc37-a72f-452a-92d7-2226a03a1adc","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"DGFZx3B+GreG4KE9u1kK2Uvjv6vcTkni0abtJ6JsKOE="}},"text":"We  notice  several  crucial  points\r\nregarding prime sampling. First of all, contrary to the pop-\r\nular  belief  that  hard  examples  are  more  preferable  over\r\neasy  examples,  PISA  shows  that,  if  balanced  properly,  the\r\npositive examples with higher IoUs, which normally incur\r\nsmaller loss values, are more useful for training compared\r\nto  OHEM  [24]  applied  to  positives. Moreover,  the  results\r\nsuggest  that  the  major  improvement  of  the  method  is  on\r\nlocalization since there is no performance improvement in\r\nAP@0:50,  and  there  is  significant  improvement  for  APs\r\nwith higher IoUs (i.e. up to2:6%inAP@0:75). As a result,\r\nthe improvement can be due to the changing nature of the\r\nIoU  distribution  rather  than  presenting  more  descriptive\r\nsamples to the classifier since the classifier performs worse\r\nbut the regressor improves (see the discussion on IoU dis-\r\ntribution imbalance in Section 6.2). 4.4    Open Issues\r\nAs we have highlighted before, class imbalance problem can\r\nbe analyzed in two main categories: foreground-background\r\nclass  imbalance  and  foreground-foreground  class  imbal-\r\nance. In  the  following,  we  identify  issues  to  be  addressed\r\nwith  a  more  focus  on  foreground-foreground  imbalance\r\nsince it has received less attention. 4.4.1  Sampling More Useful Examples\r\nOpen  Issue.Many  criteria  to  identify  useful  examples  (e.g. hard  example,  example  with  higher  IoUs  etc. )  for  both\r\npositive and negative classes have been proposed. However,\r\nrecent  studies  point  out  interesting  phenomena  that  need\r\nfurther  investigation:  (i)  For  the  foreground-background\r\nclass imbalance, soft sampling methods have become more\r\nprominent and optimal weighting of examples needs further\r\ninvestigation. (ii) Hard example mining [24] for the positive\r\nexamples  is  challenged  by  the  idea  that  favors  examples\r\nwith higher IoUs, i.e. the easier ones [30].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"56tYTDyYzQUGFDz80fzfTZx9yjs2kl1sXhZ3/pH3axk="},"78e7bc37-a72f-452a-92d7-2226a03a1adc":{"id_":"78e7bc37-a72f-452a-92d7-2226a03a1adc","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gTPWbvd+8twOP1s+CHK9CnD9Y4cqU2chodQtB1QzdBM="},"PREVIOUS":{"nodeId":"cce778c3-764f-4342-b832-d841bf42fe97","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"56tYTDyYzQUGFDz80fzfTZx9yjs2kl1sXhZ3/pH3axk="}},"text":"the easier ones [30]. (iii) The usefulness\r\nof the examples are not considered in terms of foreground-\r\nforeground class imbalance. Explanation6.In  terms  of  the  usefulness  of  the  examples,\r\nthere  are  two  criteria  to  be  identified:  (i)  The  usefulness\r\nof the background examples, and (ii) the usefulness of the\r\nforeground examples. The existing approaches mostly concentrated around the\r\nfirst criterion using different properties (i.e. IoU, loss value,\r\nground truth confidence score) to sample a useful example\r\nfor the background. However, the debate is still open after\r\nLi et al. [59] showed that there are large number of outliers\r\nduring sampling using these properties, which will result in\r\nhigher  loss  values  and  lower  confidence  scores. Moreover,\r\nthe  methods  preferring  a  weighting  over  all  the  examples\r\n6. Issues  that  may  not  be  immediately  clear  include  a  detailingexplanation section","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DGFZx3B+GreG4KE9u1kK2Uvjv6vcTkni0abtJ6JsKOE="},"b30c9d7d-db86-4c5f-8dfc-399dc86a938f":{"id_":"b30c9d7d-db86-4c5f-8dfc-399dc86a938f","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jasCtpvuGzNonrjKxxGK6eRa+i7UsNUjj1yqriN1Alo="},"NEXT":{"nodeId":"2efa1c17-f6b2-4054-aaf5-5647f1e2a577","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"sOnfx72PROHSrDV8SS0pnoIF/OcJM5VtWdhk35EK4VA="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW12\r\n[22], [59] rather than discarding a large portion of samples\r\nhave proven to yield more performance improvement. For\r\nthis reason, currently, soft sampling approaches that assign\r\nweights to the examples are more popular, but which nega-\r\ntive examples are more useful needs more investigation. For  the  foreground  examples,  Cao  et  al. [30]  apply  a\r\nspecific  sampling  methodology  to  the  positives  based  on\r\nthe  IoU,  which  has  proven  useful. Note  that,  this  idea\r\nconflicts  with  the  hard  example  mining  approach  since\r\nwhile OHEM [24] offers to pick the difficult samples, prime\r\nsamples  concentrate  on  the  positive  samples  with  higher\r\nIoUs  with  the  ground  truth,  namely  the  easier  ones. For\r\nthis  reason,  currently  it  seems  that  the  usefulness  of  the\r\nexamples is different for the positives and negatives. To sum\r\nup, further investigation is required for identifying the best\r\nset of examples, or figuring out how to weigh the positive\r\nexamples during training. Moreover, sampling methods have proven to be useful\r\nfor foreground-background class imbalance, however, their\r\neffectiveness for the foreground-foreground class imbalance\r\nproblem needs to be investigated. 4.4.2  Foreground-Foreground Class Imbalance Problem\r\nOpen Issue.Foreground-foreground class imbalance has not\r\nbeen  addressed  as  thoroughly  as  foreground-background\r\nclass  imbalance. For  instance,  it  is  still  not  known  why\r\na  class  performs  worse  than  others;  a  recent  work  [65]\r\ndiscredits the correlation between the number of examples\r\nin a class and its detection performance. Moreover, despite\r\nits differences, the rich literature for addressing imbalance in\r\nimage classification has not been utilized in object detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"l20lBFKVzNtGqsaVGL7V664PTuzyoLcXP9rB5dyQh0Y="},"2efa1c17-f6b2-4054-aaf5-5647f1e2a577":{"id_":"2efa1c17-f6b2-4054-aaf5-5647f1e2a577","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jasCtpvuGzNonrjKxxGK6eRa+i7UsNUjj1yqriN1Alo="},"PREVIOUS":{"nodeId":"b30c9d7d-db86-4c5f-8dfc-399dc86a938f","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"l20lBFKVzNtGqsaVGL7V664PTuzyoLcXP9rB5dyQh0Y="},"NEXT":{"nodeId":"07cab49c-a372-42d2-9f13-175c6976d4b1","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"4P1f58DIZHoxWPipEZEvPQvmyOfrsaCz7fUIc/TC/hY="}},"text":"Explanation.One  important  finding  of  a  recent  study  [65]\r\nis that a class with the fewest examples in the dataset can\r\nyield  one  of  the  best  detection  performances  and  thus  the\r\ntotal  number  of  instances  in  the  dataset  is  not  the  only\r\nissue to balance the performance of foreground classes. Such\r\ndiscrepancies presents the necessity of an in-depth analysis\r\nto identify the root cause and investigate for better sampling\r\nmechanisms to employ while balancing a dataset. Moreover,  we  identify  a  similarity  and  a  difference  be-\r\ntween  the  class  imbalance  from  image  classification  per-\r\nspective  and  the  foreground-foreground  class  imbalance\r\nproblem. The  similarity  is  that  neither  has  a  background\r\nclass. On  the  other  hand,  the  input  BBs  are  labeled  and\r\nsampled  in  an  online  fashion  in  object  detection,  which\r\nmakes the data that the detector is trained with not static. Class imbalance problem is addressed in image classifi-\r\ncation from a larger scope by using not only classical over-\r\nsampling and under-sampling methods but also (i) transfer\r\nlearning to transfer the information from over-represented\r\nto  under-represented  classes,  (ii)  data  redundancy  meth-\r\nods  to  be  useful  for  under-sampling  the  over-represented\r\nclasses, (iii) weak supervision in order to be used in favor\r\nof under-represented class and (iv) a specific loss function\r\nfor  balancing  foreground  classes. Such  approaches  can  be\r\nadopted  for  addressing  foreground-foreground  imbalance\r\nin object detection as well. PersonParking MeterPersonParking Meter\r\n(a)\r\nPersonParking MeterPersonParking Meter\r\n(b)\r\nFig. 6:  Illustration  of  batch-level  class  imbalance.(a)An\r\nexample that is consistent with the overall dataset (person\r\nclass has more instances than parking meter).(b)An exam-\r\nple that has a different distribution from the dataset. Images\r\nare from the MS COCO dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"sOnfx72PROHSrDV8SS0pnoIF/OcJM5VtWdhk35EK4VA="},"07cab49c-a372-42d2-9f13-175c6976d4b1":{"id_":"07cab49c-a372-42d2-9f13-175c6976d4b1","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jasCtpvuGzNonrjKxxGK6eRa+i7UsNUjj1yqriN1Alo="},"PREVIOUS":{"nodeId":"2efa1c17-f6b2-4054-aaf5-5647f1e2a577","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"sOnfx72PROHSrDV8SS0pnoIF/OcJM5VtWdhk35EK4VA="},"NEXT":{"nodeId":"c5302ab8-c971-4271-95f7-87f38b9ee91f","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"umHlraoTqNPrCM3WslA7fCOldCcZU04XhZZqgWtDWPQ="}},"text":"Images\r\nare from the MS COCO dataset. 4.4.3  Foreground-Foreground Imbalance Owing to the\r\nBatch\r\nOpen  Issue.The  distribution  of  the  foreground  classes  in  a\r\nbatch might be very different than that of the overall dataset,\r\nespecially when the batch size is small. An important ques-\r\ntion  is  whether  this  may  have  an  influence  on  the  overall\r\nperformance. Explanation.A  specific  example  is  provided  in  Figure  6\r\nfrom the MS COCO dataset [90]: An over-represented class\r\n(‘person’  class  in  this  example)  across  the  dataset  may\r\nbe  under-represented  in  a  batch  or  vice  versa. Similar  to\r\nthe foreground-background class imbalance problem, over-\r\nrepresenting a class in a batch will increase the probability\r\nof the corresponding class to dominate more. Even when a batch has uniform foreground-foreground\r\nclass distribution, an imbalance may occur during sampling\r\ndue  to  the  fact  that  sampling  algorithms  either  select  a\r\nsubset or apply a weighting to the large number of boxes. If  the  sampling  mechanism  tends  to  choose  the  samples\r\nfrom specific classes in the image, balancing the dataset (or\r\nthe batch) may not be sufficient to address the foreground-\r\nforeground class imbalance entirely. 4.4.4  Ranking-Based Loss Functions\r\nOpen Issue.AP Loss [61] sorts the confidence scores pertain-\r\ning  to  all  classes  together  to  make  a  ranking  between  the\r\ndetection  boxes. However,  this  conflicts  with  the  observa-\r\ntion  that  the  optimal  confidence  scores  vary  from  class  to\r\nclass [102]. Explanation.Chen  et  al. [61]  use  all  the  confidence  scores\r\nall  together  without  paying  attention  to  the  fact  that  the\r\noptimal  threshold  per  class  may  vary  [102]. Therefore,  a\r\nmethod  that  sorts  the  confidence  scores  in  a  class-specific\r\nmanner  might  achieve  a  better  performance. Addressing\r\nthis issue is an open problem.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4P1f58DIZHoxWPipEZEvPQvmyOfrsaCz7fUIc/TC/hY="},"c5302ab8-c971-4271-95f7-87f38b9ee91f":{"id_":"c5302ab8-c971-4271-95f7-87f38b9ee91f","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jasCtpvuGzNonrjKxxGK6eRa+i7UsNUjj1yqriN1Alo="},"PREVIOUS":{"nodeId":"07cab49c-a372-42d2-9f13-175c6976d4b1","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"4P1f58DIZHoxWPipEZEvPQvmyOfrsaCz7fUIc/TC/hY="}},"text":"Addressing\r\nthis issue is an open problem. 5    IMBALANCE2: SCALEIMBALANCE\r\nWe  discuss  scale  the  imbalance  problem  in  two  parts:  the\r\nfirst  part,  Object/Box-Level  Scale  Imbalance,  analyses  the\r\nproblems  arising  from  the  imbalanced  distribution  of  the","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"umHlraoTqNPrCM3WslA7fCOldCcZU04XhZZqgWtDWPQ="},"df430181-da58-4f5a-9f88-5a9041f45511":{"id_":"df430181-da58-4f5a-9f88-5a9041f45511","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"JEQlFp8lk7u+JAIkerZ9lyHwXqlQZJ2uqqXyRZR3EGg="},"NEXT":{"nodeId":"c220c58c-e954-4e4e-ad24-1df8dfcac9ae","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Zsh5Pqiv5mxJGOcIQv4MsocvpJw2gorkz4si/6CaSQI="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW13\r\n0.00.20.40.60.81.0Normalized Width\r\n103\r\n104\r\n105\r\n106\r\nFrequency\r\nNormalized Width Distribution\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(a)\r\n0.00.20.40.60.81.0Normalized Height\r\n103\r\n104\r\n105\r\n106\r\nFrequency\r\nNormalized Height Distribution\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(b)\r\n0.00.20.40.60.81.0Normalized Area\r\n103\r\n104\r\n105\r\n106\r\n107\r\nFrequency\r\nNormalized Area Distribution\r\nObjects365Open Images\r\nCOCOPASCAL-VOC\r\n(c)\r\nFig. 7: Imbalance in scales of the objects in common datasets: the distributions of BB width(a), height(b), and area(c). Values are with respect to the normalized image (i.e. relative to the image). For readability, theyaxes are in log-scale. C2\r\nC3\r\nC4\r\nC5Predict\r\nC2\r\nC3\r\nC4\r\nC5PredictPredict\r\nPredict\r\nPredict\r\nC2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5PredictPredict\r\nPredict\r\nPredict\r\nI1\r\nI2\r\nI3\r\nI4PredictPredict\r\nPredict\r\nPredict\r\n(a)(b)(c)\r\n(d)(e)\r\nC2\r\nC3\r\nC4\r\nC5PredictPredict\r\nPredict\r\nPredict\r\nLight NetBackbone\r\nFig. 8:  An  illustration  and  comparison  of  the  solutions\r\nfor  scale  imbalance. “Predict”  refers  to  the  prediction  per-\r\nformed  by  a  detection  network. The  layered  boxes  cor-\r\nrespond  to  convolutional  layers.(a)No  scale  balancing\r\nmethod  is  employed.(b)Prediction  is  performed  from\r\nbackbone  features  at  different  levels  (i.e. scales)  (e.g. SSD\r\n[19]).(c)The intermediate features from different scales are\r\ncombined before making prediction at multiple scales (e.g. Feature  Pyramid  Networks  [26]).(d)The  input  image  is\r\nscaled  first  and  then  processed.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"xMLVfy+qRlvNRKe8CDb3i3gbc9qsV1TS/K5xgdEOXVc="},"c220c58c-e954-4e4e-ad24-1df8dfcac9ae":{"id_":"c220c58c-e954-4e4e-ad24-1df8dfcac9ae","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"JEQlFp8lk7u+JAIkerZ9lyHwXqlQZJ2uqqXyRZR3EGg="},"PREVIOUS":{"nodeId":"df430181-da58-4f5a-9f88-5a9041f45511","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"xMLVfy+qRlvNRKe8CDb3i3gbc9qsV1TS/K5xgdEOXVc="},"NEXT":{"nodeId":"60dee03a-7c90-4667-900d-60e7fb4fbc98","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"mWBeqLE9t/RZaMGA9qD9gcfdVJWMq0QenEE8zDcxOQA="}},"text":"Each  I  corresponds  to  an\r\nimage  pyramidal  feature  (e.g. Image  Pyramids  [27]).(e)\r\nImage  and  feature  pyramids  are  combined. Rather  than\r\napplying  backbones,  light  networks  are  used  in  order  to\r\nextract features from smaller images. scales of the objects and input bounding boxes. The second\r\npart,  Feature  Imbalance,  analyses  the  problems  arising  at\r\nthe feature extraction level and concerns the methods using\r\npyramidal features. 5.1    Object/Box-Level Scale Imbalance\r\nDefinition.Scale imbalance occurs when certain sizes of the\r\nobjects or input bounding boxes are over-represented in the\r\ndataset. It has been shown that this affects the scales of the\r\nestimated RoIs and the overall detection performance [27]. Figure  7  presents  the  relative  width,  height  and  the  area\r\nof  the  objects  in  the  MS-COCO  dataset  [90];  we  observe  a\r\nskewness in the distributions in favor of smaller objects. Many  of  the  deep  object  detectors  rely  on  a  backbone\r\nconvolutional  neural  network  (e.g. [93],  [101],  [103],  [104],\r\n[105]),  pretrained  on  an  image  classification  task,  in  order\r\nto  extract  visual  features  from  the  input  image. Li  et  al. [106] discuss the cons of employing such networks designed\r\nfor the classification task and propose a backbone specially\r\ndesigned  for  the  object  detection  task  where  they  limit\r\nthe  spatial  downsampling  rate  for  the  high  level  features. Overall, these networks, also called the backbones, play an\r\nimportant role for the performance of the object detectors,\r\nbut they alone are insufficient for handling the scale diver-\r\nsity of the input bounding boxes. Solutions.First  examples  of  deep  object  detectors  made\r\npredictions from the final layer of the backbone network (see\r\n[16], [17] and Figure 8(a)), and therefore, neglected the scale-\r\ndiversity of BBs.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Zsh5Pqiv5mxJGOcIQv4MsocvpJw2gorkz4si/6CaSQI="},"60dee03a-7c90-4667-900d-60e7fb4fbc98":{"id_":"60dee03a-7c90-4667-900d-60e7fb4fbc98","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"JEQlFp8lk7u+JAIkerZ9lyHwXqlQZJ2uqqXyRZR3EGg="},"PREVIOUS":{"nodeId":"c220c58c-e954-4e4e-ad24-1df8dfcac9ae","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Zsh5Pqiv5mxJGOcIQv4MsocvpJw2gorkz4si/6CaSQI="}},"text":"The solutions to addressing scale imbalance\r\ncan  be  grouped  into  four  (Figure  8):  methods  predicting\r\nfrom  the  hierarchy  of  the  backbone  features  (Figure  8(b)),\r\nmethods based on feature pyramids (Figure 8(c)), methods\r\nbased on image pyramids (Figure 8(d)) and finally methods\r\ncombining image and feature pyramids (Figure 8(e)). 5.1.1  Methods Predicting from the Feature Hierarchy of\r\nBackbone Features\r\nThese  methods  make  independent  predictions  from  the\r\nfeatures at different levels of the backbone network (Figure\r\n8(b)). This approach naturally considers object detection at\r\nmultiple scales since the different levels encode information\r\nat different scales; e.g., if the input contains a small object,\r\nthen  earlier  levels  already  contain  strong  indicators  about\r\nthe small object [69]. An  illustratory  example  for  one-stage  detectors  is  the\r\nSingle  Shot  Detector  (SSD)  [19],  which  makes  predictions\r\nfrom features at different layers. Two-stage   detectors   can   exploit   features   at   different\r\nscales while either estimating the regions (in the first stage)\r\nor  extracting  features  from  these  regions  (for  the  second\r\nstage). For  example,  theMulti  Scale  CNN(MSCNN)  [70]\r\nuses  different  layers  of  the  backbone  network  while  esti-\r\nmating  the  regions  in  the  first  stage  whereas  Yang  et  al. [69] choose an appropriate layer to pool based on the scale\r\nof  the  estimated  RoI;  calledScale  Dependent  Pooling(SDP),\r\nthe method, e.g., pools features from an earlier layer if the\r\nheight of the RoI is small. Alternatively, theScale Aware Fast\r\nR-CNN[71] learns an ensemble of two classifiers, one for the\r\nsmall scale and one for the large scale objects, and combines\r\ntheir predictions.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"mWBeqLE9t/RZaMGA9qD9gcfdVJWMq0QenEE8zDcxOQA="},"13239a32-a9cf-403a-803f-1ec093b5255b":{"id_":"13239a32-a9cf-403a-803f-1ec093b5255b","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XAwC4UlFAu0VDgJ7yfIKe1JGilQZoDXIxnWIZu/n+so="},"NEXT":{"nodeId":"31e2f92b-0c9f-4860-8553-d532207a694b","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"8K1/AwXS9ySpW9D8+9Av4In+9kmfY0bGwq3pxoiEPqs="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW14\r\n5.1.2  Methods Based on Feature Pyramids\r\nMethods  based  on  feature  hierarchies  use  features  from\r\ndifferent levels independently without integrating low-level\r\nand high-level features. However, the abstractness (semantic\r\ncontent)  of  information  varies  among  different  layers,  and\r\nthus  it  is  not  reliable  to  make  predictions  directly  from  a\r\nsingle  layer  (especially  the  lower  layers)  of  the  backbone\r\nnetwork. To address this issue, theFeature Pyramid Networks(FPN)\r\n[26] combine the features at different scales before making\r\npredictions. FPN exploits an additional top-down pathway\r\nalong which the features from the higher level are supported\r\nby the features from a lower level using lateral connections\r\nin  order  to  have  a  balanced  mixed  of  these  features  (see\r\nFigure 8(c)). The top-down pathway involves upsampling to\r\nensure the sizes to be compatible and lateral connections are\r\nbasically1\u00021convolutions. Similar to feature hierarchies,\r\na  RoI  pooling  step  takes  the  scale  of  the  RoI  into  account\r\nto  choose  which  level  to  pool  from. These  improvements\r\nallow the predictor network to be applied at all levels which\r\nimproves the performance especially for small and medium\r\nsized objects. Although  FPN  was  originally  proposed  for  object  de-\r\ntection,  it  quickly  became  popular  and  has  been  used  for\r\ndifferent (but related) tasks such as shadow detection [107],\r\ninstance segmentation [108], [109] and panoptic segmenta-\r\ntion [110]. Despite its benefits, FPN is known to suffer from a major\r\nshortcoming due to the straightforward combination of the\r\nfeatures gathered from the backbone network – thefeature\r\nimbalance problem. We discuss this problem in Section 5.2. 5.1.3  Methods Based on Image Pyramids\r\nThe idea of using multi-scale image pyramids, presented in\r\nFigure 8(d), for the image processing tasks goes back to the\r\nearly work by Adelson et al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VeN7er9SnV6LDDKlpSRSpK+k8Zn67QxBaycCeNMqnpA="},"31e2f92b-0c9f-4860-8553-d532207a694b":{"id_":"31e2f92b-0c9f-4860-8553-d532207a694b","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XAwC4UlFAu0VDgJ7yfIKe1JGilQZoDXIxnWIZu/n+so="},"PREVIOUS":{"nodeId":"13239a32-a9cf-403a-803f-1ec093b5255b","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VeN7er9SnV6LDDKlpSRSpK+k8Zn67QxBaycCeNMqnpA="},"NEXT":{"nodeId":"9b5bcaab-e7c2-40c7-9754-578c9f44d2cb","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ynGXs4B24ghH8laUYn0dqupQCPvJXvBfTmhmvsU3tIQ="}},"text":"[111] and was popular before\r\ndeep  learning. In  deep  learning,  these  methods  are  not\r\nutilized as much due to their relatively high computational\r\nand memory costs. However, recently, Singh and Davis [27]\r\npresented a detailed analysis on the effect of the scale im-\r\nbalance problem with important conclusions and proposed\r\na  method  to  alleviate  the  memory  constraint  for  image\r\npyramids. They investigated the conventional approach of\r\ntraining object detectors in smaller scales but testing them\r\nin  larger  scales  due  to  memory  constraints,  and  showed\r\nthat this inconsistency between test and training time scales\r\nhas  an  impact  on  the  performance. In  their  controlled  ex-\r\nperiments, upsampling the image by two performed better\r\nthan  reducing  the  stride  by  two. Upon  their  analysis,  the\r\nauthors proposed a novel training method coined asSNIP\r\nbased  on  image  pyramids  rather  than  feature  pyramids. They  argue  that,  while  training  scale-specific  detectors  by\r\nproviding  the  input  to  the  appropriate  detector  will  lose\r\na  significant  portion  of  the  data,  and  that  using  multi-\r\nscale  training  on  a  single  detector  will  increase  the  scale\r\nimbalance by preserving the variation in the data. Therefore,\r\nSNIP trains multiple proposal and detector networks with\r\nimages at different sizes, however, for each network only the\r\nappropriate input bounding box scales are marked as valid,\r\nby  which  it  ensures  multi-scale  training  without  any  loss\r\nin  the  data. Another  challenge,  the  limitation  of  the  GPU\r\nmemory, is overcome by an image cropping approach. The\r\nimage cropping approach is made more efficient in a follow-\r\nup method, calledSNIPER[28]. 5.1.4  Methods Combining Image and Feature Pyramids\r\nImage  pyramid  based  methods  are  generally  less  efficient\r\nthan feature pyramid based methods in terms of computa-\r\ntional  time  and  memory.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"8K1/AwXS9ySpW9D8+9Av4In+9kmfY0bGwq3pxoiEPqs="},"9b5bcaab-e7c2-40c7-9754-578c9f44d2cb":{"id_":"9b5bcaab-e7c2-40c7-9754-578c9f44d2cb","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XAwC4UlFAu0VDgJ7yfIKe1JGilQZoDXIxnWIZu/n+so="},"PREVIOUS":{"nodeId":"31e2f92b-0c9f-4860-8553-d532207a694b","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"8K1/AwXS9ySpW9D8+9Av4In+9kmfY0bGwq3pxoiEPqs="},"NEXT":{"nodeId":"a0e3acc5-d2e3-46b6-ad44-4545ca76e473","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"y4Kyh4/14+BVwbFo3RkWrbj9SH4Nso9hf6y4VSXjI8k="}},"text":"However,  image  pyramid  based\r\nmethods are expected to perform better, since feature pyra-\r\nmid  based  methods  are  efficient  approximations  of  such\r\nmethods. Therefore,  to  benefit  from  advantages  of  both\r\napproaches, they can be combined in a single model. Even though there are different architectures, the generic\r\napproach is illustrated in Figure 8(e). For example,Efficient\r\nFeaturized Image Pyramids[72] uses five images at different\r\nscales, four of which are provided to the light-weight featur-\r\nized image pyramid network module (i.e. instead of addi-\r\ntional backbone networks) and the original input is fed into\r\nthe  backbone  network. This  light-weight  network  consists\r\nof4consecutive convolutional layers designed specifically\r\nfor each input. The features gathered from this module are\r\nintegrated to the backbone network features at appropriate\r\nlevels according to their sizes, such that the image features\r\nare combined with the features extracted from the backbone\r\nnetwork using feature attention modules. Furthermore, after\r\nattention modules, the gathered features are integrated with\r\nthe higher levels by means of forward fusion modules before\r\nthe final predictions are obtained. A  similar  method  that  is  also  built  on  SSD  and  uses\r\ndownsampled images isEnriched Feature Guided Refinement\r\nNetwork[58]  in  which  a  single  downsampled  image  is\r\nprovided  as  an  input  to  the  Multi-Scale  Contextual  Fea-\r\ntures  (MSCF)  Module. The  MSCF  consists  of  two  consec-\r\nutive convolutional layers followed by three parallel dilated\r\nconvolutions,  which  is  also  similar  to  the  idea  in  Trident\r\nNetworks [74]. The outputs of the dilated convolutions are\r\nagain combined using1\u00021convolutions. The authors set the\r\ndownsampled image size to meet the first prediction layer\r\nin the regular SSD architecture (e.g. for320\u0002320input, the\r\nsize of the downsampled image is40\u000240). While Pang et al. [72] only provide experiments with the SSD backbone, Nie\r\net al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ynGXs4B24ghH8laUYn0dqupQCPvJXvBfTmhmvsU3tIQ="},"a0e3acc5-d2e3-46b6-ad44-4545ca76e473":{"id_":"a0e3acc5-d2e3-46b6-ad44-4545ca76e473","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XAwC4UlFAu0VDgJ7yfIKe1JGilQZoDXIxnWIZu/n+so="},"PREVIOUS":{"nodeId":"9b5bcaab-e7c2-40c7-9754-578c9f44d2cb","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ynGXs4B24ghH8laUYn0dqupQCPvJXvBfTmhmvsU3tIQ="}},"text":"[72] only provide experiments with the SSD backbone, Nie\r\net al. [58] show that their modules can also be used in the\r\nResNet backbone. An alternative approach is to generate super-resolution\r\nfeature  maps. Noh  et  al. [73]  proposedsuper-resolution  for\r\nsmall object detectionfor two-stage object detectors which lack\r\nstrong representations of small RoIs after RoI standardiza-\r\ntion layers. Their architecture adds four additional modules\r\nto the baseline detector: (i) Given the original image, target\r\nextractor outputs the targets for the discriminator by using\r\ndilated  convolutions. This  network  also  shares  parameters\r\nwith the backbone network. (ii) Given the features obtained\r\nfrom the backbone network using the original image and a\r\n0:5x downsampled image, the generator network generates\r\nsuper  resolution  feature  maps  for  small  RoIs. (iii)  Given\r\nthe outputs of (i) and (ii), a discriminator is trained in the\r\nconventional GAN setting. (iv) Finally, if the RoI is a small\r\nobject according to a threshold, then the prediction is carried\r\nout by the small predictor, or else by the large predictor.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"y4Kyh4/14+BVwbFo3RkWrbj9SH4Nso9hf6y4VSXjI8k="},"d202cb39-fae2-43ff-bfc7-9fb1518d3012":{"id_":"d202cb39-fae2-43ff-bfc7-9fb1518d3012","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iEeF3g9hK9LJbkHWlHwXg5B5e9k9LA8T07b703PQai0="},"NEXT":{"nodeId":"5f6210b3-57c2-4f82-9ed6-a374a9c3bed0","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gInDFYNCbt1YLC/H+rlP2aSqdNuYPnpVGV5Pi0XXMuE="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW15\r\nC2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5\r\nHigh Level\r\nLow Level\r\nLow to HighLevel Features\r\nFig. 9:  Feature-Level  imbalance  is  illustrated  on  the  FPN\r\narchitecture. Another  approach,Scale  Aware  Trident  Networks[74],\r\ncombines  the  advantages  of  the  methods  based  on  fea-\r\nture pyramids and image pyramids without using multiple\r\ndownsampled  images  but  employing  only  dilated  convo-\r\nlutions. The  authors  use  dilated  convolutions  [112]  with\r\ndilation  rates1;2and3in  parallel  branches  in  order  to\r\ngenerate  scale-specific  feature  maps,  making  the  approach\r\nmore accurate compared to feature pyramid based methods. In  order  to  ensure  that  each  branch  is  specialized  for  a\r\nspecific  scale,  an  input  bounding  box  is  provided  to  the\r\nappropriate branch according to its size. Their analysis on\r\nthe effect of receptive field size on objects of different scales\r\nshows  that  larger  dilation  rates  are  more  appropriate  for\r\nobjects with larger scales. In addition, since using multiple\r\nbranches  is  expected  to  degrade  the  efficiency  due  to  the\r\nincreasing  number  of  operations,  they  proposed a  method\r\nfor approximating these branches with a single parameter-\r\nsharing  branch,  with  minimal  (insignificant)  performance\r\nloss. 5.2    Feature-level Imbalance\r\nDefinition.The integration of the features from the backbone\r\nnetwork  is  expected  to  be  balanced  in  terms  of  low-  and\r\nhigh-level features so that consistent predictions can follow.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"OyBIKQbp3geFzJwYg1Yvdn71hLR0owyDWerX4woCft4="},"5f6210b3-57c2-4f82-9ed6-a374a9c3bed0":{"id_":"5f6210b3-57c2-4f82-9ed6-a374a9c3bed0","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iEeF3g9hK9LJbkHWlHwXg5B5e9k9LA8T07b703PQai0="},"PREVIOUS":{"nodeId":"d202cb39-fae2-43ff-bfc7-9fb1518d3012","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"OyBIKQbp3geFzJwYg1Yvdn71hLR0owyDWerX4woCft4="},"NEXT":{"nodeId":"162f47c6-ef8d-4478-bc1c-6cf98d1e1cd1","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"C4NxIDknRDtIy6GE97YzfrfMyGfuqD/5z9YvFLmUjHQ="}},"text":"To  be  more  specific,  if  we  analyze  the  conventional  FPN\r\narchitecture  in  Figure  9,  we  notice  that,  while  there  are\r\nseveral layers from theC2layer of the bottom-up pass with\r\nlow-level  features  to  theP5layer  of  the  feature  pyramid,\r\ntheC2layer  is  directly  integrated  to  theP2layer,  which\r\nimplies the effect of the high-level and low-level features in\r\ntheP2andP5layers to be different. Solutions.There  are  several  methods  to  address  imbalance\r\nin  the  FPN  architectures,  which  range  from  designing  im-\r\nproved  top-down  pathway  connections  [56],  [57]  to  com-\r\npletely novel architectures. Here, we consider the methods\r\nto alleviate the feature-level imbalance problem using novel\r\narchitectures, which we group into two according to what\r\nthey use as a basis, pyramidal or backbone features. 5.2.1  Methods Using Pyramidal Features as a Basis\r\nThese methods aim to improve the pyramidal features gath-\r\nered by FPN using additional operations or steps – see an\r\noverview of these methods in Figure 10(a,b). Path  Aggregation  Network(PANet)  [75]  is  the  first  to\r\nshow that the features gathered by an FPN can be further\r\nenhanced  and  an  RoI  can  be  mapped  to  each  layer  of\r\nthe  pyramid  rather  than  associating  it  with  a  single  one. The authors suggest that low-level features, such as edges,\r\ncorners, are useful for localizing objects, however, the FPN\r\narchitecture does not sufficiently make use of these features.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gInDFYNCbt1YLC/H+rlP2aSqdNuYPnpVGV5Pi0XXMuE="},"162f47c6-ef8d-4478-bc1c-6cf98d1e1cd1":{"id_":"162f47c6-ef8d-4478-bc1c-6cf98d1e1cd1","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iEeF3g9hK9LJbkHWlHwXg5B5e9k9LA8T07b703PQai0="},"PREVIOUS":{"nodeId":"5f6210b3-57c2-4f82-9ed6-a374a9c3bed0","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gInDFYNCbt1YLC/H+rlP2aSqdNuYPnpVGV5Pi0XXMuE="},"NEXT":{"nodeId":"5c1a7aa5-09c0-4a5f-b676-0d4f2ef38551","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"KRt/5qxxuNJiainnmftYMeU+GAMYrmEla51+K6Y1HU8="}},"text":"Motivated from this observation, PANet, depicted in Figure\r\n10(a), improves the FPN architecture with two new contri-\r\nbutions:\r\n1)Bottom-up   path   augmentationextends   the   feature\r\npyramid in order to allow the low-level features to\r\narrive  at  the  layers  where  the  predictions  occur  in\r\nshorter steps (see red arrows in Figure 10(a) within\r\nFPN and to the final pyramidal features to see the\r\nshortcut). For  this  reason,  in  a  way  a  shortcut  is\r\ncreated for the features in the initial layers. This is\r\nimportant since these features have rich information\r\nabout localization thanks to edges or instance parts. 2)     While  in  the  FPN,  each  RoI  is  associated  with  a\r\nsingle level of feature based on its size, PANet asso-\r\nciates each RoI to every level, applies RoI Pooling,\r\nfuses using element-wise max or sum operation and\r\nthe resulting fixed-sized feature grid is propagated\r\nto the detector network. This process is calledAdap-\r\ntive Feature Pooling. Despite  these  contributions,  PANet  still  uses  a  sequential\r\npathway to extract the features. Different  from  the  sequential  enhancement  pathway  of\r\nPANet,Libra  FPN[29]  aims  to  learn  the  residual  features\r\nby using all of the features from all FPN layers at once (see\r\nFigure 10(b)). Residual feature layer computation is handled\r\nin two steps:\r\n1)Integrate:All feature maps from different layers are\r\nreduced to one single feature map by rescaling and\r\naveraging. For this reason, this step does not have\r\nany learnable parameter. 2)Refine:The  integrated  feature  map  is  refined  by\r\nmeans  of  convolution  layers  or  non-local  neural\r\nnetworks [113]. Finally  the  refined  features  are  added  to  each  layer  of  the\r\npyramidal  features. The  authors  argue  that  in  addition  to\r\nFPN,  their  method  is  complementary  to  other  methods\r\nbased on pyramidal features as well, such as PANet [75].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"C4NxIDknRDtIy6GE97YzfrfMyGfuqD/5z9YvFLmUjHQ="},"5c1a7aa5-09c0-4a5f-b676-0d4f2ef38551":{"id_":"5c1a7aa5-09c0-4a5f-b676-0d4f2ef38551","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iEeF3g9hK9LJbkHWlHwXg5B5e9k9LA8T07b703PQai0="},"PREVIOUS":{"nodeId":"162f47c6-ef8d-4478-bc1c-6cf98d1e1cd1","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"C4NxIDknRDtIy6GE97YzfrfMyGfuqD/5z9YvFLmUjHQ="}},"text":"5.2.2  Methods Using Backbone Features as a Basis\r\nThese methods build their architecture on the backbone fea-\r\ntures and ignore the top-down pathway of FPN by employ-\r\ning  different  feature  integration  mechanisms,  as  displayed\r\nin Figure 10(c-h). Scale-Transferrable  Detection  Network(STDN)  [76]  gen-\r\nerates  the  pyramidal  features  from  the  last  layer  of  the\r\nbackbone features which are extracted using DenseNet [114]\r\nblocks (Figure 10(c)). In a DenseNet block, all the lower level\r\nfeatures are propagated to every next layer within a block. In  Figure  10(c),  the  number  of  DenseNet  (dense)  blocks  is\r\nfour and theith block is denoted byDi. Motivated by the\r\nidea that direct propagation of lower-level layers to the sub-\r\nsequent  layers  also  carries  lower-level  information,  STDN\r\nbuilds pyramidal features consisting of six layers by using\r\nthe last block of DenseNet. In order to map these layers to\r\nlower sizes, the approach uses mean pooling with different\r\nreceptive field sizes. For the fourth feature map, an identity\r\nmapping is used. For the last two layers which the feature","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KRt/5qxxuNJiainnmftYMeU+GAMYrmEla51+K6Y1HU8="},"39cd85f0-99e3-4586-bae7-216ffb133ec7":{"id_":"39cd85f0-99e3-4586-bae7-216ffb133ec7","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"NfXPpuTG7I/RipfMNbF5zn4zqsIGzBDttyiFN052lio="},"NEXT":{"nodeId":"4c5fe99f-2107-432a-96ac-d49aa18562c1","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XDMT1VFCxxWNwMLsS0/Cp6qgiM0Iymwd4uOTbzoQT14="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW16\r\n(a) Path Aggregation Network\r\n3x3 Conv. Downsampling, Addition\r\nFPN\r\nC2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5\r\nN2\r\nN3\r\nN4\r\nN5\r\n(b) Libra FPN\r\nIntegrate: Rescale, AverageRefineFPN\r\nC2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5\r\nN2\r\nN3\r\nN4\r\nN5\r\nD1\r\nD2\r\nD3\r\nD4\r\nP3\r\nP4P5\r\nP6\r\nP1P2\r\nMean PoolingIdentity\r\nScale Transfer Layer\r\nDenseNet\r\n(c) Scale-Transferrable Detection Network\r\nXAdaptive Sampling\r\nGlobal Attention\r\nLocal Reconf.C2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5\r\n(d) Parallel FPN\r\n1x1 Conv\r\nP4Multi \r\nScale Context \r\nAggreg. Spatial Pyramid  Pooling\r\nP2\r\nP3\r\nC2\r\nC3\r\nC4\r\nC5\r\n(e) Deep Feature Pyramid Reconfiguration(f) Zoom Out-and-In Network\r\nP2\r\nP3\r\nP4\r\nP5\r\nFFMv1: 1x1 conv,  UpsampleTUM\r\nFFMv2:  1x1 conv\r\nScale-wise Feature \r\nAggregation\r\nC2\r\nC3\r\nC4\r\nC5\r\n(g) Multi-Level FPN\r\nIdentified Connections With \r\nNeural Architecture \r\nSearch Using Merging Cells\r\nC2\r\nC3\r\nC4\r\nC5\r\nP2\r\nP3\r\nP4\r\nP5\r\n(h) NAS-FPN\r\nC2\r\nC3\r\nC4\r\nC5\r\nMap Attention Decision\r\nB3\r\nB4\r\nB5\r\nP5\r\nB3\r\nC5\r\nB4C4\r\nC3P31P32\r\nP41P4\r\n2\r\nDeconvolution\r\nTUM\r\nBottom-Up Path Augmentation\r\nFig. 10:  High-level  diagrams  of  the  methods  designed  for  feature-level  imbalance.(a)Path  Aggregation  Network.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"eE6BFXg1DbQ/GPMv61Jhysk+TguL1XljWewqs7j1u3w="},"4c5fe99f-2107-432a-96ac-d49aa18562c1":{"id_":"4c5fe99f-2107-432a-96ac-d49aa18562c1","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"NfXPpuTG7I/RipfMNbF5zn4zqsIGzBDttyiFN052lio="},"PREVIOUS":{"nodeId":"39cd85f0-99e3-4586-bae7-216ffb133ec7","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"eE6BFXg1DbQ/GPMv61Jhysk+TguL1XljWewqs7j1u3w="}},"text":"FPN  is\r\naugmented by  an additional bottom-up  pathway to facilitate  a shortcut of  the low-level  features  to the  final pyramidal\r\nfeature  maps. Red  arrows  represent  the  shortcuts.(b)Libra  FPN. FPN  pyramidal  features  are  integrated  and  refined  to\r\nlearn a residual feature map. We illustrate the process originating fromP2feature map. Remaining feature maps (P3-P5)\r\nfollow the same operations.(c)Scale Transferrable Detection Network. Pyramidal features are learned via pooling, identity\r\nmapping and scale transfer layers depending on the layer size.(d)Parallel FPN. Feature maps with difference scales are\r\nfollowed by spatial pyramid pooling. These feature maps are fed into the multi-scale context aggregation (MSCA) module. We show the input and outputs of MSCA module forP3by red arrows.(e)Deep Feature Pyramid Reconfiguration. A set of\r\nresidual features are learned via global attention and local reconfiguration modules. We illustrate the process originating\r\nfromP2feature map. Remaining feature maps (P3-P5) follow the same operations.(f)Zoom Out-And-In Network. A zoom-\r\nin phase based on deconvolution (shown with red arrows) is adopted before stacking the layers of zoom-out and zoom-in\r\nphases. The  weighting  between  them  is  determined  by  map  attention  decision  module.(g)Multi-Level  FPN. Backbone\r\nfeatures  from  two  different  levels  are  fed  into  Thinned  U-Shape  Module  (TUM)  recursively  to  generate  a  sequence  of\r\npyramidal  features,  which  are  finally  combined  into  one  by  acale-wise  feature  aggregation  module.(h)NAS-FPN. The\r\nlayers between backbone features and pyramidal features are learned via Neural Architecture Search.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"XDMT1VFCxxWNwMLsS0/Cp6qgiM0Iymwd4uOTbzoQT14="},"1a7e350e-cdd8-4f1e-9f63-5f65132d624d":{"id_":"1a7e350e-cdd8-4f1e-9f63-5f65132d624d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"OLuD9SHVQE1QLkBei/j6+23uF8niabY+sB5ZZI/4+eQ="},"NEXT":{"nodeId":"7fea994f-a20b-42c4-bb02-f2e14223c5e3","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"H2WIpWC4XGzX4ZiNAhG363Q20MBbHhRZN4yBwsT5flo="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW17\r\nmaps of DenseNet are to be mapped to higher dimensions,\r\nthe  authors  propose  ascale  transfer  layerapproach. This\r\nlayer  does  not  have  any  learnable  parameter  and  givenr,\r\nthe  desired  enlargement  for  a  feature  map,  the  width  and\r\nheight of the feature map are enlarged byrby decreasing\r\nthe  total  number  of  feature  maps  (a.k.a. channels). STDN\r\nincorporates  high-  and  low-level  features  with  the  help\r\nof  DenseNet  blocks  and  is  not  easily  adaptable  to  other\r\nbackbone  networks. In  addition,  no  method  is  adopted  to\r\nbalance the low- and high-level features within the last block\r\nof DenseNet. Similar  to  STDN,Parallel  FPN[77]  also  employs  only\r\nthe last layer of the backbone network and generates multi-\r\nscale  features  by  exploiting  the  spatial  pyramid  pooling\r\n(SPP) [115] – Figure 10(d). Differently, it increases the width\r\nof  the  network  by  pooling  the  lastDfeature  maps  of  the\r\nbackbone network multiple times with different sizes, such\r\nthat feature maps with different scales are obtained. Figure\r\n10(e) shows the case when it is pooled for three times and\r\nD=  2. The  number  of  feature  maps  is  decreased  to1by\r\nemploying1\u00021convolutions. These feature maps are then\r\nfed  into  themulti-scale  context  aggregation  (MSCA)  module,\r\nwhich  integrates  context  information  from  other  scales  for\r\na  corresponding  layer. For  this  reason,  MSCA,  operating\r\non a scale-based manner, has the following inputs: Spatial\r\npyramid pooledDfeature maps and reduced feature maps\r\nfrom  other  scales. We  illustrate  the  inputs  and  outputs  to\r\nthe MSCA module for the middle scale feature map by red\r\narrows in Figure 10(e).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"aTGn6yWB62qUXnVF+3F5Fmua3v55SbnnmUfoEuLqwBY="},"7fea994f-a20b-42c4-bb02-f2e14223c5e3":{"id_":"7fea994f-a20b-42c4-bb02-f2e14223c5e3","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"OLuD9SHVQE1QLkBei/j6+23uF8niabY+sB5ZZI/4+eQ="},"PREVIOUS":{"nodeId":"1a7e350e-cdd8-4f1e-9f63-5f65132d624d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"aTGn6yWB62qUXnVF+3F5Fmua3v55SbnnmUfoEuLqwBY="},"NEXT":{"nodeId":"ae9839be-4fd6-43d9-8df6-292e926d6287","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"xwW6Bff+PW1Yl1nU/nImdTuJ7Dn7eLL4PwpT1dzWkwg="}},"text":"MSCA first ensures the sizes of each\r\nfeature map to be equal and applies3\u00023convolutions. While  previous  methods  based  on  backbone  features\r\nonly  use  the  last  layer  of  the  backbone  network,Deep\r\nFeature Pyramid Reconfiguration[78] combines features from\r\ndifferent levels of backbone features into a single tensor (X\r\nin  Figure  10(e))  and  then  learns  a  set  of  residual  features\r\nfrom  this  tensor. A  sequence  of  two  modules  are  applied\r\nto  the  tensorXin  order  to  learn  a  residual  feature  map\r\nto be added to each layer of the backbone network. These\r\nmodules are,\r\n1)Global  Attention  Moduleaims  to  learn  the  inter-\r\ndependencies among different feature maps for ten-\r\nsorX. The  authors  adopt  Squeeze  and  Excitation\r\nBlocks [116] in which the information is squeezed to\r\nlower dimensional features for each feature map ini-\r\ntially (i.e. squeeze step), and then a weight is learned\r\nfor  each  feature  map  based  on  learnable  functions\r\nincluding non-linearity (i.e. excitation step). 2)Local Configuration Moduleaims to improve the fea-\r\ntures  after  global  attention  module  by  employing\r\nconvolutional  layers. The  output  of  this  module\r\npresents  the  residual  features  to  be  added  for  a\r\nfeature layer from the backbone network. Similarly,Zoom  Out-and-In  Network[79]  also  combines\r\nlow- and high-level features of the backbone network. Ad-\r\nditionally, it includes deconvolution-based zoom-in phase in\r\nwhich intermediate step pyramidal features, denoted byBis\r\nin Figure 10(f), are learned. Note that, unlike FPN [26], there\r\nis  no  lateral  connection  to  the  backbone  network  during\r\nthe zoom-in phase, which is basically a sequence of decon-\r\nvolutional  layers  (see  red  arrows  for  the  zoom-in  phase).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"H2WIpWC4XGzX4ZiNAhG363Q20MBbHhRZN4yBwsT5flo="},"ae9839be-4fd6-43d9-8df6-292e926d6287":{"id_":"ae9839be-4fd6-43d9-8df6-292e926d6287","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"OLuD9SHVQE1QLkBei/j6+23uF8niabY+sB5ZZI/4+eQ="},"PREVIOUS":{"nodeId":"7fea994f-a20b-42c4-bb02-f2e14223c5e3","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"H2WIpWC4XGzX4ZiNAhG363Q20MBbHhRZN4yBwsT5flo="},"NEXT":{"nodeId":"d0ce9cf8-9617-4d05-861f-0d6567ef6cda","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"d0SWBxseD9EHCJg4GFhsXK4jvcna/HIBmK60y5iCqG0="}},"text":"Integration of the high- and low-level features are achieved\r\nby stacking the same-size feature maps from zoom-out and\r\nzoom-in  phases  after  zoom-in  phase  (i.e.B3andC3). On\r\nthe  other  hand,  these  concatenated  feature  maps  are  to  be\r\nbalanced  especially  for  theB3\u0000C3andB4\u0000C4blocks\r\nsinceB3andC3(orB4andC4)  are  very  far  from  each\r\nother  in  the  feature  hierarchy,  which  makes  them  have\r\ndifferent representations of the data. In order to achieve this,\r\nthe  proposedmap  attention  decision  modulelearns  a  weight\r\ndistribution on the layers. Note that the idea is similar to the\r\nsqueeze  and  excitation  modules  [116]  employed  by  Kong\r\net  al. [78],  however,  it  is  shown  by  the  authors  that  their\r\ndesign performs better for their architecture. One drawback\r\nof  the  method  is  that  it  is  built  upon  Inception  v2  (a.k.a. Inception  BN)  [117]  and  corresponding  inception  modules\r\nare exploited throughout the method, which may make the\r\nmethod difficult to adopt for other backbone networks. Different from Kong et al. [78] and Li et al. [79],Multi-\r\nLevel FPN[80] stacks one highest and one lower level feature\r\nlayers and recursively outputs a set of pyramidal features,\r\nwhich are all finally combined into a single feature pyramid\r\nin  a  scale-wise  manner  (see  Figure  10(g)).Feature  fusion\r\nmodule (FFM) v1equals the dimensions of the input feature\r\nmaps by a sequence of1\u00021convolution and upsampling\r\noperations. Then, the resulting two-layer features are prop-\r\nagated  tothinned  U-shape  modules  (TUM). Excluding  the\r\ninitial  propagation,  each  time  these  two-layer  features  are\r\nintegrated  to  the  output  of  the  previous  TUM  by1\u00021\r\nconvolutions inFFMv2.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"xwW6Bff+PW1Yl1nU/nImdTuJ7Dn7eLL4PwpT1dzWkwg="},"d0ce9cf8-9617-4d05-861f-0d6567ef6cda":{"id_":"d0ce9cf8-9617-4d05-861f-0d6567ef6cda","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"OLuD9SHVQE1QLkBei/j6+23uF8niabY+sB5ZZI/4+eQ="},"PREVIOUS":{"nodeId":"ae9839be-4fd6-43d9-8df6-292e926d6287","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"xwW6Bff+PW1Yl1nU/nImdTuJ7Dn7eLL4PwpT1dzWkwg="}},"text":"Note that the depth of the network\r\nis  increasing  after  each  application  of  the  TUM  and  the\r\nfeatures are becoming more high level. As a result of this, a\r\nsimilar problem with the FPN feature imbalance arise again. As  in  the  work  proposed  by  [78],  the  authors  employed\r\nsqueeze and excitation networks [116] to combine different\r\npyramidal shape features. Rather than using hand-crafted architectures,Neural Ar-\r\nchitecture  Search  FPN(NAS-FPN)  [81]  aims  to  search  for\r\nthe  best  architecture  to  generate  pyramidal  features  given\r\nthe  backbone  features  by  using  neural  architecture  search\r\nmethods [118] – Figure 10(h). This idea was also previously\r\napplied  to  the  image  classification  task  and  showed  to\r\nperform  well  [119],  [120],  [121].Auto-FPNis  also  another\r\nexample for using NAS while learning the connections from\r\nbackbone features to pyramidal features and beyond. While\r\nNAS-FPN achieves higher performance, Auto-FPN is more\r\nefficient  and  has  less  memory  footprint. The  idea  is  also\r\napplied  to  backbone  design  for  object  detection  by  Chen\r\net  al. [122],  however  it  is  not  within  the  scope  of  our\r\npaper. Considering their performance in other tasks such as\r\nEfficientNet [121] and different definitions of search spaces\r\nmay lead to better performance in NAS methods, more work\r\nis expected for FPN design using NAS. 5.3    Comparative Summary\r\nSSD [19] provides an analysis on the importance of making\r\npredictions  from  different  number  of  layers  with  varying\r\nscales. Increasing  the  number  of  prediction  layers  leads  to\r\na  significant  performance  gain. While  predicting  from  one\r\nlayer  provides62:4mAP@0.5  on  Pascal  VOC  2007  test  set\r\n[51] with SSD-300, it is70:7and74:6when the number of","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"d0SWBxseD9EHCJg4GFhsXK4jvcna/HIBmK60y5iCqG0="},"d4027622-9be1-49a3-a4dd-83bd2c42ddbd":{"id_":"d4027622-9be1-49a3-a4dd-83bd2c42ddbd","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ptWQxT9u+NXtMVEBFolWSAjxAAnwGYYvN0jUhSDrTys="},"NEXT":{"nodeId":"9d894443-b6ef-403a-8077-b7a8128da042","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zWyGeKy6FvUGFm3ft6ZxxvbwN8rtXZBoM5cQLROPyhc="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW18\r\nprediction layers is3and5respectively (while keeping the\r\nnumber  of  predictions  almost  equal). Therefore,  once  ad-\r\ndressed in the detection pipeline, scale imbalance methods\r\ncan significantly boost performance. Feature  pyramid  based  methods  increased  the  perfor-\r\nmance  significantly  compared  to  SSD. Once  included  in\r\nFaster R-CNN with ResNet-101, the pioneering FPN study\r\n[26]  has  a  relative  improvement  of3:7%on  MS  COCO\r\ntestdev  (from34:9to36:2). Another  virtue  of  the  feature\r\npyramids  is  the  efficiency  due  to  having  lighter  detection\r\nnetworks: e.g., the model with FPN is reported to be more\r\nthan  two  times  faster  than  baseline  Faster  R-CNN  with\r\nResNet-50 backbone (150ms vs320ms per image on a single\r\nNVIDIA  M40  GPU)  [26]. Currently,  the  most  promising\r\nresults  are  obtained  via  NAS  by  learning  how  to  com-\r\nbine the backbone features from different levels. Using the\r\nResNet-50 backbone with1024\u00021024image size, a10:2%\r\nrelative  improvement  is  obtained  on  MS  COCO  testdev\r\n(from40:1to44:2) by NAS-FPN [81] but it needs also to be\r\nnoted  that  number  of  parameters  in  the  learned  structure\r\nis  approximately  two  times  higher  and  inference  time  is\r\n26%more  (92:1ms  vs73:0ms  per  image  on  a  single  P100\r\nGPU). Inference time is also a major drawback of the image\r\npyramid  based  methods. While  SNIP  reports  inference  at\r\napproximately1image/sec,  the  faster  version,  SNIPER,\r\nimproves it to5images/sec on a V100 GPU (200ms/image).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/8jbB+4BX/s+l3AnqzG+RLLmQdAA7SisXx07LlooAns="},"9d894443-b6ef-403a-8077-b7a8128da042":{"id_":"9d894443-b6ef-403a-8077-b7a8128da042","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ptWQxT9u+NXtMVEBFolWSAjxAAnwGYYvN0jUhSDrTys="},"PREVIOUS":{"nodeId":"d4027622-9be1-49a3-a4dd-83bd2c42ddbd","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"/8jbB+4BX/s+l3AnqzG+RLLmQdAA7SisXx07LlooAns="},"NEXT":{"nodeId":"51d61cf4-e8ee-4d64-9092-cc5dd4a3a3a0","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"WuC53UYYSXrqaSLDfSV2ZccKtHPRLBKDdpcUVirgsLU="}},"text":"In order to adjust the inference time-performance trade-\r\noff  in  scale  imbalance,  a  common  method  is  to  use  multi-\r\nscale  images  with  one  backbone  and  multiple  lighter  net-\r\nworks. All methods in Section 5.1.4 are published last year. To illustrate the performance gains: Pang et al. [72] achieved\r\n19:5%on  MS  COCO  test-dev  relative  improvement  with\r\n12ms  per  image  inference  time  compared  to  SSD300  with\r\nthe  same  size  image  and  backbone  network  having10ms\r\nper image inference time (mAPs are25:1vs30:0). 5.4    Open Issues\r\nHere we discuss open problems concerning scale imbalance. 5.4.1  Characteristics of Different Layers of Feature Hierar-\r\nchies\r\nIn  feature-pyramid  based  methods  (Section  5.2),  a  promi-\r\nnent  and  common  pattern  is  to  include  a  top-down  path-\r\nway in order to integrate higher-layer features with lower-\r\nlayer ones. Although this approach has yielded promising\r\nimprovements  in  performance,  an  established  perspective\r\nabout what critical aspects of features (or information) are\r\nhandled differently  in those  methods is  missing. Here, we\r\nhighlight three such aspects:\r\n(i) Abstractness. Higher layers in a feature hierarchy carry\r\nhigh-level, semantically more meaningful information about\r\nthe objects or object parts whereas the lower layers represent\r\nlow-level information in the scene, such as edges, contours,\r\ncorners etc. In other words, higher-layer features are more\r\nabstract. (ii)  Coarseness. To  reduce  dimensions,  feature  networks\r\ngradually reduce the size of the layers towards the top of the\r\nhierarchy. Although this is reasonable given the constraints,\r\nit  has  an  immediate  outcome  on  the  number  of  neurons\r\nthat  a  fixed  bounding  box  at  the  image  level  encapsulates\r\nat the different feature layers. Namely, the BB will include\r\nless  neurons  when  projected  to  the  highest  layer. In  other\r\nwords, higher layers are more coarse.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"zWyGeKy6FvUGFm3ft6ZxxvbwN8rtXZBoM5cQLROPyhc="},"51d61cf4-e8ee-4d64-9092-cc5dd4a3a3a0":{"id_":"51d61cf4-e8ee-4d64-9092-cc5dd4a3a3a0","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ptWQxT9u+NXtMVEBFolWSAjxAAnwGYYvN0jUhSDrTys="},"PREVIOUS":{"nodeId":"9d894443-b6ef-403a-8077-b7a8128da042","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"zWyGeKy6FvUGFm3ft6ZxxvbwN8rtXZBoM5cQLROPyhc="},"NEXT":{"nodeId":"34c87b9d-20e7-487a-af17-2d9462009079","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"X6Ig7MpFJPiUommFRUimTnU8Y7+Gp/DcR9iIbgzGf5U="}},"text":"In  other\r\nwords, higher layers are more coarse. (iii) Cardinality. In FPN and many of its variants, prediction\r\nis performed for an object from the layer that matches the\r\nobject’s  scale. Since  the  scales  of  objects  are  not  balanced,\r\nthis  approach  has  a  direct  affect  on  the  number  of  predic-\r\ntions and backpropagation performed through a layer. We argue that analyzing and addressing these aspects in\r\na  more  established  manner  is  critical  for  developing  more\r\nprofound  solutions. Although  we  see  that  some  methods\r\nhandle  imbalance  in  these  aspects  (e.g. Libra  FPN  [29]\r\naddresses all three aspects, Path Aggregation Network [75]\r\nhandles  abstractness  and  cardinality,  whereas  FPN  solves\r\nonly abstractness to a certain extent), these aspects should\r\nbe quantified and used for comparing different methods. 5.4.2  Image Pyramids in Deep Object Detectors\r\nOpen Issue.It is hard to exploit image pyramids using neural\r\nnetworks  due  to  memory  limitations. Therefore,  finding\r\nsolutions  alleviating  this  constraint  (e.g.,  as  in  SNIP  [27])\r\nis still an open problem. Explanation.The  image  pyramids  (Figure  8(d))  were  com-\r\nmonly  adopted  by  the  pre-deep  learning  era  object  detec-\r\ntors. However, memory limitations motivated the methods\r\nbased on pyramidal features which, with less memory, are\r\nstill able to generate a set of features with different scales al-\r\nlowing predictions to occur at multiple scales. On the other\r\nhand, feature-pyramids are actually approximations of the\r\nfeatures  extracted  from  image  pyramids,  and  there  is  still\r\nroom for improvement given that using image pyramids is\r\nnot common among deep object detectors. 6    IMBALANCE3: SPATIALIMBALANCE\r\nDefinition.Size, shape, location – relative to both the image\r\nor another box – and IoU are spatial attributes of bounding\r\nboxes.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WuC53UYYSXrqaSLDfSV2ZccKtHPRLBKDdpcUVirgsLU="},"34c87b9d-20e7-487a-af17-2d9462009079":{"id_":"34c87b9d-20e7-487a-af17-2d9462009079","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ptWQxT9u+NXtMVEBFolWSAjxAAnwGYYvN0jUhSDrTys="},"PREVIOUS":{"nodeId":"51d61cf4-e8ee-4d64-9092-cc5dd4a3a3a0","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"WuC53UYYSXrqaSLDfSV2ZccKtHPRLBKDdpcUVirgsLU="}},"text":"Any  imbalance  in  such  attributes  is  likely  to  affect\r\nthe training and generalization performance. For example,\r\na slight shift in position may lead to drastic changes in the\r\nregression  (localization)  loss,  causing  an  imbalance  in  the\r\nloss values, if a suitable loss function is not adopted. In this\r\nsection,  we  discuss  these  problems  specific  to  the  spatial\r\nattributes and regression loss. 6.1    Imbalance in Regression Loss\r\nDefinition.This  imbalance  problem  is  concerned  with  the\r\nuneven contributions of different individual examples to the\r\nregression  loss. Figure  11  illustrates  the  problem  usingL1\r\nandL2losses, where the hard example (i.e. the one with low\r\nIoU, the yellow box) is dominating theL2loss, whereasL1\r\nloss assigns relatively more balanced errors to all examples. Solutions.The  regression  losses  for  object  detection  have\r\nevolved  under  two  main  streams:  The  first  one  is  theLp-\r\nnorm-based  (e.g.L1,L2)  loss  functions  and  the  second\r\none  is  the  IoU-based  loss  functions. Table  5  presents  a\r\ncomparison of the widely used regression losses. ReplacingL2regression loss [17], [123],SmoothL1Loss\r\n[17] is the first loss function designed specifically for deep\r\nobject detectors, and it has been widely adopted (e.g. [19],","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"X6Ig7MpFJPiUommFRUimTnU8Y7+Gp/DcR9iIbgzGf5U="},"9ff0f361-f5c4-46ec-ae67-b2f172024444":{"id_":"9ff0f361-f5c4-46ec-ae67-b2f172024444","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ufxa6pNQ/g4pvWNJ2aG8xRkg1oYdTSor3B9LRmhNALo="},"NEXT":{"nodeId":"d79c48dd-3b9f-4526-912d-d8c6613ee463","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"eCc9B23tW5f4EWljqMNtSlYkpQpywl3vrP9qXeLJu9I="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW19\r\nTABLE 5: A list of widely used loss functions for the BB regression task. Loss FunctionExplanation\r\nL2LossEmployed in earlier deep object detectors [16]. Stable for small errors but\r\npenalizes outliers heavily. L1LossNot stable for small errors. SmoothL1Loss [17]Baseline regression loss function. More robust to outliers compared toL1\r\nLoss. BalancedL1Loss [29]Increases the contribution of the inliers compared to smoothL1loss. Kullback-Leibler Loss [53]Predicts a confidence about the input bounding box based on KL diver-\r\ngence. IoU Loss [83]Uses an indirect calculation of IoU as the loss function. Bounded IoU Loss [84]Fixes all parameters of an input box in the IoU definition except the one\r\nwhose gradient is estimated during backpropagation. GIoU Loss [85]Extends the definition of IoU based on the smallest enclosing rectangle of\r\nthe inputs to the IoU, then uses directly IoU and the extended IoU, called\r\nGIoU, as the loss function. DIoU Loss, CIoU Loss [86]Extends the definition of IoU by adding additional penalty terms concern-\r\ning aspect ratio difference and center distances between two boxes. IoUL1L2\r\n0.941.000.50\r\n0.656.006.00\r\n0.5110.0014.00\r\nIoUL1L2L1\r\nSmooth\r\n0.882.002.001\r\n0.656.004.822.82\r\n0.5110.006.284.28Fig. 11: An illustration of imbalance in regression loss. Blue\r\ndenotes  the  ground  truth  BB. There  are  three  prediction\r\nboxes,  marked  with  green,  red  and  yellow  colors. In  the\r\ntable  on  the  right,L1andL2columns  show  the  sum  of\r\nL1andL2errors between the box corners of the associated\r\nprediction  box  and  the  ground-truth  (blue)  box.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wRqhOl7ziBYjxs5a00IOI84RNLui9CKa/x4VHxFeYPI="},"d79c48dd-3b9f-4526-912d-d8c6613ee463":{"id_":"d79c48dd-3b9f-4526-912d-d8c6613ee463","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ufxa6pNQ/g4pvWNJ2aG8xRkg1oYdTSor3B9LRmhNALo="},"PREVIOUS":{"nodeId":"9ff0f361-f5c4-46ec-ae67-b2f172024444","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"wRqhOl7ziBYjxs5a00IOI84RNLui9CKa/x4VHxFeYPI="},"NEXT":{"nodeId":"4a5eaf2e-8f61-4293-9783-93e9479f051b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"UZ49s++6GDtdopFLCva3zBidemZq1lM2Pfriwnvg9Mo="}},"text":"Note  that\r\nthe  contribution  of  the  yellow  box  to  theL2loss  is  more\r\ndominating than its effect on totalL1error. Also, the contri-\r\nbution of the green box is less for theL2error. [21], [22]) since it reduces the effect of the outliers (compared\r\ntoL2loss) and it is more stable for small errors (compared\r\ntoL1loss). SmoothL1loss,  a  special  case  of  Huber  Loss\r\n[124], is defined as:\r\nL1smooth(^x) =\r\n8>\r\n>><\r\n>>>:\r\n0:5^x2;ifj^xj<1\r\nj^xj\u00000:5;otherwise. (9)\r\nwhere^xis  the  difference  between  the  estimated  and  the\r\ntarget BB coordinates. Motivated  by  the  fact  that  the  gradients  of  the  outliers\r\nstill  have  a  negative  effect  on  learning  the  inliers  with\r\nsmaller gradients in SmoothL1loss,BalancedL1Loss[29]\r\nincreases the gradient contribution of the inliers to the total\r\nloss  value. To  achieve  this,  the  authors  first  derive  the\r\ndefinition of the loss function originating from the desirable\r\nbalanced gradients across inliers and outliers:\r\n@L1balanced\r\n@^x=\r\n8>\r\n>><\r\n>>>:\r\n\u000bln(bj^xj+ 1);ifj^xj<1\r\n\u0012;otherwise,\r\n(10)\r\nwhere\u000bcontrols how much the inliers are promoted (small\r\n\u000bincreases the contribution of inliers);\u0012is the upper bound\r\nof the error to help balancing between the tasks.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"eCc9B23tW5f4EWljqMNtSlYkpQpywl3vrP9qXeLJu9I="},"4a5eaf2e-8f61-4293-9783-93e9479f051b":{"id_":"4a5eaf2e-8f61-4293-9783-93e9479f051b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ufxa6pNQ/g4pvWNJ2aG8xRkg1oYdTSor3B9LRmhNALo="},"PREVIOUS":{"nodeId":"d79c48dd-3b9f-4526-912d-d8c6613ee463","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"eCc9B23tW5f4EWljqMNtSlYkpQpywl3vrP9qXeLJu9I="}},"text":"Integrating\r\nEquation (10),L1balancedis derived as follows:\r\nL1balanced(^x) =\r\n8>\r\n><\r\n>>:\r\n\u000bb(bj^xj+ 1)ln(bj^xj+ 1)\u0000\u000bj^xj;ifj^xj<1\r\n\rj^xj+Z;otherwise,\r\n(11)\r\nwherebensuresL1balanced(^x= 1)is a continuous function,\r\nZis  a  constant  and  the  association  between  the  hyper-\r\nparamaters is:\r\n\u000bln(b+ 1) =\r:(12)\r\nHaving put more emphasis on inliers, BalancedL1Loss im-\r\nproves the performance especially for larger IoUs (namely,\r\nAP@0:75improves by%1:1). Another  approach,Kullback-Leibler  Loss(KL  Loss)  [53],\r\nis  driven  by  the  fact  that  the  ground  truth  boxes  can  be\r\nambiguous in some cases due to e.g. occlusion, shape of the\r\nobject  or  inaccurate  labeling. For  this  reason,  the  authors\r\naim  to  predict  a  probability  distribution  for  each  BB  coor-","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"UZ49s++6GDtdopFLCva3zBidemZq1lM2Pfriwnvg9Mo="},"7387e4e7-463f-4653-8cb8-1f297c5a8919":{"id_":"7387e4e7-463f-4653-8cb8-1f297c5a8919","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+HOgzV7Wm10mzdIndxUa+Y1P3NPvjGQ4m5fGM4J9BMA="},"NEXT":{"nodeId":"13b554db-859e-4591-91f9-d48e1e76bed2","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ef4qoeFk1iH4IRQOCMPWhulR1Al+fgfakqrzmHHPtEA="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW20\r\ndinate rather than direct BB prediction. The idea is similar\r\nto the networks with an additional localization confidence\r\nassociated  prediction  branch  [89],  [125],  [126],  [127],  [128],\r\nbesides  classification  and  regression,  in  order  to  use  the\r\npredicted confidence during inference. Differently, KL Loss,\r\neven  without  its  proposed  NMS,  has  an  improvement  in\r\nlocalization compared to the baseline. The method assumes\r\nthat  each  box  coordinate  is  independent  and  follows  a\r\nGaussian distribution with mean^xand standard deviation\r\n\u001b. Therefore,  in  addition  to  conventional  boxes,  a  branch\r\nis added to the network to predict the standard deviation,\r\nthat  is\u001b,  and  the  loss  is  backpropagated  using  the  KL\r\ndivergence  between  the  prediction  and  the  ground  truth\r\nsuch that the ground truth boxes are modelled by the dirac\r\ndelta  distribution  centered  at  the  box  coordinates. With\r\nthese assumptions, KL Loss is proportional to:\r\nLKL(^x;\u001b)/^x22\u001b2+12log\u001b2:(13)\r\nThey also employ gradient clipping similar to smoothL1in\r\norder  to  decrease  the  effect  of  the  outliers. During  NMS,\r\na  voting  scheme  is  also  proposed  to  combine  bounding\r\nboxes with different probability distributions based on the\r\ncertainty  of  each  box;  however,  this  method  is  out  of  the\r\nscope  of  our  paper. Note  that  the  choice  of  probability\r\ndistribution for bounding boxes matters since the loss defi-\r\nnition is affected by this choice. For example, Equation (13)\r\ndegenerates to Euclidean distance when\u001b= 1. In  addition  to  theLp-norm-based  loss  functions,  there\r\nare also IoU based loss functions which exploit the differen-\r\ntiable nature of IoU.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gCp72JsHhrw+hdX1dVYpHoJoC6X6pwsVHefL9ptGPro="},"13b554db-859e-4591-91f9-d48e1e76bed2":{"id_":"13b554db-859e-4591-91f9-d48e1e76bed2","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+HOgzV7Wm10mzdIndxUa+Y1P3NPvjGQ4m5fGM4J9BMA="},"PREVIOUS":{"nodeId":"7387e4e7-463f-4653-8cb8-1f297c5a8919","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gCp72JsHhrw+hdX1dVYpHoJoC6X6pwsVHefL9ptGPro="},"NEXT":{"nodeId":"cb9cac3e-4155-4fc6-949b-1efcb976a82b","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"SSAoIoFi8SeTmaalWdDYnOyYlxok7exnHlVk0zeRM34="}},"text":"An earlier example is theIoU Loss[83],\r\nwhere an object detector is successfully trained by directly\r\nformulating a loss based on the IoU as:\r\nLIoU=\u0000ln(IoU):(14)\r\nAnother approach to exploit the metric-nature of1\u0000IoUis\r\ntheBounded IoUloss [84]. This loss warps a modified version\r\nof1\u0000IoUto the smoothL1function. The modification in-\r\nvolves bounding the IoU by fixing all the parameters except\r\nthe one to be computed, which implies the computation of\r\nthe maximum attainable IoU for one parameter:\r\nLBIoU(x;^x) = 2L1smooth(1\u0000IoUB(x;^x));(15)\r\nwhere the bounding boxes are represented by center coordi-\r\nnates, width and height as[cx;cy;w;h]. Here, we define the\r\nbounded IoU only forcxandwsincecyandhhave similar\r\ndefinitions. We  follow  our  notation  in  Section  2  to  denote\r\nground truth and detection (i.e.cxfor ground truth and\u0016cx\r\nfor detection). With this notation,IoUB, the bounded IoU,\r\nis defined as follows:\r\nIoUB(cx;\u0016cx) = max\r\n\u0012\r\n0;w\u00002j\u0016cx\u0000cxjw+ 2j\u0016c\r\nx\u0000cxj\r\n\u0013\r\n;(16)\r\nIoUB(w;\u0016w) = min\r\n\u0012\u0016w\r\nw;\r\nw\r\n\u0016w\r\n\u0013\r\n:(17)\r\nIn  such  a  setting,IoUB\u0015IoU. Also,  an  IoU  based  loss\r\nfunction is warped into the smoothL1function in order to\r\nmake  the  ranges  of  the  classification  and  localization  task\r\nconsistent and to decrease the effect of outliers. Motivated  by  the  idea  that  the  best  loss  function  is\r\nthe performance metric itself, inGeneralized Intersection over\r\nUnion(GIoU)  [85]  showed  that  IoU  can  be  directly  opti-\r\nmized  and  that  IoU  and  the  proposed  GIoU  can  be  used\r\nas  a  loss  function.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ef4qoeFk1iH4IRQOCMPWhulR1Al+fgfakqrzmHHPtEA="},"cb9cac3e-4155-4fc6-949b-1efcb976a82b":{"id_":"cb9cac3e-4155-4fc6-949b-1efcb976a82b","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+HOgzV7Wm10mzdIndxUa+Y1P3NPvjGQ4m5fGM4J9BMA="},"PREVIOUS":{"nodeId":"13b554db-859e-4591-91f9-d48e1e76bed2","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ef4qoeFk1iH4IRQOCMPWhulR1Al+fgfakqrzmHHPtEA="},"NEXT":{"nodeId":"d6f57b2a-6e0b-4b91-a599-c5177c41517e","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Kuhg/VuelSds10J9iFf0/3nOcLE/ftvPoNLjVK9B8tw="}},"text":"GIoU  is  proposed  as  both  a  perfor-\r\nmance  measure  and  a  loss  function  while  amending  the\r\nmajor  drawback  of  the  IoU  (i.e. the  plateau  when  IoU=0)\r\nby  incorporating  an  additional  smallest  enclosing  boxE. In  such  a  way,  even  when  two  boxes  do  not  overlap,  a\r\nGIoU  value  can  be  assigned  to  them  and  this  allows  the\r\nfunction  to  have  non-zero  gradient  throughout  the  entire\r\ninput  domain  rather  being  limited  to  IoU>0. Unlike  IoU,\r\ntheGIoU(B;\u0016B)2[\u00001;1]. Having  computedE,  GIoU  is\r\ndefined as:\r\nLGIoU(B;\u0016B) =IoU(B;\u0016B)\u0000A(E)\u0000A(B[\r\n\u0016B)\r\nA(E);(18)\r\nwhere GIoU is a lower bound for IoU, and it converges to\r\nIoU  whenA(B[\u0016B)  =A(E). GIoU  preserves  the  advan-\r\ntages  of  IoU,  and  makes  it  differentiable  when  IoU=0. On\r\nthe other hand, since positive labeled BBs have IoU larger\r\nthan 0.5 by definition, this portion of the function is never\r\nvisited in practice, yet still, GIoU Loss performs better than\r\nusing IoU directly as a loss function. A   different   idea   proposed   by   Zheng   et   al. [86]   is\r\nto  add  penalty  terms  to  the  conventional  IoU  error  (i.e. 1\u0000IoU(B;\u0016B)) in order to ensure faster and more accurate\r\nconvergence.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"SSAoIoFi8SeTmaalWdDYnOyYlxok7exnHlVk0zeRM34="},"d6f57b2a-6e0b-4b91-a599-c5177c41517e":{"id_":"d6f57b2a-6e0b-4b91-a599-c5177c41517e","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+HOgzV7Wm10mzdIndxUa+Y1P3NPvjGQ4m5fGM4J9BMA="},"PREVIOUS":{"nodeId":"cb9cac3e-4155-4fc6-949b-1efcb976a82b","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"SSAoIoFi8SeTmaalWdDYnOyYlxok7exnHlVk0zeRM34="}},"text":"To achieve that, in Distance IoU (DIoU) Loss,\r\na penalty term related with the distances of the centers ofB\r\nand\u0016Bis added as:\r\nLDIoU(B;\u0016B) = 1\u0000IoU(B;\u0016B) +d2(BC;\r\n\u0016BC)\r\nE2D;(19)\r\nwhered()is the Euclidean distance,BCis the center point\r\nof boxBandEDis the diagonal length ofE(i.e. smallest\r\nenclosing box). To further enhance their method,LDIoUis\r\nextended with an additional penalty term for inconsistency\r\nin  aspect  ratios  of  two  boxes. The  resulting  loss  function,\r\ncoined as Complete IoU (CIoU), is defined as:\r\nLCIoU(B;\u0016B) = 1\u0000IoU(B;\u0016B) +d2(BC;\r\n\u0016BC)\r\nE2D+\u000bv;(20)\r\nsuch that\r\nv=4\u00192\r\n\u0012\r\narctan(wh)\u0000arctan(\u0016w\u0016h)\r\n\u00132\r\n;(21)\r\nand\r\n\u000b=v(1\u0000IoU(B;\u0016B)) +v:(22)\r\nIn this formulation,\u000b, the trade-off parameter, ensures the\r\nimportance  of  the  cases  with  smaller  IoUs  to  be  higher. In  the  paper,  one  interesting  approach  is  to  validate  faster\r\nand  more  accurate  convergence  by  designing  simulation\r\nscenarios  since  it  is  not  straightforward  to  analyze  IoU-\r\nbased loss functions (see Section 6.5). 6.2    IoU Distribution Imbalance\r\nDefinition.IoU  distribution  imbalance  is  observed  when\r\ninput bounding boxes have a skewed IoU distribution. The\r\nproblem is illustrated in Figure 12(a), where the IoU distri-\r\nbution of the anchors in RetinaNet [22] are observed to be","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Kuhg/VuelSds10J9iFf0/3nOcLE/ftvPoNLjVK9B8tw="},"83eeb295-f44a-4433-9f78-d99cf9581f5b":{"id_":"83eeb295-f44a-4433-9f78-d99cf9581f5b","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bSx8/ZxceE1zifds6kNdh3rWbX4OjHehtUfT/N8G5VE="},"NEXT":{"nodeId":"c9959fac-067d-4091-9803-78ddb6d8ebe3","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+WgrGZbnukx+RERhQrxjQPLWJHwg03bUxiFhCc421Nk="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW21\r\n0.50.60.70.80.91.0IoU0\r\n100k\r\n200k\r\n300k\r\n400k\r\n500k\r\n600k\r\nCount\r\n(a)\r\n0.50.60.70.80.91.0IoU\r\nB\r\n0.00\r\n0.25\r\n0.50\r\n0.75\r\n1.00\r\nIoUA\r\nIoUA=IoUBIoU\r\nA=0.5\r\nIoUA=IoUBIoU\r\nA=0.5100\r\n101\r\n102\r\n103\r\n(b)\r\n0.5-0.60.6-0.70.7-0.80.8-0.90.9-1.0IoU\r\nB Intervals\r\n0\r\n20\r\n40\r\n60\r\n80\r\n% Anchor\r\n% Anchors with IoUA<0.5% Anchors with IoU\r\nA<IoUB\r\n(c)\r\nFig. 12: The IoU distribution of the positive anchors for a converged RetinaNet [22] with ResNet-50 [93] on MS COCO [90]\r\nbefore regression(a), and(b)the density of how the IoU values are affected by regression (IoUB: before regression,IoUA:\r\nafter regression). For clarity, the density is plotted in log-scale.(c)A concise summary of how regression changes IoUs of\r\nanchors as a function of their starting IoUs (IoUB). Notice that while it is better not to regress the boxes in larger IoUs,\r\nregressor causes false positives more in lower IoUs. skewed towards lower IoUs. It has been previously shown\r\nthat this imbalance is also observed while training two-stage\r\ndetectors  [87]. Differently,  we  present  in  Figure  12(b)  how\r\neach anchor is affected by regression. The rate of degraded\r\nanchors after regression (i.e. positive anchors under the blue\r\nline)  decreases  towards  the  threshold  that  the  regressor  is\r\ntrained  upon,  which  quantitatively  confirms  the  claims  of\r\nCai et al. (see Figure 12(c)). On the other hand, the rate of\r\nthe anchors that become a false positive (i.e.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"TUMTzJGpdDC3+T9Pgh8JBAMS79Vaq096ovZ8W6HOOPE="},"c9959fac-067d-4091-9803-78ddb6d8ebe3":{"id_":"c9959fac-067d-4091-9803-78ddb6d8ebe3","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bSx8/ZxceE1zifds6kNdh3rWbX4OjHehtUfT/N8G5VE="},"PREVIOUS":{"nodeId":"83eeb295-f44a-4433-9f78-d99cf9581f5b","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"TUMTzJGpdDC3+T9Pgh8JBAMS79Vaq096ovZ8W6HOOPE="},"NEXT":{"nodeId":"e2e4fe00-9f0b-43ac-912e-38acde823851","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"1IEiKrb6uVKEsDcA67xQA6Ie6AQpQjWzWfFM5+JmjA0="}},"text":"On the other hand, the rate of\r\nthe anchors that become a false positive (i.e. positive anchors\r\nunder the red line), is increasing towards the0:5\u00000:6bin,\r\nfor  which  around5%of  the  positive  anchors  are  lost  by\r\nthe regressor (see Figure 12(c)). Furthermore, comparing the\r\naverage IoU error of the anchors before and after regression\r\non a converged model, we notice that it is better off without\r\napplying regression and use the unregressed anchors for the\r\nIoUBintervals0:8\u00000:9and0:9\u00001:0. These results suggest\r\nthat there is still room to improve by observing the effect of\r\nthe regressor on the IoUs of the input bounding boxes. Solutions.TheCascade R-CNNmethod [87] has been the first\r\nto address the IoU imbalance. Motivated by the arguments\r\nthat  (i)  a  single  detector  can  be  optimal  for  a  single  IoU\r\nthreshold,  and  (ii)  skewed  IoU  distributions  make  the  re-\r\ngressor  overfit  for  a  single  threshold,  they  show  that  the\r\nIoU  distribution  of  the  positive  samples  has  an  effect  on\r\nthe regression branch. In order to alleviate the problem, the\r\nauthors trained three detectors, in a cascaded pipeline, with\r\nIoU  thresholds0:5,0:6and0:7for  positive  samples. Each\r\ndetector  in  the  cascade  uses  the  boxes  from  the  previous\r\nstage  rather  than  sampling  them  anew. In  this  way,  they\r\nshow  that  the  skewness  of  the  distribution  can  be  shifted\r\nfrom  the  left-skewed  to  approximately  uniform  and  even\r\nto  the  right-skewed,  thereby  allowing  the  model  to  have\r\nenough  samples  for  the  optimal  IoU  threshold  that  it  is\r\ntrained with.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+WgrGZbnukx+RERhQrxjQPLWJHwg03bUxiFhCc421Nk="},"e2e4fe00-9f0b-43ac-912e-38acde823851":{"id_":"e2e4fe00-9f0b-43ac-912e-38acde823851","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bSx8/ZxceE1zifds6kNdh3rWbX4OjHehtUfT/N8G5VE="},"PREVIOUS":{"nodeId":"c9959fac-067d-4091-9803-78ddb6d8ebe3","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"+WgrGZbnukx+RERhQrxjQPLWJHwg03bUxiFhCc421Nk="},"NEXT":{"nodeId":"504bc6b3-85f1-480b-883b-da60dc527428","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"uy1Pod//n8skk+I47e2id5hX3EHflPL0kb2S5tMxxPo="}},"text":"The authors show that such a cascaded scheme\r\nworks better compared to the previous work, such as Multi-\r\nRegion  CNN  [129]  and  AttractioNet  [130],  that  iteratively\r\napply  the  same  network  to  the  bounding  boxes. Another\r\ncascade-based  structure  implicitly  addressing  the  IoU  im-\r\nbalance isHierarchical Shot Detector(HSD) [88]. Rather than\r\nusing  a  classifier  and  regressor  at  different  cascade-levels,\r\nthe method runs its classifier after the boxes are regressed,\r\nresulting in a more balanced distribution. In another set of studies, randomly generated bounding\r\nboxes are utilized to provide a set of positive input bound-\r\ning boxes with balanced IoU distribution to the second stage\r\nof Faster R-CNN.IoU-uniform R-CNN[89] adds controllable\r\njitters  and  in  such  a  way  provides  approximately  uniform\r\npositive  input  bounding  boxes  to  the  regressor  only  (i.e. the  classification  branch  still  uses  the  RPN  RoIs). Differ-\r\nently,pRoI  Generator[66]  trains  both  branches  with  the\r\ngenerated  RoIs  but  the  performance  improvement  is  not\r\nthat  significant  probably  because  the  training  set  covers  a\r\nmuch larger space than the test set. However, one significant\r\ncontribution  of  Oksuz  et  al. [66]  is,  rather  than  adding\r\ncontrollable  jitters,  they  systematically  generate  bounding\r\nboxes  using  the  proposed  bounding  box  generator. Using\r\nthis  pRoI  generator,  they  conducted  a  set  of  experiments\r\nfor  different  IoU  distributions  and  reported  the  following:\r\n(i) The IoU distribution of the input bounding boxes has an\r\neffect not only on the regression but also on the classification\r\nperformance. (ii) Similar to the finding of Pang et al. [29], the\r\nIoU of the examples is related to their hardness. However,\r\ncontrary to Cao et al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"1IEiKrb6uVKEsDcA67xQA6Ie6AQpQjWzWfFM5+JmjA0="},"504bc6b3-85f1-480b-883b-da60dc527428":{"id_":"504bc6b3-85f1-480b-883b-da60dc527428","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_21","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bSx8/ZxceE1zifds6kNdh3rWbX4OjHehtUfT/N8G5VE="},"PREVIOUS":{"nodeId":"e2e4fe00-9f0b-43ac-912e-38acde823851","metadata":{"page_number":21,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"1IEiKrb6uVKEsDcA67xQA6Ie6AQpQjWzWfFM5+JmjA0="}},"text":"However,\r\ncontrary to Cao et al. [30], who argued that OHEM [24] has\r\nan adverse effect when applied only to positive examples,\r\nOksuz et al. [66] showed that the effect of OHEM depends\r\non  the  IoU  distribution  of  the  positive  input  BBs. When\r\na  right-skewed  IoU  distribution  is  used  with  OHEM,  a\r\nsignificant performance improvement is observed. (iii) The\r\nbest performance is achieved when the IoU distribution is\r\nuniform. 6.3    Object Location Imbalance\r\nDefinition.The  distribution  of  the  objects  throughout  the\r\nimage matters because current deep object detectors employ\r\ndensely sampled anchors as sliding window classifiers. For\r\nmost  of  the  methods,  the  anchors  are  evenly  distributed\r\nwithin the image, so that each part in the image is consid-\r\nered with the same importance level. On the other hand, the\r\nobjects in an image do not follow a uniform distribution (see\r\nFigure 13), i.e. there is an imbalance about object locations. Solutions.Motivated  by  the  fact  that  the  objects  are  not\r\ndistributed uniformly over the image, Wang et al. [67] aim\r\nto learn the location, scale and aspect ratio attributes of the\r\nanchors  concurrently  in  order  to  decrease  the  number  of\r\nanchors  and  improve  recall  at  the  same  time. Specifically,\r\ngiven  the  backbone  feature  maps,  a  prediction  branch  is","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"uy1Pod//n8skk+I47e2id5hX3EHflPL0kb2S5tMxxPo="},"0de9c76c-2782-49f9-9707-063575221751":{"id_":"0de9c76c-2782-49f9-9707-063575221751","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"AIHgendMPhHLZLu492nWuqd6Y/kqUe51dN77pzHtlC4="},"NEXT":{"nodeId":"15e86af5-7730-40ff-b1f8-fa9c0c391554","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"kINox3HV9jNFBsBCAccPmwpqnCSvbxKi/Q4v9W2n7Uk="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW22\r\n0.000.250.500.751.00x1.00\r\n0.75\r\n0.50\r\n0.25\r\n0.00\r\ny\r\nDistribution of Object Centers\r\n1K\r\n2K\r\n3K\r\n(a) PASCAL-VOC [51]\r\n0.000.250.500.751.00x1.00\r\n0.75\r\n0.50\r\n0.25\r\n0.00\r\ny\r\nDistribution of Object Centers\r\n10K\r\n20K\r\n30K\r\n40K\r\n(b) MS-COCO [90]\r\n0.000.250.500.751.00x1.00\r\n0.75\r\n0.50\r\n0.25\r\n0.00\r\ny\r\nDistribution of Object Centers\r\n200K\r\n400K\r\n600K\r\n800K\r\n(c) Open Images [91]\r\n0.000.250.500.751.00x1.00\r\n0.75\r\n0.50\r\n0.25\r\n0.00\r\ny\r\nDistribution of Object Centers\r\n 50K\r\n100K\r\n150K\r\n200K\r\n250K\r\n(d) Objects 365 [92]\r\nFig. 13: Distribution of the centers of the objects in the common datasets over the normalized image. designed  for  each  of  these  tasks  to  generate  anchors:  (i)\r\nanchor location prediction branch predicts a probability for\r\neach  location  to  determine  whether  the  location  contains\r\nan  object,  and  a  hard  thresholding  approach  is  adopted\r\nbased on the output probabilities to determine the anchors,\r\n(ii)  anchor  shape  prediction  branch  generates  the  shape\r\nof  the  anchor  for  each  location. Since  the  anchors  vary\r\ndepending  on  the  image,  different  from  the  conventional\r\nmethods  (i.e. one-stage generators  and  RPN)  using  a fully\r\nconvolutional  classifier  over  the  feature  map,  the  authors\r\nproposed  anchor-guided  feature  adaptation  based  on  de-\r\nformable  convolutions  [131]  in  order  to  have  a  balanced\r\nrepresentation  depending  on  the  anchor  size.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"8jZJh8FBwX90Sy5UCk0jWJHTz8nA57At33ObkvFDVIM="},"15e86af5-7730-40ff-b1f8-fa9c0c391554":{"id_":"15e86af5-7730-40ff-b1f8-fa9c0c391554","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"AIHgendMPhHLZLu492nWuqd6Y/kqUe51dN77pzHtlC4="},"PREVIOUS":{"nodeId":"0de9c76c-2782-49f9-9707-063575221751","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"8jZJh8FBwX90Sy5UCk0jWJHTz8nA57At33ObkvFDVIM="},"NEXT":{"nodeId":"f80efbbe-4cff-4a85-ba45-6bfebf25dba8","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"DgaLbQZuL862221JV63raPSNBLD/7PwW0nfEFKyiWI8="}},"text":"Rather  than\r\nlearning  the  anchors,free  anchormethod  [68]  loosens  the\r\nhard constraint of the matching strategy (i.e. a sample being\r\npositive  ifIoU >0:5)  and  in  such  a  way,  each  anchor  is\r\nconsidered  a  matching  candidate  for  each  ground  truth. In  order  to  do  that,  the  authors  pick  a  bag  of  candidate\r\nanchors  for  each  ground  truth  using  the  sorted  IoUs  of\r\nthe  anchors  with  the  ground  truth. Among  this  bag  of\r\ncandidates, the proposed loss function aims to enforce the\r\nmost suitable anchor by considering both the regression and\r\nthe classification task to be matched with the ground truth. 6.4    Comparative Summary\r\nAddressing   spatial   imbalance   problems   has   resulted   in\r\nsignificant  improvements  for  object  detectors. In  general,\r\nwhile the methods for imbalance in regression loss and IoU\r\ndistribution imbalance yield improvement especially in the\r\nregressor  branch,  removing  the  bias  in  the  anchors  in  the\r\nlocation imbalance improves classification, too. Despite the widespread use of the SmoothL1loss, four\r\nnew  loss  functions  have  been  proposed  last  year. Chen  et\r\nal. [132]  compared  BalancedL1,  IoU,  GIoU,  Bounded  IoU\r\non  Faster  R-CNN+FPN  against  SmoothL1,  and  reported\r\nrelative  improvement  of0:8%,1:1%,1:4and\u00000:3%re-\r\nspectively. With  the  same  configuration,  DIoU  and  CIoU\r\nlosses are reported to have a relative improvement of0:2%\r\nand1:7%against the GIoU baseline (without DIoU-NMS). Compared to the loss functions designed for the foreground-\r\nbackground imbalance problem (i.e. Focal Loss - see Section\r\n4.1.2), the relative improvement of the regression losses over\r\nSmoothL1are  smaller.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"kINox3HV9jNFBsBCAccPmwpqnCSvbxKi/Q4v9W2n7Uk="},"f80efbbe-4cff-4a85-ba45-6bfebf25dba8":{"id_":"f80efbbe-4cff-4a85-ba45-6bfebf25dba8","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"AIHgendMPhHLZLu492nWuqd6Y/kqUe51dN77pzHtlC4="},"PREVIOUS":{"nodeId":"15e86af5-7730-40ff-b1f8-fa9c0c391554","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"kINox3HV9jNFBsBCAccPmwpqnCSvbxKi/Q4v9W2n7Uk="},"NEXT":{"nodeId":"f23c3e9b-e64a-4f9a-b4e2-cbf786af8470","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"4e7M2OiEvsky9M/ebs4gs490ihN4HDdJY1RorI6CiS4="}},"text":"We  also  analyzed  for  which  cases\r\nthe  baseline  loss  function  fails  in  Figure  12(b),  which  can\r\nbe  used  as  a  tool  to  compare  the  pros/cons  of  different\r\nregression loss functions. Cascaded  structures  are  proven  to  be  very  useful  for\r\nobject detection by regulating the IoU distribution. Cascade\r\nR-CNN, being a two-stage detector, has a relative improve-\r\nment of18:2%on MS-COCO testdev compared to its base-\r\nline  Faster  R-CNN+FPN  with  ResNet-101  backbone  (36:2\r\nvs.42:8). A one-stage architecture, HSD, also has a relative\r\nimprovement of34:7%compared to the SSD512 with VGG-\r\n16 backbone (28:8vs38:8). It is difficult to compare Cascade\r\nR-CNN  and  HSD  since  they  are  different  in  nature  (one-\r\nstage and two-stage pipelines), and their performances were\r\nreported  for  different  input  resolutions. However,  we  ob-\r\nserved that HSD with a768\u0002768input image runs approx-\r\nimately1:5\u0002faster than Cascade R-CNN for a1333\u0002800\r\nimage  and  achieves  slightly  lower  performance  on  MS-\r\nCOCO  testdev  (42:3vs.42:8). One  observation  between\r\nthese  two  is  that,  even  though  HSD  performs  worse  than\r\nCascade  R-CNN  atAP@0:5,  it  yields  better  performance\r\nforAP@0:75than Cascade R-CNN, which implies that the\r\nregressor of HSD is better trained. As  for  the  methods  that  we  discussed  for  location  im-\r\nbalance,  guided  anchoring  [67]  increases  average  recall  by\r\n9:1%with90%fewer anchors via learning the parameters\r\nof the anchors.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DgaLbQZuL862221JV63raPSNBLD/7PwW0nfEFKyiWI8="},"f23c3e9b-e64a-4f9a-b4e2-cbf786af8470":{"id_":"f23c3e9b-e64a-4f9a-b4e2-cbf786af8470","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_22","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"AIHgendMPhHLZLu492nWuqd6Y/kqUe51dN77pzHtlC4="},"PREVIOUS":{"nodeId":"f80efbbe-4cff-4a85-ba45-6bfebf25dba8","metadata":{"page_number":22,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"DgaLbQZuL862221JV63raPSNBLD/7PwW0nfEFKyiWI8="}},"text":"Compared to guided anchoring, free anchor\r\n[68] reports a lower relative improvement for average recall\r\nwith2:4%against RetinaNet with ResNet50, however it has\r\na8:4%relative improvement (from35:7to38:7). 6.5    Open Issues\r\nThis section discusses the open issues related to the spatial\r\nproperties of the input bounding boxes and objects. 6.5.1  A Regression Loss with Many Aspects\r\nOpen Issue.Recent studies have proposed alternative regres-\r\nsion loss definitions with different perspectives and aspects. Owing to their benefits, a single regression loss function that\r\ncan combine these different aspects can be beneficial. Explanation.Recent regression loss functions have different\r\nmotivations: (i) BalancedL1Loss [29] increases the contri-\r\nbution  of  the  inliers. (ii)  KL  Loss  [53]  is  motivated  from\r\nthe  ambiguity  of  the  positive  samples. (iii)  IoU-based  loss\r\nfunctions have the motive to use a performance metric as a\r\nloss function. These seemingly mutually exclusive motives\r\ncan be integrated to a single regression loss function. 6.5.2  Analyzing the Loss Functions\r\nIn  order  to  analyze  how  outliers  and  inliers  affect  the\r\nregression  loss,  it  is  useful  to  analyze  the  loss  function","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4e7M2OiEvsky9M/ebs4gs490ihN4HDdJY1RorI6CiS4="},"795c5c26-3bcd-4f28-8ac6-928a36969297":{"id_":"795c5c26-3bcd-4f28-8ac6-928a36969297","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"IjwuYcSdEQj8zfWZ4QgM7qm2WK576TOpp4Q2g+VPTdg="},"NEXT":{"nodeId":"64339136-2ffe-4829-a04b-0c07d1976a6d","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rAFyf+gAueiHFzXHO1VQDLEBEvKgfHjYxLLj8Dns/j4="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW23\r\nFig. 14: The (relative) spatial distributions of1KRoIs with\r\nIoU between0:5to0:6from the RPN (of Faster R-CNN with\r\nResNet101 backbone) collected from Pascal [51] during the\r\nlast  epoch  of  the  training. A  bias  is  observed  around  the\r\ntop-left corners of ground truths such that RoIs are densely\r\nconcentrated at the top-left corner of the normalized ground\r\ntruth box. and  its  gradient  with  respect  to  the  inputs. To  illustrate\r\nsuch  an  analysis,  in  Focal  Loss  [22],  the  authors  plot  the\r\nloss  function  with  respect  to  the  confidence  score  of  the\r\nground truth class with a comparison to the cross entropy\r\nloss, the baseline. Similarly, in Balanced L1 Loss [29], both\r\nthe loss function itself and the gradients are depicted with\r\na  comparison  to  Smooth  L1  Loss. Such  an  analysis  might\r\nbe  more  difficult  for  the  recently  proposed  more  complex\r\nloss  functions. As  an  example,  AP  Loss  [61]  is  computed\r\nconsidering the ranking of the individual examples, which\r\nis  based  on  the  confidence  scores  of  all  BBs. So,  the  loss\r\ndepends on the entire set rather than individual examples,\r\nwhich  makes  it  difficult  to  plot  the  loss  (and  its  gradient)\r\nfor a single input as conventionally done. Another example\r\nis  GIoU  Loss  [85],  which  uses  the  ground  truth  box  and\r\nthe smallest enclosing box in addition to the detection box. Each box is represented by four parameters (see Section 6.1),\r\nwhich creates a total of twelve parameters. For this reason,\r\nit is necessary to develop appropriate analysis methods to\r\nobserve how these loss functions penalize the examples. 6.5.3  Designing Better Anchors\r\nDesigning  an  optimal  anchor  set  with  high  recall  has  re-\r\nceived  limited  attention.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jdYgaj9tN99mF5DH4hVMgoSnVpblSZFbzI8KuOQ9vlo="},"64339136-2ffe-4829-a04b-0c07d1976a6d":{"id_":"64339136-2ffe-4829-a04b-0c07d1976a6d","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"IjwuYcSdEQj8zfWZ4QgM7qm2WK576TOpp4Q2g+VPTdg="},"PREVIOUS":{"nodeId":"795c5c26-3bcd-4f28-8ac6-928a36969297","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jdYgaj9tN99mF5DH4hVMgoSnVpblSZFbzI8KuOQ9vlo="},"NEXT":{"nodeId":"dbd5cdb4-bdf9-4e42-86c2-386248269c4a","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"fBcsmz47ddgAPJgMr1Zfummlh0zhshRCqQ8zn29ww3E="}},"text":"The  Meta  Anchor  [133]  method\r\nattempts  to  find  an  optimal  set  of  aspect  ratios  and  scales\r\nfor anchors. More recently, Wang et al. [67] have improved\r\nrecall more than 9% on MS COCO dataset [90] while using\r\n90% less anchors than RPN [21]. Addressing the imbalanced\r\nnature of the locations and scales of the objects seems to be\r\nan open issue. 6.5.4  Relative Spatial Distribution Imbalance\r\nOpen Issue.As we discussed in Section 6, the distribution of\r\nthe IoU between the estimated BBs and the ground truth is\r\nimbalanced and this has an influence on the performance. A\r\ncloser inspection [66] reveals that the locations of estimated\r\nBBs  relative  to  the  matching  ground  truths  also  have  an\r\nimbalance. Whether this imbalance affects the performance\r\nof the object detectors remains to be investigated. Explanation.During  the  conventional  training  of  the  object\r\ndetectors,  input  bounding  boxes  are  labeled  as  positive\r\n234210\r\n235321\r\n125432\r\n124322\r\n112111\r\n(a)\r\n(c)\r\nGroundTruth\r\nPositiveInputBB\r\nNegativeInputBB\r\n(a)(b)\r\nFig. 15: An illustration of the imbalance in overlapping BBs. The  grid  represents  the  pixels  of  the  image/feature  map,\r\nand blue bounding box is the ground-truth.(a)Four nega-\r\ntive input bounding boxes.(b)Two positive input bounding\r\nboxes.(c)Per-pixel number-of-overlaps of the input bound-\r\ning boxes. Throughout the image, the number of sampling\r\nfrequencies  for  the  pixels  vary  due  to  the  variation  in  the\r\noverlapping number of bounding boxes. when their IoU with a ground truth is larger than0:5. This\r\nis  adopted  in  order  to  provide  more  diverse  examples  to\r\nthe  classifier  and  the  regressor,  and  to  allow  good  quality\r\npredictions  at  test  time  from  noisy  input  bounding  boxes. The  work  by  Oksuz  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rAFyf+gAueiHFzXHO1VQDLEBEvKgfHjYxLLj8Dns/j4="},"dbd5cdb4-bdf9-4e42-86c2-386248269c4a":{"id_":"dbd5cdb4-bdf9-4e42-86c2-386248269c4a","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"IjwuYcSdEQj8zfWZ4QgM7qm2WK576TOpp4Q2g+VPTdg="},"PREVIOUS":{"nodeId":"64339136-2ffe-4829-a04b-0c07d1976a6d","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rAFyf+gAueiHFzXHO1VQDLEBEvKgfHjYxLLj8Dns/j4="},"NEXT":{"nodeId":"5825e199-7728-4ee3-ab3e-387dbe1e27fa","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"XMmqdc+DXVgWOn5eIhLUsoB+mznyERuWN0UDhgmnHM8="}},"text":"The  work  by  Oksuz  et  al. [66]  is  currently  the  only  study\r\nthat points to an imbalance in the distribution of the relative\r\nlocation  of  BBs. Exploiting  the  scale-invariance  and  shift-\r\ninvariance  properties  of  the  IoU,  they  plotted  the  top-left\r\npoint  of  the  RoIs  from  the  RPN  (of  Faster  R-CNN)  with\r\nrespect  to  a  single  reference  box  representing  the  ground\r\ntruth (see Figure 14). They reported that the resulting spatial\r\ndistribution  of  the  top-left  points  of  the  RPN  RoIs  are\r\nskewed towards the top-left point of the reference ground\r\ntruth box. We see that the samples are scarce away from the\r\ntop-left corner of the reference box. 6.5.5  Imbalance in Overlapping BBs\r\nOpen  Issue.Due  to  the  dynamic  nature  of  bounding  box\r\nsampling methods (Section 4.1), some regions in the input\r\nimage  may  be  over-sampled  (i.e. regions  coinciding  with\r\nmany overlapping boxes) and some regions may be under-\r\nsampled  (or  not  even  sampled  at  all). The  effect  of  this\r\nimbalance  caused  by  BB  sampling  methods  has  not  been\r\nexplored. Explanation.Imbalance  in  overlapping  BBs  is  illustrated  in\r\nFigure  15(a-c)  on  an  example  grid  representing  the  image\r\nand  six  input  BBs  (four  negative  and  two  positive). The\r\nnumber of overlapping BBs for each pixel is shown in Figure\r\n15(c); in this example, this number ranges from0to5. This  imbalance  may  affect  the  performance  for  two\r\nreasons:  (i)  The  number  of  highly  sampled  regions  will\r\nplay  more  role  in  the  final  loss  functions,  which  can  lead\r\nthe  method  to  overfit  for  specific  features.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"fBcsmz47ddgAPJgMr1Zfummlh0zhshRCqQ8zn29ww3E="},"5825e199-7728-4ee3-ab3e-387dbe1e27fa":{"id_":"5825e199-7728-4ee3-ab3e-387dbe1e27fa","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_23","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"IjwuYcSdEQj8zfWZ4QgM7qm2WK576TOpp4Q2g+VPTdg="},"PREVIOUS":{"nodeId":"dbd5cdb4-bdf9-4e42-86c2-386248269c4a","metadata":{"page_number":23,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"fBcsmz47ddgAPJgMr1Zfummlh0zhshRCqQ8zn29ww3E="}},"text":"(ii)  The  fact\r\nthat  some  regions  are  over-sampled  and  some  are  under-\r\nsampled might have adverse effects on learning, as the size\r\nof  sample  (i.e. batch  size)  is  known  to  be  related  to  the\r\noptimal learning rate [134]. 6.5.6  Analysis of the Orientation Imbalance\r\nOpen Issue.The effects of imbalance in the orientation distri-\r\nbution of objects need to be investigated. Explanation.The distribution of the orientation of object in-\r\nstances  might  have  an  effect  on  the  final  performance. If\r\nthere is a typical orientation for the object, then the detector\r\nwill  likely  overfit  to  this  orientation  and  will  make  errors","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"XMmqdc+DXVgWOn5eIhLUsoB+mznyERuWN0UDhgmnHM8="},"641f530e-97b7-482e-95a2-b25d94d02a7d":{"id_":"641f530e-97b7-482e-95a2-b25d94d02a7d","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gdgQ7AN49bZjUxUgBHe9c810YcVPPYAVe/MZ2DxWSUM="},"NEXT":{"nodeId":"0a3c5365-49c9-4d76-931f-c83bca5f5138","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"g/34yF1iG7DNkMKfHK89UwQwvq6ZawKLzIItRdgsaYE="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW24\r\n(a)(b)\r\nFig. 16:(a)Randomly  sampled  32  positive  RoIs  using  the\r\npRoI  Generator  [66].(b)Average  classification  and  regres-\r\nsion  losses  of  these  RoIs  at  the  initialization  of  the  object\r\ndetector for MS COCO dataset [90] with 80 classes. We use\r\ncross  entropy  for  the  classification  task  assuming  that  ini-\r\ntially each class has the same confidence score, and smooth\r\nL1 loss for regression task. Note that right after initialization,\r\nthe classification loss has more effect on the total loss. for the other orientations. To the best of our knowledge, this\r\nproblem has not yet been explored. 7    IMBALANCE4: OBJECTIVEIMBALANCE\r\nDefinition.Objective  imbalance  pertains  to  the  objective\r\n(loss)  function  that  is  minimized  during  training. By  defi-\r\nnition,  object  detection  requires  a  multi-task  loss  in  order\r\nto  solve  classification  and  regression  tasks  simultaneously. However,  different  tasks  can  lead  to  imbalance  because  of\r\nthe following differences: (i) The norms of the gradients can\r\nbe  different  for  the  tasks,  and  one  task  can  dominate  the\r\ntraining (see Figure 16). (ii) The ranges of the loss functions\r\nfrom  different  tasks  can  be  different,  which  hampers  the\r\nconsistent and balanced optimization of the tasks. (iii) The\r\ndifficulties  of  the  tasks  can  be  different,  which  affects  the\r\npace at which the tasks are learned, and hence hinders the\r\ntraining process [135]. Figure 16 illustrates a case where the loss of the classifi-\r\ncation dominates the overall gradient. Solutions.The  most  common  solution  isTask  Weighting\r\nwhich  balances  the  loss  terms  by  an  additional  hyper-\r\nparameter as the weighting factor. The hyper-parameter is\r\nselected  using  a  validation  set.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"BEBweH1b6oe6ijaIN2Tl09ZctKkgp1h1j+VI7DCnd5Y="},"0a3c5365-49c9-4d76-931f-c83bca5f5138":{"id_":"0a3c5365-49c9-4d76-931f-c83bca5f5138","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gdgQ7AN49bZjUxUgBHe9c810YcVPPYAVe/MZ2DxWSUM="},"PREVIOUS":{"nodeId":"641f530e-97b7-482e-95a2-b25d94d02a7d","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"BEBweH1b6oe6ijaIN2Tl09ZctKkgp1h1j+VI7DCnd5Y="},"NEXT":{"nodeId":"b281462e-8927-44eb-a92b-5cdb3d9c42fc","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Xji6BtHqZY0yWNnPpGv06dVcqZbZJNPJ3e7sV5m3JFk="}},"text":"The hyper-parameter is\r\nselected  using  a  validation  set. Naturally,  increasing  the\r\nnumber of tasks, as in the case of two-stage detectors, will\r\nincrease  the  number  of  weighting  factors  and  the  dimen-\r\nsions  of  the  search  space  (note  that  there  are  four  tasks  in\r\ntwo-stage detectors and two tasks in one-stage detectors). An issue arising from the multi-task nature is the possi-\r\nble range inconsistencies among different loss functions. For\r\nexample, inAP Loss, smooth L1, which is in the logarithmic\r\nrange (since the input to the loss is conventionally provided\r\nafter applying a logarithmic transformation) with[0;1), is\r\nused  for  regression  whileLAP2[0;1]. Another  example\r\nis  the  GIoU  Loss  [85],  which  is  in  the[\u00001;1]range  and\r\nused  together  with  cross  entropy  loss. The  authors  set  the\r\nweighting  factor  of  GIoU  Loss  to10and  regularization\r\nis  exploited  to  balance  this  range  difference  and  ensure\r\nbalanced training. Since  it  is  more  challenging  to  balance  terms  with  dif-\r\nferent ranges, it is a better strategy to first make the ranges\r\ncomparable. A  more  prominent  approach  to  combine  classification\r\nand  regression  tasks  isClassification-Aware  Regression  Loss\r\n(CARL) [30], which assumes that classification and regres-\r\nsion  tasks  are  correlated. To  combine  the  loss  terms,  the\r\nregression loss is scaled by a coefficient determined by the\r\n(classification) confidence score of the bounding box:\r\nLCARL(x) =c0iL1smooth(x);(23)\r\nwherec0iis  a  factor  based  onpi,  i.e.,  an  estimation  from\r\nthe classification task. In this way, regression loss provides\r\ngradient  signals  to  the  classification  branch  as  well,  and\r\ntherefore,  this  formulation  improves  localization  of  high-\r\nquality (prime) examples.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"g/34yF1iG7DNkMKfHK89UwQwvq6ZawKLzIItRdgsaYE="},"b281462e-8927-44eb-a92b-5cdb3d9c42fc":{"id_":"b281462e-8927-44eb-a92b-5cdb3d9c42fc","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gdgQ7AN49bZjUxUgBHe9c810YcVPPYAVe/MZ2DxWSUM="},"PREVIOUS":{"nodeId":"0a3c5365-49c9-4d76-931f-c83bca5f5138","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"g/34yF1iG7DNkMKfHK89UwQwvq6ZawKLzIItRdgsaYE="},"NEXT":{"nodeId":"2872f64a-01ab-485f-a935-7c4fa3c02bda","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"yJ4Z/VNePoWzlhCF0FpRU11znWANQikt4n0+aMkKlyU="}},"text":"An  important  contribution  of  CARL  is  to  employ  the\r\ncorrelation  between the  classification  and  regression tasks. However,  as  we  discuss  in  Section  7.2,  this  correlation\r\nshould be investigated and exploited more extensively. Recently,  Chen  et  al. [54]  showed  that  cumulative  loss\r\noriginating  from  cross  entropy  needs  to  be  dynamically\r\nweighted  since,  when  cross  entropy  loss  is  used,  the  con-\r\ntribution  rate  of  individual  loss  component  at  each  epoch\r\ncan be different. To prevent this imbalance, the authors pro-\r\nposedGuided  Losswhich  simply  weights  the  classification\r\ncomponent by considering the total magnitude of the losses\r\nas:\r\nwregLReg\r\nLcls:(24)\r\nThe motivation of the method is that regression loss consists\r\nof  only  foreground  examples  and  is  normalized  only  by\r\nnumber  of  foreground  classes,  therefore  it  can  be  used  as\r\na normalizer for the classification loss. 7.1    Comparative Summary\r\nCurrently,  except  for  linear  task  weighting,  there  is  no\r\nmethod  suitable  for  all  architectures  alleviating  objective\r\nimbalance,  and  unless  the  weights  of  the  tasks  are  set\r\naccordingly, training diverges [54]. While many studies use\r\nequal  weights  for  the  regression  and  classification  tasks\r\n[19], [22], it is also shown that appropriate linear weighting\r\ncan lead to small improvements on the performance [132]. However,  an  in-depth  analysis  on  objective  imbalance  is\r\nmissing in the literature. CARL   [30]   is   a   method   to   promote   examples   with\r\nhigher  IoUs,  and  in  such  a  way,1:6%relative  improve-\r\nment  is  achieved  compared  to  prime  sampling  without\r\nCARL  (i.e. from37:9to38:5). Another  method,  Guided\r\nLoss  [54],  weights  the  classification  loss  dynamically  to\r\nensure  balanced  training.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Xji6BtHqZY0yWNnPpGv06dVcqZbZJNPJ3e7sV5m3JFk="},"2872f64a-01ab-485f-a935-7c4fa3c02bda":{"id_":"2872f64a-01ab-485f-a935-7c4fa3c02bda","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_24","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gdgQ7AN49bZjUxUgBHe9c810YcVPPYAVe/MZ2DxWSUM="},"PREVIOUS":{"nodeId":"b281462e-8927-44eb-a92b-5cdb3d9c42fc","metadata":{"page_number":24,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Xji6BtHqZY0yWNnPpGv06dVcqZbZJNPJ3e7sV5m3JFk="}},"text":"This  method  achieves  a  similar\r\nperformance  with  the  baseline  RetinaNet,  without  using\r\nFocal  Loss. However,  their  method  does  not  discard  the\r\nlinear  weighting,  and  they  search  for  the  optimal  weight\r\nfor  different  architectures. Moreover,  the  effect  of  Guided\r\nLoss  is  not  clear  for  the  two-stage  object  detectors,  which\r\nconventionally employ cross entropy loss.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"yJ4Z/VNePoWzlhCF0FpRU11znWANQikt4n0+aMkKlyU="},"c8085b67-4ae4-4440-ba80-c492e81fb043":{"id_":"c8085b67-4ae4-4440-ba80-c492e81fb043","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Js0TzMtSXK9iXn1wIGTFSXRLi9nzoYFx5zbT3OxgNMs="},"NEXT":{"nodeId":"4ae1f4d3-257f-4be8-884c-b366127cf619","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"PX0W6jFLyg+XLVNC8Powc2aHI/8/C8wJoDGhQWGhNeQ="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW25\r\nFig. 17:  Paces  of  the  tasks  in  RPN  (with  Resnet-101  [93]\r\nbackbone) [21] on Pascal VOC 2007 training set [51]. 7.2    Open Issues\r\nOpen  Issue.Currently,  the  most  common  approach  is  to\r\nlinearly  combine  the  loss  functions  of  different  tasks  to\r\nobtain  the  overall  loss  function  (except  for  classification-\r\naware regression loss in [30]). However, as shown in Figure\r\n18(a-c),  as  the  input  bounding  box  is  slid  over  the  image,\r\nboth  classification  and  regression  losses  are  affected,  im-\r\nplying  their  dependence. This  suggests  that  current  linear\r\nweighting strategy may not be able to address the imbalance\r\nof  the  tasks  that  is  related  to  (i)  the  loss  values  and  their\r\ngradients, and (ii) the paces of the tasks. Explanation.The loss function of one task (i.e. classification)\r\ncan  affect  the  other  task  (i.e. regression). To  illustrate,AP\r\nLoss[61]  did  not  modify  the  regression  branch;  however,\r\nCOCO styleAP@0:75increased around3%. This example\r\nshows that the loss functions for different branches (tasks)\r\nare  not  independent  (see  also  Figure  18). This  interdepen-\r\ndence  of  tasks  has  been  explored  in  classification-aware\r\nregression  loss  by  Cao  et  al. [30]  (as  discussed  in  Section\r\n7)  to  a  certain  extent. Further  research  is  needed  for  a\r\nmore  detailed  analysis  of  this  interdependence  and  fully\r\nexploiting it for object detection. Some  studies  in  multi-task  learning  [135]  pointed  out\r\nthat  learning  pace  affects  performance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rBt7ZdqPvB70xrrdyeeFiMjonT5l5hYWroOMp3Xpe4k="},"4ae1f4d3-257f-4be8-884c-b366127cf619":{"id_":"4ae1f4d3-257f-4be8-884c-b366127cf619","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Js0TzMtSXK9iXn1wIGTFSXRLi9nzoYFx5zbT3OxgNMs="},"PREVIOUS":{"nodeId":"c8085b67-4ae4-4440-ba80-c492e81fb043","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rBt7ZdqPvB70xrrdyeeFiMjonT5l5hYWroOMp3Xpe4k="},"NEXT":{"nodeId":"2292bd4f-67ae-4a4d-8779-bbf4949ec4e8","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"9BUNyoOA/UgGmnhwERh5zPSL5vZAKYhBNT5HVV7BY/c="}},"text":"With  this  in  mind,\r\nwe  plotted  the  regression  and  classification  losses  of  the\r\nRPN [21] during training on the Pascal VOC dataset [51] in\r\nFigure 17. We observe that the classification loss decreases\r\nfaster than the regression loss. Analyzing and balancing the\r\nlearning paces of different tasks involved in the object de-\r\ntection problem might be another fruitful research direction. 8    IMBALANCEPROBLEMS INOTHERDOMAINS\r\nIn this section, we cover imbalance problems in other related\r\ndomains  in  order  to  motivate  the  adaptation  of  methods\r\nfrom  related  domains  to  the  object  detection  problem. We\r\nidentify two problems that are closely related to object de-\r\ntection: image classification and metric learning, discussed\r\nin individual subsections. In addition, the methods pertain-\r\ning to the other domains are discussed in the last subsection. 8.1    Image Classification\r\nImage classification is the problem of assigning a category\r\nlabel  for  an  image. This  problem  is  closely  related  to  the\r\nobject detection problem since it is one of two tasks in object\r\ndetection. For image classification problem, class imbalance\r\nhas  been  extensively  studied  from  very  different  perspec-\r\ntives  (compared  to  other  imbalance  problems),  and  in  this\r\nsection, we will focus only on class imbalance. A  common  approach  isResampling  the  dataset[136],\r\n[137], [138], [139], [140], including oversampling and under-\r\nsampling to balance the dataset. While oversampling adds\r\nmore  samples  from  the  under-represented  classes,  under-\r\nsampling  balances  over  the  classes  by  ignoring  some  of\r\nthe data from the over-represented classes. When employed\r\nnaively, oversampling may suffer from overfitting because\r\nduplication of samples from the under-represented classes\r\ncan  introduce  bias. Therefore,  despite  it  means  ignoring  a\r\nportion of the training data, undersampling was preferable\r\nfor non-deep-learning approaches [137]. On the other hand,\r\nBuda  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"PX0W6jFLyg+XLVNC8Powc2aHI/8/C8wJoDGhQWGhNeQ="},"2292bd4f-67ae-4a4d-8779-bbf4949ec4e8":{"id_":"2292bd4f-67ae-4a4d-8779-bbf4949ec4e8","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Js0TzMtSXK9iXn1wIGTFSXRLi9nzoYFx5zbT3OxgNMs="},"PREVIOUS":{"nodeId":"4ae1f4d3-257f-4be8-884c-b366127cf619","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"PX0W6jFLyg+XLVNC8Powc2aHI/8/C8wJoDGhQWGhNeQ="},"NEXT":{"nodeId":"f2c9eb69-e717-42f4-8ceb-0c48c279ed52","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Hq3v3XYD+oizFm36y9Fc3OkSPBaA2UVLxZz1UNf3j2Y="}},"text":"On the other hand,\r\nBuda  et  al. [139]  showed  that  deep  neural  networks  do\r\nnot suffer from overfitting under oversampling and in fact,\r\nbetter performance can be achieved compared to undersam-\r\npling. Over  the  years,  methods  more  complicated  than  just\r\nduplicating  examples  have  been  developed. For  example,\r\nChawla et al. [136] proposedSmoteas a new way of over-\r\nsampling by producing new samples as an interpolation of\r\nneighboring  samples.Adasyn[138],  an  extension  of  Smote\r\nto  generate  harder  examples,  aims  to  synthesize  samples\r\nfrom the underrepresented classes. Li et al. [140] sample a\r\nmini-batch  as  uniform  as  possible  from  all  classes  also  by\r\nrestricting the same example and class to appear in the same\r\norder, which implies promoting the minority classes. Another approach addressing class imbalance in image\r\nclassification istransfer learning[141], [142]. For example,\r\nWang  et  al. [141]  design  a  model  (i.e. meta-learner)  to\r\nlearn  how  a  model  evolves  when  the  size  of  the  training\r\nset  increases. The  meta-learner  model  is  trained  gradually\r\nby  increasing  the  provided  number  of  examples  from  the\r\nclasses  with  a  large  number  of  examples. The  resulting\r\nmeta-model is able to transform another model trained with\r\nless examples to a model trained with more examples, which\r\nmakes  it  useful  to  be  exploited  by  an  underrepresented\r\nclass. Another study [142] adopted a different strategy dur-\r\ning transfer learning: Firstly, the network is trained with the\r\nentire imbalanced dataset, and then the resulting network is\r\nfine-tuned by using a balanced subset of the dataset. Weighting  the  loss  function[143], [144] is yet another\r\nway of balancing the classes (we called it soft sampling in\r\nthe  paper). Among  these  approaches,  Huang  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9BUNyoOA/UgGmnhwERh5zPSL5vZAKYhBNT5HVV7BY/c="},"f2c9eb69-e717-42f4-8ceb-0c48c279ed52":{"id_":"f2c9eb69-e717-42f4-8ceb-0c48c279ed52","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_25","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Js0TzMtSXK9iXn1wIGTFSXRLi9nzoYFx5zbT3OxgNMs="},"PREVIOUS":{"nodeId":"2292bd4f-67ae-4a4d-8779-bbf4949ec4e8","metadata":{"page_number":25,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"9BUNyoOA/UgGmnhwERh5zPSL5vZAKYhBNT5HVV7BY/c="}},"text":"Among  these  approaches,  Huang  et  al. [143]\r\nuse  the  inverse  class  frequency  (i.e.1=jCij)  to  give  more\r\nweight  to  under-represented  classes. This  is  extended  by\r\nclass-balanced loss [144] to adopt inverse class frequency by\r\nemploying a hyperparameter\faswi= (1\u0000\f)=(1\u0000\fjCij). When\f= 0, the weight degenerates to1; and\f!1makes\r\nthe weight approximate1=jCij, the inverse class frequency. Similar  to  object  detection,  giving  more  importance  to\r\n“useful” examples is also common [145], [146], [147], [148]. As done in object detection, hard examples have been uti-\r\nlized, e.g. by Dong et al. [147] who identify hard (positive\r\nand  negative)  samples  in  the  batch  level. The  proposed\r\nmethod uses the hardness in two levels: (i) Class-level hard\r\nsamples are identified based on the predicted confidence for\r\nthe ground-truth class. (ii) Instance-level hard examples are\r\nthe ones with largerL2distance to a representative example","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Hq3v3XYD+oizFm36y9Fc3OkSPBaA2UVLxZz1UNf3j2Y="},"8dfbf89f-b367-4b2a-a029-2a3de052af36":{"id_":"8dfbf89f-b367-4b2a-a029-2a3de052af36","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VrllEHHzqOhiXlNnx1JPpnGrPBc7G768nb4PmIOYZSA="},"NEXT":{"nodeId":"c657bb19-648f-44ff-b435-b08ef4f0ca21","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vC/KfeNdXl9fqX65Pl2vgpqe0xOdOqWlRX8382Y0IW0="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW26\r\nin  the  feature  space. An  interesting  approach  that  has  not\r\nbeen utilized in object detection yet is to focus training on\r\nexamples on which the classifier is uncertain (based on its\r\nprediction  history)  [148]. Another  approach  is  to  generate\r\nhard examples by randomly cropping parts of an image as\r\nin  Hide  and  Seek  [146],  or  adopting  a  curricular  learning\r\napproach  by  first  training  the  classifier  on  easy  examples\r\nand then on hard examples [145]. Another  related  set  of  methods  addresses  image  clas-\r\nsification  problem  from  thedata  redundancyperspective\r\n[149], [150], [151], [152], [153]. Birodkar et al. [153] showed\r\nthat around 10% of the data in ImageNet [52] and CIFAR-10\r\ndatasets is redundant during training. This can be exploited\r\nnot  only  for  balancing  the  data  but  also  to  obtain  faster\r\nconvergence  by  ignoring  useless  data. The  methods  differ\r\nfrom each other on how they mine for the redundant exam-\r\nples. Regardless of whether imbalance problem is targeted\r\nor  not,  a  subset  of  these  methods  uses  the  active  learning\r\nparadigm,  where  an  oracle  is  used  to  find  out  the  best\r\nset  of  training  examples. The  core  set  approach  [151]  uses\r\nthe  relative  distances  of  the  examples  in  the  feature  space\r\nto  determine  redundant  samples  whereas  Vodrahalli  et  al. [152] determine redundancies by looking at the magnitude\r\nof the gradients. Another  mechanism  to  enrich  a  dataset  is  to  useweak\r\nsupervisionfor incorporating unlabelled examples. An ex-\r\nample  study  is  by  Mahajan  et  al. [154],  who  augment  the\r\ndataset by systematically including Instagram images with\r\nthe hashtags as the labels, which are rather noisy.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"jR6Z6KMctXqL8jVE56F/sIM2kc4jAANGZVcPrcSd/SY="},"c657bb19-648f-44ff-b435-b08ef4f0ca21":{"id_":"c657bb19-648f-44ff-b435-b08ef4f0ca21","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VrllEHHzqOhiXlNnx1JPpnGrPBc7G768nb4PmIOYZSA="},"PREVIOUS":{"nodeId":"8dfbf89f-b367-4b2a-a029-2a3de052af36","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jR6Z6KMctXqL8jVE56F/sIM2kc4jAANGZVcPrcSd/SY="},"NEXT":{"nodeId":"6a8ad5f7-f8c5-42b4-9d28-c0583650911e","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YDTH8R4ZL57QpDlE4to0qNLnrp58izJ8fKlrFw6dTWw="}},"text":"In another\r\nexample,  Liu  et  al. [155]  selectively  add  unlabeled  data  to\r\nthe training samples after labeling these examples using the\r\nclassifier. Generative  models  (e.g. GANs)  can  also  be  used  for\r\nextending  the  dataset  to  address  imbalance. Many  studies\r\n[156], [157], [158] have successfuly used GANs to generate\r\nexamples  for  under-represented  classes  for  various  image\r\nclassification problems. Special  cases  of  image  classification  (e.g. face  recogni-\r\ntion) are also affected by imbalance [159], [160], [161]. The\r\ngeneral  approaches  are  similar  and  therefore,  due  to  the\r\nspace  constraint,  we  omit  imbalance  in  specialized  classi-\r\nfication problems. Comparative  Summary.Our  analysis  reveals  that  object  de-\r\ntection community can benefit from the imbalance studies in\r\nimage classification in many different aspects. The discussed\r\nmethods  for  image  classification  are  (by  definition)  the\r\npresent solutions for the foreground-foreground class imbal-\r\nance problem (see Section 4.2); however, they can possibly\r\nbe extended to the foreground-background class imbalance\r\nproblem. Foreground-background  class  imbalance  is  gen-\r\nerally  handled  by  under-sampling  for  the  object  detection\r\nproblem, and other advanced resampling or transfer learn-\r\ning methods are not adopted yet for object detection from a\r\nclass imbalance perspective. While there are loss functions\r\n(discussed in Section 4.1.2) that exploit a weighting scheme\r\n[22], [59], Cui et al. [144] showed that class-balanced loss is\r\ncomplementary to the focal loss [22] in that focal loss aims\r\nto focus on hard examples while class-balanced loss implies\r\nbalancing  over  all  classes. However,  since  the  number  of\r\nbackground examples is not defined, the current definition\r\ndoes not fit into the object detection context.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"vC/KfeNdXl9fqX65Pl2vgpqe0xOdOqWlRX8382Y0IW0="},"6a8ad5f7-f8c5-42b4-9d28-c0583650911e":{"id_":"6a8ad5f7-f8c5-42b4-9d28-c0583650911e","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VrllEHHzqOhiXlNnx1JPpnGrPBc7G768nb4PmIOYZSA="},"PREVIOUS":{"nodeId":"c657bb19-648f-44ff-b435-b08ef4f0ca21","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"vC/KfeNdXl9fqX65Pl2vgpqe0xOdOqWlRX8382Y0IW0="},"NEXT":{"nodeId":"83c1c117-cd47-4b16-ad60-23efa7c1d364","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"r0G2IHuwx2+DuMRA7qCKFoGCvKcUJAmAlHO5ULaaYvE="}},"text":"Similarly, the\r\nadoption of weakly supervised methods to balance under-\r\nrepresented classes or data redundancy by also decreasing\r\nthe  samples  from  an  over-represented  class  can  be  used\r\nto  alleviate  the  class  imbalance  problem. Finally,  there  are\r\nonly a few generative approaches in object detection, much\r\nless than those proposed for addressing imbalance in image\r\nclassification. 8.2    Metric Learning\r\nMetric  learning methods  aim  to  find an  embedding  of the\r\ninputs,  where  the  distance  between  the  similar  examples\r\nis  smaller  than  the  distance  between  dissimilar  examples. In order to model such similarities, the methods generally\r\nemploy pairs [162], [163] or triplets [164], [165], [166] during\r\ntraining. In the pair case, the loss function uses the informa-\r\ntion about whether both of the samples are from the same or\r\ndifferent classes. In contrast, training using triplets require\r\nan anchor example, a positive example from the same class\r\nwith  the  anchor  and  a  negative  example  from  a  different\r\nclass  from  the  anchor. The  triplet-wise  training  scheme\r\nintroduces imbalance over positive and negative examples\r\nin favor of the latter one, and the methods also look for the\r\nhard  examples  as  in  object  detection  in  both  the  pair-wise\r\nand  triplet-wise  training  schemes. Accordingly,  to  present\r\nthe imbalance and useful example mining requirement, note\r\nthat there are approximatelyO(n2)pairs andO(n3)triplets\r\nassuming  that  the  dataset  size  isn. This  increase  in  the\r\ndataset size makes it impossible to mine for hard examples\r\nby  processing  the  entire  dataset  to  search  for  the  most\r\nuseful (i.e. hard) examples. For this reason, similar to object\r\ndetection, a decision is to be made on which samples to be\r\nused during training. The  metric  learning  methods  use  sampling,  generative\r\nmethods  or  novel  loss  functions  to  alleviate  the  problem.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"YDTH8R4ZL57QpDlE4to0qNLnrp58izJ8fKlrFw6dTWw="},"83c1c117-cd47-4b16-ad60-23efa7c1d364":{"id_":"83c1c117-cd47-4b16-ad60-23efa7c1d364","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_26","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VrllEHHzqOhiXlNnx1JPpnGrPBc7G768nb4PmIOYZSA="},"PREVIOUS":{"nodeId":"6a8ad5f7-f8c5-42b4-9d28-c0583650911e","metadata":{"page_number":26,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YDTH8R4ZL57QpDlE4to0qNLnrp58izJ8fKlrFw6dTWw="}},"text":"Note   that   this   is   very   similar   to   our   categorization   of\r\nforeground-background class imbalance methods in Section\r\n4.1,  which  also  drove  us  to  examine  metric  learning  in\r\ndetail. Owing to its own training, and resulting loss function\r\nconfiguration,  here  we  only  examine  sampling  strategies\r\nand generative methods. Sampling  Methodsaim to find a useful set of training\r\nexamples from a large training dataset. The usefulness crite-\r\nrion is considered to be highly relevant to the hardness of an\r\nexample. Unlike the methods in object detection, some of the\r\nmethods  avoid  using  the  hardest  possible  set  of  examples\r\nduring  traning. One  example  proposed  by  Schroff  et  al. [164] use a rule based on Euclidean distance to define “semi-\r\nhard” triplets since selecting the hardest triplets can end up\r\nin local minima during the early stages of the training. This\r\nway, they decrease the effect of confusing triplets and avoid\r\nrepeating  the  same  hardest  examples. Another  approach\r\nthat avoids considering the hardest possible set of examples\r\nis  Hard-Aware  Deeply  Cascaded  Embedding  [167],  which\r\nproposes  training  a  cascaded  model  such  that  the  higher\r\nlayers are trained with harder examples while the first layers\r\nare trained with the entire dataset. Similarly, Smart Mining\r\n[168] also mines semi-hard examples exploiting the distance\r\nbetween nearest neighbor of anchor and the corresponding\r\nanchor  and  one  novelty  is  that  they  increase  the  hardness\r\nof  the  negative  examples  in  the  latter  epochs  adaptively.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"r0G2IHuwx2+DuMRA7qCKFoGCvKcUJAmAlHO5ULaaYvE="},"74920889-84c4-4861-8efd-fcb0a0083f99":{"id_":"74920889-84c4-4861-8efd-fcb0a0083f99","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZRFAxqN8j7slYB8MsXN9fl7hLmyBvf5nsri5GNEiZLQ="},"NEXT":{"nodeId":"087375d3-8e23-41b6-9bf0-a7a164c4330b","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"RLfej6wbWGSMNYzS3HnvSo1b2rqne5E2Xhm5+luRSCc="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW27\r\nNote that neither semi-hardness nor adaptive setting of the\r\nhardness level is considered by object detectors. As a different approach, Cui et al. [169] consider to ex-\r\nploit humans to label false positives during training, which\r\nare identified as hard examples and be added to the mini-\r\nbatch  for  the  next  iteration. Song  et  al. [170]  mine  hard\r\nnegatives  not  only  for  the  anchor  but  also  for  all  of  the\r\npositives. Note  that  no  object  detection  method  considers\r\nthe relation between all positives and the negative example\r\nwhile  assigning  a  hardness  value  to  a  negative  example. One  promising  idea  shown  by  Huang  et  al. [171]  is  that\r\nlarger  intra-class  distances  can  confuse  the  hard  example\r\nmining process while only inter-class distance is considered\r\nduring mining. For this reason, a position-dependent deep\r\nmetric unit is proposed to take into account the intra-class\r\nvariations. Similar  to  the  generative  methods  for  object  detection\r\n(Section  4.1.4),Generative  Methodshave  been  used  for\r\ngenerating examples or features for metric learning as well. Deep   Adversarial   Metric  Learning   [172]  simultaneously\r\nlearns and embedding and a generator. Here, the generator\r\noutputs  a  synthetic  hard  negative  example  given  the  orig-\r\ninal  triplet. Similar  to  Tripathi  et  al. [64],  Zhao  et  al. [173]\r\nalso  use  a  GAN  [174]  in  order  to  generate  not  only  hard\r\nnegatives but also hard positives. The idea to consider inter-\r\nclass similarity have proven well as in the work by Huang\r\net al. [171]. Finally, a different approach from the previous\r\ngenerative models, Hardness Aware Metric Learning [175],\r\naims to learn an autoencoder in the feature space.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"qZFTcIWMGZmK1L0ZsrBsg4XZUvF7dff6c0s0go9os/g="},"087375d3-8e23-41b6-9bf0-a7a164c4330b":{"id_":"087375d3-8e23-41b6-9bf0-a7a164c4330b","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZRFAxqN8j7slYB8MsXN9fl7hLmyBvf5nsri5GNEiZLQ="},"PREVIOUS":{"nodeId":"74920889-84c4-4861-8efd-fcb0a0083f99","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"qZFTcIWMGZmK1L0ZsrBsg4XZUvF7dff6c0s0go9os/g="},"NEXT":{"nodeId":"27a86009-8156-4c9b-a205-c5a175954fc2","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"8Ir6ICpv8ln026EaDnPcZ3bIDfFpMOCbNMyyLz+DiyE="}},"text":"The idea\r\nis as follows: The authors first manipulate the features after\r\nthe backbone such that the hardness of the example can be\r\ncontrolled by linearly interpolating the embedding towards\r\nthe  anchor  by  employing  a  coefficient  relying  on  the  loss\r\nvalues  at  the  last  epoch. Since  it  is  not  certain  that  the\r\ninterpolated embedding preserves the original label, a label\r\npreserving mapping back to the feature space is employed\r\nusing  the  autoencoder. Also,  similar  to  Harwood  et  al. [168],  the  hardness  of  the  examples  in  the  latter  epochs  is\r\nincreased. Comparative  Summary.Looking  at  the  studies  presented\r\nabove, we observe that the metric learning methods are able\r\nto learn an embedding of the data that preserves the desired\r\nsimilarity between data samples. Object detection literature\r\nhave  used  different  measures  and  metrics  that  have  been\r\ndesigned  by  humans. However,  as  shown  by  the  metric\r\nlearning community, a metric that is directly learned from\r\nthe data itself can yield better results and have interesting\r\nproperties. Moreover,  the  self-paced  learning,  where  the\r\nhardness  levels  of  the  examples  is  increased  adaptively,  is\r\ndefinitely an important concept for addressing imbalance in\r\nobject  detection. Another  idea  that  can  be  adopted  by  the\r\nobject  detectors  is  to  label  the  examples  by  humans  in  an\r\nonline  manner  (similar  to  the  work  by  Yao  [176])  during\r\ntraining and to use the semi-hardness concept. 8.3    Multi-Task Learning\r\nMulti-task  learning  involves  learning  multiple  tasks  (with\r\npotentially  conflicting  objectives)  simultaneously. A  com-\r\nmon  approach  is  to  weigh  the  objectives  of  the  tasks  to\r\nbalance them. Ground Truth\r\nPositive Example\r\n(a)(b)(c)\r\nNegative Example\r\nFig. 18: An example suggesting the necessity of considering\r\ndifferent imbalance problems together in a unified manner.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"RLfej6wbWGSMNYzS3HnvSo1b2rqne5E2Xhm5+luRSCc="},"27a86009-8156-4c9b-a205-c5a175954fc2":{"id_":"27a86009-8156-4c9b-a205-c5a175954fc2","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZRFAxqN8j7slYB8MsXN9fl7hLmyBvf5nsri5GNEiZLQ="},"PREVIOUS":{"nodeId":"087375d3-8e23-41b6-9bf0-a7a164c4330b","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"RLfej6wbWGSMNYzS3HnvSo1b2rqne5E2Xhm5+luRSCc="},"NEXT":{"nodeId":"c8d18e91-01ad-4c27-93b7-c283a97214fc","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6c9t2CnecWauwP36vf0iAHa7wHeXULbjLso3Wgqudtc="}},"text":"18: An example suggesting the necessity of considering\r\ndifferent imbalance problems together in a unified manner. Blue,  green  and  red  colors  indicate  ground-truth,  positive\r\nexample  (prediction)  and  negative  examples,  respectively. The  larger  example  (i.e. the  one  with  higher  IoU  with  the\r\nblue box) in (a) is shifted right. In (b), its IoU is significantly\r\ndecreased  and  it  eventually  becomes  a  negative  example\r\nin  (c). This  example  shows  the  interplay  between  class\r\nimbalance, scale imbalance, spatial imbalance and objective\r\nimbalance by a change in BB positions. Many  methods  have  been  proposed  for  assigning  the\r\nweights in a more systematic manner. For example, Li et al. [177]  extended  the  self-paced  learning  paradigm  to  multi-\r\ntask  learning  based  on  a  novel  regularizer. The  hyperpa-\r\nrameters in the proposed regularizer control the hardness of\r\nnot  only  the  instances  but  also  the  tasks,  and  accordingly,\r\nthe  hardness  level  is  increased  during  training. In  another\r\nwork  motivated  by  the  self-paced  learning  approach,  Guo\r\net  al. [135]  use  more  diverse  set  of  tasks,  including  ob-\r\nject detection. Their method weighs the losses dynamically\r\nbased  on  the  exponential  moving  average  of  a  predefined\r\nkey performance indicator (e.g. accuracy, average precision)\r\nfor each task. Similar to image classification, one can also use\r\nthe uncertainty of the estimations [178] or their loss values\r\n[179] to assign weights to the tasks. In addition to the importance or hardness of tasks, Zhao\r\nChen and Rabinovich [180] identified the importance of the\r\npace at which the tasks are learned. They suggested that the\r\ntasks  are  required  to  be  trained  in  a  similar  pace.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"8Ir6ICpv8ln026EaDnPcZ3bIDfFpMOCbNMyyLz+DiyE="},"c8d18e91-01ad-4c27-93b7-c283a97214fc":{"id_":"c8d18e91-01ad-4c27-93b7-c283a97214fc","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_27","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"ZRFAxqN8j7slYB8MsXN9fl7hLmyBvf5nsri5GNEiZLQ="},"PREVIOUS":{"nodeId":"27a86009-8156-4c9b-a205-c5a175954fc2","metadata":{"page_number":27,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"8Ir6ICpv8ln026EaDnPcZ3bIDfFpMOCbNMyyLz+DiyE="}},"text":"For  this\r\nend, they proposed balancing the training pace of different\r\ntasks  by  adjusting  their  weights  dynamically  based  on  a\r\nnormalization algorithm motivated by batch normalization\r\n[117]. Comparative Summary.Being a multi-task problem, object de-\r\ntection can benefit significantly from the multi-task learning\r\napproaches. However, this aspect of object detectors has not\r\nreceived attention from the community. 9    OPENISSUES FORALLIMBALANCEPROBLEMS\r\nIn this section, we identify and discuss the issues relevant\r\nto  all  imbalance  problems. For  open  issues  pertaining  to\r\na specific imbalance problem, please see its corresponding\r\nsection in the text. 9.1    A Unified Approach to Addressing Imbalance\r\nOpen Issue.One of the main challenges is to come up with a\r\nunified approach that addresses all imbalance problems by\r\nconsidering the inter-dependence between them.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"6c9t2CnecWauwP36vf0iAHa7wHeXULbjLso3Wgqudtc="},"0e62c1f6-ded2-4813-b054-63ed20142186":{"id_":"0e62c1f6-ded2-4813-b054-63ed20142186","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"7DksPq3+12k1951Ujk99WtV2W8SZttngsdmNLYzk+AQ="},"NEXT":{"nodeId":"2f33cc23-9ec6-4bb5-bb5d-7b58bdaf7163","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Iza05KaDOQcFy5ayNsXySCYPtBpM7HEsOeQ284lI8FU="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW28\r\nExplanation.We illustrate this inter-dependency using a toy\r\nexample  in  Figure  18. In  this  figure,  we  shift  an  input\r\nbounding  box with  a  high IoU  (see Figure  18(a))  to worse\r\nqualities  in  terms  of  IoU  in  two  steps  (see  Figure  18(b-c))\r\nand  observe  how  this  shift  affects  the  different  imbalance\r\nproblems. For  the  base  case  in  Figure  18(a),  there  are  two\r\npositive bounding boxes (relevant for class imbalance) with\r\ndifferent  scales  (relevant  for  scale  imbalance),  loss  values\r\n(relevant for objective imbalance) and IoUs (relevant for BB\r\nimbalance). Shifting  the  box  to  the  right,  we  observe  the\r\nfollowing:\r\n\u000fIn  Figure  18(b),  we  still  have  two  positives,  both\r\nof  which  now  have  less  IoU  with  the  ground  truth\r\n(compared to (a)). This leads to the following: (i) There are more hard\r\nexamples  (considering  hard  example  mining  [24],\r\n[29]),  and  less  prime  samples  [30]. For  this  reason,\r\nthe methods for class imbalance are affected. (ii) The\r\nscales  of  the  RoIs  and  ground  truth  do  not  change. Considering  this,  the  scale  imbalance  seems  not  af-\r\nfected. (iii)  Objective  imbalance  is  affected  in  two\r\nways: Firstly, the shifted BB will incur more loss (for\r\nregression, and possibly for classification) and thus,\r\nbecome  more  dominant  in  its  own  task. Secondly,\r\nsince the cumulative individual loss values change,\r\nthe  contribution  to  the  total  loss  of  the  individual\r\nloss values will also change, which implies its effect\r\non  objective  imbalance.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"YF3+XHJiFpxFR36ju2flDfTTvTq+1QXseF4ffG40A1A="},"2f33cc23-9ec6-4bb5-bb5d-7b58bdaf7163":{"id_":"2f33cc23-9ec6-4bb5-bb5d-7b58bdaf7163","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"7DksPq3+12k1951Ujk99WtV2W8SZttngsdmNLYzk+AQ="},"PREVIOUS":{"nodeId":"0e62c1f6-ded2-4813-b054-63ed20142186","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YF3+XHJiFpxFR36ju2flDfTTvTq+1QXseF4ffG40A1A="},"NEXT":{"nodeId":"9c9eaba0-71c9-4044-8cb7-29c40abf057b","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Xc3JrHA/QXdfGTtFK2EGcedburlPYMx0P2sNme1aYww="}},"text":"(iv)  Finally,  both  BB  IoU\r\ndistribution  and  spatial  distribution  of  the  positive\r\nexamples will be affected by this shift. \u000fIn Figure 18(c), by applying a small shift to the same\r\nBB, its label changes. This leads to the following: (i) There are less positive\r\nexamples and more negative examples. The ground\r\ntruth class loses an example. Note that this example\r\nevolves from being a hard positive to a hard negative\r\nin terms hard example mining [24], [29], and the cri-\r\nterion that the prime sample attention [30] considers\r\nfor  the  example  changed  from  IoU  to  classification\r\nscore. For  this  reason,  in  this  case,  the  methods  in-\r\nvolving class imbalance and foreground-foreground\r\nclass  imbalance  are  affected. (ii)  A  positive  RoI  is\r\nremoved  from  the  set  of  RoIs  with  similar  scales. Therefore, there will be less positive examples with\r\nthe  same  scale,  which  affects  scale  imbalance. (iii)\r\nObjective imbalance is affected in two ways: Firstly,\r\nthe now-negative BB is an additional hard example\r\nin terms of classification possibly with a larger loss\r\nvalue. Secondly,  the  shifted  example  is  totally  free\r\nfrom  the  regression  branch,  and  moved  to  the  set\r\nof  hard  examples  in  terms  of  classification. Hence,\r\nthe  contribution  of  the  regression  loss  to  the  total\r\nloss  is  expected  to  decrease,  and  the  contribution\r\nof classification would increase. (iv) Finally, the IoU\r\ndistribution of the positive examples will be affected\r\nby this shift since a positive example is lost. Therefore, the aforementioned imbalance problems have\r\nan intertwined nature, which needs to be investigated and\r\nidentified in detail to effectively address all imbalance prob-\r\nlems. 9.2    Measuring and Identifying Imbalance\r\nOpen  Issue.Another  critical  issue  that  has  not  been  ad-\r\ndressed yet is how to quantify or measure imbalance, and\r\nhow  to  identify  imbalance  when  there  is  one.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Iza05KaDOQcFy5ayNsXySCYPtBpM7HEsOeQ284lI8FU="},"9c9eaba0-71c9-4044-8cb7-29c40abf057b":{"id_":"9c9eaba0-71c9-4044-8cb7-29c40abf057b","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"7DksPq3+12k1951Ujk99WtV2W8SZttngsdmNLYzk+AQ="},"PREVIOUS":{"nodeId":"2f33cc23-9ec6-4bb5-bb5d-7b58bdaf7163","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Iza05KaDOQcFy5ayNsXySCYPtBpM7HEsOeQ284lI8FU="},"NEXT":{"nodeId":"f6c07be3-c077-4f21-af37-ae1cc99c8eb6","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"EMedKIsluSUClIC0nc7mXuUhhgsoIyByLkJ5Loxkfes="}},"text":"We  identify\r\nthree questions that need to be studied:\r\n1)What  is  a  balanced  distribution  for  a  property  that  is\r\ncritical  for  a  task?This  is  likely  to  be  uniform  dis-\r\ntribution for many properties like, e.g. class distri-\r\nbution. However, different modalities may imply a\r\ndifferent  concept  of  balance. For  example,  OHEM\r\nprefers a skewed distribution around0:5for the IoU\r\ndistribution; left-skewed for the positives and right-\r\nskewed for the negatives. 2)What is the desired distribution for the properties that are\r\ncritical for a task?Note that the desired distribution\r\nmay  be  different  from  the  balanced  distribution\r\nsince  skewing  the  distribution  in  one  way  may  be\r\nbeneficial for faster convergence and better general-\r\nization. For  example,  online  hard  negative  mining\r\n[24] favors a right-skewed IoU distribution towards\r\n0.5 [29], whereas prime sample attention prefers the\r\npositive  examples  with  larger  IoUs  [30]  and  the\r\nclass  imbalance  methods  aim  to  ensure  a  uniform\r\ndistribution from the classes. 3)How can we quantify how imbalanced a distribution is? A  straightforward  approach  is  to  consider  optimal\r\ntransport  measures  such  as  the  Wasserstein  dis-\r\ntance; however, such methods would neglect the ef-\r\nfect of a unit change (imbalance) in the distribution\r\non the overall performance, thereby jeopardizing a\r\ndirect and effective consideration (and comparison)\r\nof the imbalance problems using the imbalance mea-\r\nsurements. 9.3    Labeling a Bounding Box as Positive or Negative\r\nOpen Issue.Currently, object detectors use IoU-based thresh-\r\nolding  (possibly  with  different  values)  for  labeling  an  ex-\r\nample as positive or negative and there is no consensus on\r\nthis  (i.e. Fast  R-CNN  [17],  Retina  Net  [22]  and  RPN  [21]\r\nlabel  input  BBs  with  IoUs0:5,0:4and0:3as  negatives\r\nrespectively.).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Xc3JrHA/QXdfGTtFK2EGcedburlPYMx0P2sNme1aYww="},"f6c07be3-c077-4f21-af37-ae1cc99c8eb6":{"id_":"f6c07be3-c077-4f21-af37-ae1cc99c8eb6","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_28","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"7DksPq3+12k1951Ujk99WtV2W8SZttngsdmNLYzk+AQ="},"PREVIOUS":{"nodeId":"9c9eaba0-71c9-4044-8cb7-29c40abf057b","metadata":{"page_number":28,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Xc3JrHA/QXdfGTtFK2EGcedburlPYMx0P2sNme1aYww="}},"text":"However, a consensus on this is critical since\r\nlabeling is very relevant to determining whether an example\r\nis a hard example. Explanation.Labeling bounding boxes is highly relevant to\r\nimbalance  problems  since  this  is  the  step  where  the  set\r\nof  all  bounding  boxes  are  split  as  positives  and  negatives\r\nin  an  online  manner. Of  special  concern  are  the  bounding\r\nboxes  around  the  decision  boundary,  which  are  typically\r\nconsidered  as  hard  examples,  and  a  noisy  labeling  over\r\nthem would result in large gradients in opposite directions. In other words, in order to define the hard negatives reliably,\r\nthe number of outliers should be as small as possible. For\r\nthis reason, consistent labeling of the input bounding boxes\r\nas  positive  or  negative  is  a  prerequisite  of  the  imbalance\r\nproblems in object detection. Currently the methods use a hard IoU threshold (gener-\r\nally0:5) to split the examples; however, Li et al. [59] showed\r\nthat this scheme results in a large number of noisy examples. In  Figure  19,  we  illustrate  two  input  bounding  boxes  that","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EMedKIsluSUClIC0nc7mXuUhhgsoIyByLkJ5Loxkfes="},"14bb83f4-e798-4908-9513-b62f3094b235":{"id_":"14bb83f4-e798-4908-9513-b62f3094b235","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_29","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jg0UJsZCk9v+1CfRANYvgjLLwHjU+jKA8xYIbq1ksVY="},"NEXT":{"nodeId":"23910c62-2f73-4477-84d2-c8864d3c0e50","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gP9wDSHyb7doszqMMw1krNGCDUdXoIzSGfshNM++gn0="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW29\r\nFig. 19:  An  illustration  on  ambiguities  resulting  from  la-\r\nbeling  examples. The  blue  boxes  denote  the  ground  truth. The  green  boxes  are  the  estimated  positive  boxes  with\r\nIoU >0:5. The input image is from the MS COCO [90]. can be misleadingly labeled as positive; and once they are\r\nlabeled as positive, it is likely that they will be sampled as\r\nhard positives:\r\n\u000fThe  estimated  (positive)  box  for  the  bicycle  (green)\r\nhas two problems: It has occlusion (for the bicycle),\r\nand  a  big  portion  of  it  includes  another  object  (a\r\nperson). For this reason, during training, this is not\r\nonly a hard example for the bicycle class but also a\r\nmisleading example for the person class in that this\r\nspecific example will try to suppress the probability\r\nof this box to be classified as person. \u000fThe  estimated  (positive)  box  for  the  person  class\r\n(green) consists of black pixels in most of it. In other\r\nwords,  the  box  does  hardly  include  any  visually\r\ndescriptive part for a person. For this reason, this is a\r\nvery hard example which is likely to fail in capturing\r\nthe ground truth class well. 9.4    Imbalance in Bottom-Up Object Detectors\r\nOpen  Issue.Bottom-up  detectors  [23],  [48],  [49]  adopt  a\r\ncompletely  different  approach  to  object  detection  than  the\r\none-stage  and  the  two-stage  detectors  (see  Section  2.1). Bottom-up  detectors  might  share  many  of  the  imbalance\r\nproblems  seen  in  the  top-down  detectors,  and  they  may\r\nhave  their  own  imbalance  issues  as  well.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"y0Oq3qlYNBTkUmh4DqX8w8gPasjLXx6BXXlmeXimmw8="},"23910c62-2f73-4477-84d2-c8864d3c0e50":{"id_":"23910c62-2f73-4477-84d2-c8864d3c0e50","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_29","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jg0UJsZCk9v+1CfRANYvgjLLwHjU+jKA8xYIbq1ksVY="},"PREVIOUS":{"nodeId":"14bb83f4-e798-4908-9513-b62f3094b235","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"y0Oq3qlYNBTkUmh4DqX8w8gPasjLXx6BXXlmeXimmw8="},"NEXT":{"nodeId":"a6b36dbd-f0a0-44b0-8115-5f819c3a6e09","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"48m4YLKk3MfiudfNQEO6/mjFwdbfX6+KGG1gKcd+UYc="}},"text":"Further  research\r\nneeds to be conducted for (i) analyzing the known methods\r\naddressing  imbalance  problems  in  the  context  of  bottom-\r\nup  object  detectors,  and  (ii)  imbalance  problems  that  are\r\nspecific to bottom-up detectors. Explanation.Addressing imbalance issues in bottom-up ob-\r\nject  detectors  has  received  limited  attention. CornerNet\r\n[23]  and  ExtremeNet  [49]  use  focal  loss  [22]  to  address\r\nforeground-background class imbalance, and the hourglass\r\nnetwork [104] to compensate for the scale imbalance. On the\r\nother hand, use of hard sampling methods and the effects of\r\nother  imbalance  problems  have  not  been  investigated. For\r\nthe top-down detectors, we can recap some of the findings:\r\nfrom the class imbalance perspective, Shrivastava et al. [24]\r\nshow  that  the  examples  with  larger  losses  are  important;\r\nfrom the scale imbalance perspective, different architectures\r\n[29],  [75],  [81]  and  training  methods  [27],  [28]  involving\r\nfeature  and  image  pyramids  are  proven  to  be  useful  and\r\nfinally from the objective imbalance perspective, Pang et al. [29] showed that smoothL1loss underestimates the effect\r\nof  the  inliers. Research  is  needed  to  come  up  with  such\r\nfindings for bottom-up object detectors. 10    CONCLUSION\r\nIn  this  paper,  we  provided  a  thorough  review  of  the  im-\r\nbalance  problems  in  object  detection. In  order  to  provide\r\na  more  complete  and  coherent  perspective,  we  introduced\r\na  taxonomy  of  the  problems  as  well  as  the  solutions  for\r\naddressing them. Following the taxonomy on problems, we\r\ndiscussed each problem separately in detail and presented\r\nthe solutions with a unifying yet critical perspective. In  addition  to  the  detailed  discussions  on  the  studied\r\nproblems  and  the  solutions,  we  pinpointed  and  presented\r\nmany open issues and imbalance problems that are critical\r\nfor  object  detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"gP9wDSHyb7doszqMMw1krNGCDUdXoIzSGfshNM++gn0="},"a6b36dbd-f0a0-44b0-8115-5f819c3a6e09":{"id_":"a6b36dbd-f0a0-44b0-8115-5f819c3a6e09","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_29","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jg0UJsZCk9v+1CfRANYvgjLLwHjU+jKA8xYIbq1ksVY="},"PREVIOUS":{"nodeId":"23910c62-2f73-4477-84d2-c8864d3c0e50","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"gP9wDSHyb7doszqMMw1krNGCDUdXoIzSGfshNM++gn0="},"NEXT":{"nodeId":"fa58f010-57e6-42c4-9b58-5c361aab7dbe","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"A8lJCjoIwvihLe5tf45BelsGY++7memTjytDFnwLI6Q="}},"text":"In  addition  to  the  many  open  aspects\r\nthat need further attention for the studied imbalance prob-\r\nlems, we identified new imbalance issues that have not been\r\naddressed or discussed before. With  this  review  and  our  taxonomy  functioning  as  a\r\nmap,  we,  as  the  community,  can  identify  where  we  are\r\nand the research directions to be followed to develop better\r\nsolutions to the imbalance problems in object detection. ACKNOWLEDGMENTS\r\nThis  work  was  partially  supported  by  the  Scientific  and\r\nTechnological   Research   Council   of   Turkey   (TÜB ̇ITAK)\r\nthrough the project titled “Object Detection in Videos with\r\nDeep  Neural  Networks”  (grant  number  117E054). Kemal\r\nÖksüz is supported by the TÜB ̇ITAK 2211-A National Schol-\r\narship Programme for Ph.D. students. REFERENCES\r\n[1]      Q. Fan, L. Brown, and J. Smith, “A closer look at faster r-cnn forvehicle detection,” inIEEE Intelligent Vehicles Symposium, 2016. [2]      Z. Fu, Y. Chen, H. Yong, R. Jiang, L. Zhang, and X. Hua, “Fore-ground gating and background refining network for surveillance\r\nobject detection,”IEEE Transactions on Image Processing-Accepted,2019. [3]      A. Geiger,  P. Lenz,  and  R. Urtasun,  “Are  we  ready  for  au-tonomous driving? the kitti vision benchmark suite,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2012. [4]      X. Dai,  “Hybridnet:  A  fast  vehicle  detection  system  for  au-tonomous   driving,”Signal   Processing:   Image   Communication,\r\nvol. 70, pp. 79 – 88, 2019. [5]      P. F. Jaeger,  S. A. A. Kohl,  S. Bickelhaupt,  F.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"48m4YLKk3MfiudfNQEO6/mjFwdbfX6+KGG1gKcd+UYc="},"fa58f010-57e6-42c4-9b58-5c361aab7dbe":{"id_":"fa58f010-57e6-42c4-9b58-5c361aab7dbe","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_29","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jg0UJsZCk9v+1CfRANYvgjLLwHjU+jKA8xYIbq1ksVY="},"PREVIOUS":{"nodeId":"a6b36dbd-f0a0-44b0-8115-5f819c3a6e09","metadata":{"page_number":29,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"48m4YLKk3MfiudfNQEO6/mjFwdbfX6+KGG1gKcd+UYc="}},"text":"A. Kohl,  S. Bickelhaupt,  F. Isensee,  T. A.Kuder,  H. Schlemmer,  and  K. H. Maier-Hein,  “Retina  u-net:\r\nEmbarrassingly simple exploitation of segmentation supervisionfor medical object detection,”arXiv, vol. 1811.08661, 2018. [6]      S.-g. Lee,  J. S. Bae,  H. Kim,  J. H. Kim,  and  S. Yoon,  “Liverlesion  detection  from  weakly-labeled  multi-phase  ct  volumes\r\nwith a grouped single shot multibox detector,” inMedical ImageComputing  and  Computer  Assisted  Intervention  (MICCAI),  A. F. Frangi,  J. A. Schnabel,  C. Davatzikos,  C. Alberola-López,  andG. Fichtinger, Eds., 2018. [7]      M. Rad and V. Lepetit, “Bb8: A scalable, accurate, robust to partialocclusion  method  for  predicting  the  3d  poses  of  challenging\r\nobjects without using depth,” inThe IEEE International Conferenceon Computer Vision (ICCV), 2017. [8]      W. Kehl,  F. Manhardt,  F. Tombari,  S. Ilic,  and  N. Navab,  “Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great\r\nagain,”  inThe  IEEE  International  Conference  on  Computer  Vision(ICCV), 2017.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"A8lJCjoIwvihLe5tf45BelsGY++7memTjytDFnwLI6Q="},"b406d4b1-d3ad-4ab0-86f9-fe4fc416f7dc":{"id_":"b406d4b1-d3ad-4ab0-86f9-fe4fc416f7dc","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"NEXT":{"nodeId":"a0fc5577-b56e-4277-b6b5-dbd392e2f324","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"INGHVLv0kfSjiTG3+Yjrm3XPmeqoLk2a8peVd6Vb7vM="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW30\r\n[9]      B. Tekin, S. N. Sinha, and P. Fua, “Real-Time Seamless Single Shot6D Object Pose Prediction,” inThe IEEE Conference on Computer\r\nVision and Pattern Recognition (CVPR), 2018. [10]    T. Hodan,  F. Michel,  E. Brachmann,  W. Kehl,  A. GlentBuch,D. Kraft,   B. Drost,   J. Vidal,   S. Ihrke,   X. Zabulis,   C. Sahin,\r\nF. Manhardt,  F. Tombari,  T.-K. Kim,  J. Matas,  and  C. Rother,“Bop: Benchmark for 6d object pose estimation,” inThe European\r\nConference on Computer Vision (ECCV), 2018. [11]    G. Du, K. Wang, and S. Lian, “Vision-based robotic grasping fromobject  localization,  pose  estimation,  grasp  detection  to  motion\r\nplanning: A review,”arXiv, vol. 1905.06658, 2019. [12]    I. Bozcan and S. Kalkan, “Cosmo: Contextualized scene modelingwith boltzmann machines,”Robotics and Autonomous Systems, vol. 113, pp. 132–148, 2019. [13]    P. F. Felzenszwalb,  R. B. Girshick,  D. McAllester,  and  D. Ra-manan,  “Object  detection  with  discriminatively  trained  part-\r\nbased models,”IEEE Transactions on Pattern Analysis and MachineIntelligence, vol. 32, no. 9, pp. 1627–1645, 2010. [14]    A. Krizhevsky, I. Sutskever, and G. E.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"1aVB4z5jDe6QWjs24OgTOuYHkCoPgMFaBTE/uzVw1W4="},"a0fc5577-b56e-4277-b6b5-dbd392e2f324":{"id_":"a0fc5577-b56e-4277-b6b5-dbd392e2f324","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"b406d4b1-d3ad-4ab0-86f9-fe4fc416f7dc","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"1aVB4z5jDe6QWjs24OgTOuYHkCoPgMFaBTE/uzVw1W4="},"NEXT":{"nodeId":"e5df25a3-0a0b-41cc-ad4a-15ceb029fffa","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CyoN3WBC8vfBBZonogaQodk5GUMZZruPeBX8rCwV/xk="}},"text":"Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classifi-cation with deep convolutional neural networks,” inAdvances in\r\nNeural Information Processing Systems (NIPS), 2012. [15]    J. Redmon and A. Farhadi, “YOLO9000: Better, faster, stronger,”inThe IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR), 2017. [16]    R. Girshick,  J. Donahue,  T. Darrell,  and  J. Malik,  “Rich  featurehierarchies  for  accurate  object  detection  and  semantic  segmen-\r\ntation,”  inThe  IEEE  Conference  on  Computer  Vision  and  PatternRecognition (CVPR), 2014. [17]    R. Girshick,  “Fast  R-CNN,”  inThe  IEEE  International  Conferenceon Computer Vision (ICCV), 2015. [18]    J. Dai,  Y. Li,  K. He,  and  J. Sun,  “R-FCN:  Object  detection  viaregion-based fully convolutional networks,” inAdvances in Neural\r\nInformation Processing Systems (NIPS), 2016. [19]    W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, andA. C. Berg, “SSD: single shot multibox detector,” inThe European\r\nConference on Computer Vision (ECCV), 2016. [20]    J. Redmon,  S. K. Divvala,  R. B. Girshick,  and  A. Farhadi,  “Youonly look once: Unified, real-time object detection,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2016. [21]    S. Ren, K. He, R. Girshick, and J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"INGHVLv0kfSjiTG3+Yjrm3XPmeqoLk2a8peVd6Vb7vM="},"e5df25a3-0a0b-41cc-ad4a-15ceb029fffa":{"id_":"e5df25a3-0a0b-41cc-ad4a-15ceb029fffa","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"a0fc5577-b56e-4277-b6b5-dbd392e2f324","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"INGHVLv0kfSjiTG3+Yjrm3XPmeqoLk2a8peVd6Vb7vM="},"NEXT":{"nodeId":"1cddb916-d0cc-4c50-9f6d-1596a7fd731c","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"wv7WpAHoZfx/fWIZBgynT25FlepTcWQTXF9pfBeBHYo="}},"text":"Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towardsreal-time object detection with region proposal networks,”IEEE\r\nTransactions  on  Pattern  Analysis  and  Machine  Intelligence,  vol. 39,no. 6, pp. 1137–1149, 2017. [22]    T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollár, “Focal lossfor dense object detection,” inThe IEEE International Conference on\r\nComputer Vision (ICCV), 2017. [23]    H. Law  and  J. Deng,  “Cornernet:  Detecting  objects  as  pairedkeypoints,” inThe European Conference on Computer Vision (ECCV),\r\n2018. [24]    A. Shrivastava,  A. Gupta,  and  R. Girshick,  “Training  region-based  object  detectors  with  online  hard  example  mining,”  in\r\nThe  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition(CVPR), 2016. [25]    W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in finetun-ing deep model for object detection with long-tail distribution,”\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2016. [26]    T. Lin,  P. Dollár,  R. B. Girshick,  K. He,  B. Hariharan,  and  S. J.Belongie,  “Feature  pyramid  networks  for  object  detection,”  in\r\nThe  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition(CVPR), 2017. [27]    B. Singh and L. S. Davis, “An analysis of scale invariance in objectdetection - snip,” inThe Conference on Computer Vision and Pattern\r\nRecognition (CVPR), 2018. [28]    B. Singh, M.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CyoN3WBC8vfBBZonogaQodk5GUMZZruPeBX8rCwV/xk="},"1cddb916-d0cc-4c50-9f6d-1596a7fd731c":{"id_":"1cddb916-d0cc-4c50-9f6d-1596a7fd731c","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"e5df25a3-0a0b-41cc-ad4a-15ceb029fffa","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CyoN3WBC8vfBBZonogaQodk5GUMZZruPeBX8rCwV/xk="},"NEXT":{"nodeId":"ecd02b84-35b3-4454-b0a4-0b2e5aeab819","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"JZtoAwACUBrcaz9l5NwKjeKDvw4e2TqtlIWJjUZqwQM="}},"text":"[28]    B. Singh, M. Najibi, and L. S. Davis, “Sniper: Efficient multi-scaletraining,”  inAdvances  in  Neural  Information  Processing  Systems\r\n(NIPS), 2018. [29]    J. Pang,  K. Chen,  J. Shi,  H. Feng,  W. Ouyang,  and  D. Lin,“Libra R-CNN: Towards balanced learning for object detection,”\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019. [30]    Y. Cao, K. Chen, C. C. Loy, and D. Lin, “Prime Sample Attentionin Object Detection,”arXiv, vol. 1904.04821, 2019. [31]    L. Liu, W. Ouyang, X. Wang, P. W. Fieguth, J. Chen, X. Liu, andM. Pietikäinen,  “Deep  learning  for  generic  object  detection:  A\r\nsurvey,”arXiv, vol. 1809.02165, 2018. [32]    Z. Zou, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: Asurvey,”arXiv, vol. 1905.05055, 2018. [33]    S. Agarwal, J. O. D. Terrail, and F. Jurie, “Recent advances in ob-ject detection in the age of deep convolutional neural networks,”\r\narXiv, vol. 1809.03193, 2018. [34]    Zehang Sun, G. Bebis, and R. Miller, “On-road vehicle detection:a  review,”IEEE  Transactions  on  Pattern  Analysis  and  Machine\r\nIntelligence (TPAMI), vol. 28, no. 5, pp. 694–711, 2006. [35]    P. Dollar, C. Wojek, B. Schiele, and P.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wv7WpAHoZfx/fWIZBgynT25FlepTcWQTXF9pfBeBHYo="},"ecd02b84-35b3-4454-b0a4-0b2e5aeab819":{"id_":"ecd02b84-35b3-4454-b0a4-0b2e5aeab819","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"1cddb916-d0cc-4c50-9f6d-1596a7fd731c","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"wv7WpAHoZfx/fWIZBgynT25FlepTcWQTXF9pfBeBHYo="},"NEXT":{"nodeId":"eaeb6051-d08f-4bc4-9422-d63b9ce119bf","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YLJSP6EpVCifQe8juctymfs3S0GhA02fGhlN+zPwcPs="}},"text":"Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian detec-tion: An evaluation of the state of the art,”IEEE Transactions on\r\nPattern  Analysis  and  Machine  Intelligence  (TPAMI),  vol. 34,  no. 4,pp. 743–761, 2012. [36]    S. Zafeiriou, C. Zhang, and Z. Zhang, “A survey on face detectionin the wild,”Computer Vision and Image Understanding, vol. 138,\r\npp. 1–24, 2015. [37]    Q. Ye  and  D. Doermann,  “Text  detection  and  recognition  inimagery:  A  survey,”IEEE  Transactions  on  Pattern  Analysis  and\r\nMachine Intelligence, vol. 37, no. 7, pp. 1480–1500, 2015. [38]    X. Yin, Z. Zuo, S. Tian, and C. Liu, “Text detection, tracking andrecognition in video: A comprehensive survey,”IEEE Transactions\r\non Image Processing, vol. 25, no. 6, pp. 2752–2773, 2016. [39]    G. Litjens,  T. Kooi,  B. E. Bejnordi,  A. A. A. Setio,  F. Ciompi,M. Ghafoorian,  J. A. van  der  Laak,  B. van  Ginneken,  and  C. I. Sánchez, “A survey on deep learning in medical image analysis,”Medical Image Analysis, vol. 42, pp. 60 – 88, 2017. [40]    B. Krawczyk, “Learning from imbalanced data: open challengesand  future  directions,”Progress  in  Artificial  Intelligence,  vol. 5,\r\nno. 4, pp. 221–232, 2016. [41]    J. L. Leevy, T. M. Khoshgoftaar, R. A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"JZtoAwACUBrcaz9l5NwKjeKDvw4e2TqtlIWJjUZqwQM="},"eaeb6051-d08f-4bc4-9422-d63b9ce119bf":{"id_":"eaeb6051-d08f-4bc4-9422-d63b9ce119bf","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"ecd02b84-35b3-4454-b0a4-0b2e5aeab819","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"JZtoAwACUBrcaz9l5NwKjeKDvw4e2TqtlIWJjUZqwQM="},"NEXT":{"nodeId":"7fdead56-ed48-4cb2-895f-e0c9995a6531","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jlrc7vZy1/vUWiKzN2/T5QXcpMFwjKC79eW7yIub4K0="}},"text":"Leevy, T. M. Khoshgoftaar, R. A. Bauder, and N. Seliya, “Asurvey on addressing high-class imbalance in big data,”Journal\r\nof Big Data, vol. 5, no. 42, 2018. [42]    A. Fernández,  S. García,  M. Galar,  R. Prati,  B. Krawczyk,  andF. Herrera,Learning from Imbalanced Data Sets. Springer Interna-\r\ntional Publishing, 2018. [43]    J. M. Johnson, Khoshgoftaar, and T. M., “Survey on deep learningwith class imbalance,”Journal of Big Data, vol. 6, no. 21, 2019. [44]    J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M.Smeulders, “Selective search for object recognition,”International\r\nJournal of Computer Vision, vol. 104, no. 2, pp. 154–171, 2013. [45]    C. L. Zitnick and P. Dollár, “Edge boxes: Locating object propos-als  from  edges,”  inThe  European  Conference  on  Computer  Vision\r\n(ECCV), 2014. [46]    C.-Y. Fu,  W. Liu,  A. Ranga,  A. Tyagi,  and  A. C. Berg,  “DSSD:Deconvolutional  single  shot  detector,”arXiv,  vol. 1701.06659,\r\n2017. [47]    J. Redmon  and  A. Farhadi,  “Yolov3:  An  incremental  improve-ment,”arXiv, vol. 1804.02767, 2018. [48]    K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"YLJSP6EpVCifQe8juctymfs3S0GhA02fGhlN+zPwcPs="},"7fdead56-ed48-4cb2-895f-e0c9995a6531":{"id_":"7fdead56-ed48-4cb2-895f-e0c9995a6531","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"eaeb6051-d08f-4bc4-9422-d63b9ce119bf","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"YLJSP6EpVCifQe8juctymfs3S0GhA02fGhlN+zPwcPs="},"NEXT":{"nodeId":"33f30c22-5538-43ea-891e-6709eef136bb","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"SAjcaw5AfcDi9KaoPltTpwe4a/P2O73G8AkbsMOKjnU="}},"text":"Xie, H. Qi, Q. Huang, and Q. Tian, “Centernet:Keypoint triplets for object detection,” inThe IEEE International\r\nConference on Computer Vision (ICCV), 2019. [49]    X. Zhou, J. Zhuo, and P. Krahenbuhl, “Bottom-up object detectionby grouping extreme and center points,” inThe IEEE Conference\r\non Computer Vision and Pattern Recognition (CVPR), 2019. [50]    A. Newell, Z. Huang, and J. Deng, “Associative embedding: End-to-end learning for joint detection and grouping,” inAdvances in\r\nNeural Information Processing Systems (NIPS), 2017. [51]    M. Everingham,  L. Van  Gool,  C. K. I. Williams,  J. Winn,  andA. Zisserman, “The pascal visual object classes (voc) challenge,”\r\nInternational Journal of Computer Vision (IJCV), vol. 88, no. 2, pp.303–338, 2010. [52]    J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-geNet: A Large-Scale Hierarchical Image Database,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2009. [53]    Y. He, C. Zhu, J. Wang, M. Savvides, and X. Zhang, “Boundingbox  regression  with  uncertainty  for  accurate  object  detection,”\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019. [54]    J. Chen,  D. Liu,  T. Xu,  S. Zhang,  S. Wu,  B. Luo,  X. Peng,  andE. Chen, “Is sampling heuristics necessary in training deep object\r\ndetectors?”arXiv, vol. 1909.04868, 2019.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Jlrc7vZy1/vUWiKzN2/T5QXcpMFwjKC79eW7yIub4K0="},"33f30c22-5538-43ea-891e-6709eef136bb":{"id_":"33f30c22-5538-43ea-891e-6709eef136bb","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_30","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"6edsGJifpFasB5Vj14jr2r6Q90qV6il8LhDp0GwIJsE="},"PREVIOUS":{"nodeId":"7fdead56-ed48-4cb2-895f-e0c9995a6531","metadata":{"page_number":30,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"Jlrc7vZy1/vUWiKzN2/T5QXcpMFwjKC79eW7yIub4K0="}},"text":"1909.04868, 2019. [55]    H. A. Rowley, S. Baluja, and T. Kanade, “Human face detectionin visual scenes,” inThe Advances in Neural Information Processing\r\nSystems (NIPS), 1995. [56]    T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen, “Ron: Reverse\r\nconnection with objectness prior networks for object detection,”","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"SAjcaw5AfcDi9KaoPltTpwe4a/P2O73G8AkbsMOKjnU="},"d46035b9-c793-412f-bcef-0b0ef92189bb":{"id_":"d46035b9-c793-412f-bcef-0b0ef92189bb","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"NEXT":{"nodeId":"079fbce5-bb0d-4b2c-a64f-12a28c41bda8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"au+i4Ax4rbmizXPmlzn4usqLN3Wf7JBcBaWEp50L2D8="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW31\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2017. [57]    S. Zhang,  L. Wen,  X. Bian,  Z. Lei,  and  S. Z. Li,  “Single-shotrefinement  neural  network  for  object  detection,”  inThe  IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2018. [58]    J. Nie,  R. M. Anwer,  H. Cholakkal,  F. S. Khan,  Y. Pang,  andL. Shao, “Enriched feature guided refinement network for object\r\ndetection,” inThe IEEE International Conference on Computer Vision(ICCV), 2019. [59]    B. Li,  Y. Liu,  and  X. Wang,  “Gradient  harmonized  single-stagedetector,” inAAAI Conference on Artificial Intelligence, 2019. [60]    J. Chen, D. Liu, B. Luo, X. Peng, T. Xu, and E. Chen, “Residualobjectness for imbalance reduction,”arXiv, vol. 1908.09075, 2019. [61]    K. Chen, J. Li, W. Lin, J. See, J. Wang, L. Duan, Z. Chen, C. He,and  J. Zou,  “Towards  accurate  one-stage  object  detection  with\r\nap-loss,”  inThe  IEEE  Conference  on  Computer  Vision  and  PatternRecognition (CVPR), 2019. [62]    Q. Qian, L. Chen, H. Li, and R. Jin, “DR Loss: Improving ObjectDetection  by  Distributional  Ranking,”arXiv,  vol. 1907.10156,\r\n2019. [63]    X. Wang, A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"y9fXvEoS3G1voBAib996hDUNh4eSZ2SlBSDGL8vEeE8="},"079fbce5-bb0d-4b2c-a64f-12a28c41bda8":{"id_":"079fbce5-bb0d-4b2c-a64f-12a28c41bda8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"d46035b9-c793-412f-bcef-0b0ef92189bb","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"y9fXvEoS3G1voBAib996hDUNh4eSZ2SlBSDGL8vEeE8="},"NEXT":{"nodeId":"3a235b69-2ce8-44f7-aebe-9898f85dd1a8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"HYdCuyXAeGZRVgpyiT7H7WaU4dLKUZB1Gh+0XbegLOU="}},"text":"[63]    X. Wang, A. Shrivastava, and A. Gupta, “A-fast-rcnn: Hard pos-itive generation via adversary for object detection,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2017. [64]    S. Tripathi,  S. Chandra,  A. Agrawal,  A. Tyagi,  J. M. Rehg,  andV. Chari, “Learning to generate synthetic data via compositing,”\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2019. [65]    H. Wang,  Q. Wang,  F. Yang,  W. Zhang,  and  W. Zuo,  “Dataaugmentation  for  object  detection  via  progressive  and  selective\r\ninstance-switching,”arXiv, vol. 1906.00358, 2019. [66]    K. Oksuz, B. C. Cam, S. Kalkan, and E. Akbas, “Generating posi-tive bounding boxes for balanced training of object detectors,” in\r\nIEEE Winter Applications on Computer Vision (WACV), 2020. [67]    J. Wang, K. Chen, S. Yang, C. C. Loy, and D. Lin, “Region proposalby guided anchoring,” inThe IEEE Conference on Computer Vision\r\nand Pattern Recognition (CVPR), 2019. [68]    X. Zhang, F. Wan, C. Liu, R. Ji, and Q. Ye, “Freeanchor: Learningto  match  anchors  for  visual  object  detection,”  inAdvances  in\r\nNeural Information Processing Systems (NIPS), 2019. [69]    F. Yang,  W. Choi,  and  Y.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"au+i4Ax4rbmizXPmlzn4usqLN3Wf7JBcBaWEp50L2D8="},"3a235b69-2ce8-44f7-aebe-9898f85dd1a8":{"id_":"3a235b69-2ce8-44f7-aebe-9898f85dd1a8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"079fbce5-bb0d-4b2c-a64f-12a28c41bda8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"au+i4Ax4rbmizXPmlzn4usqLN3Wf7JBcBaWEp50L2D8="},"NEXT":{"nodeId":"3cf5d84b-6ee0-42ff-9ba2-7af2b349acc9","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rqz1GE0LjHz5Bzx6gjn0RwM2KNwWyGLdTWDbgaFl5sI="}},"text":"Yang,  W. Choi,  and  Y. Lin,  “Exploit  all  the  layers:  Fast  andaccurate  cnn  object  detector  with  scale  dependent  pooling  and\r\ncascaded rejection classifiers,” inThe IEEE Conference on ComputerVision and Pattern Recognition (CVPR), 2016. [70]    Z. Cai,  Q. Fan,  R. Feris,  and  N. Vasconcelos,  “A  unified  multi-scale  deep  convolutional  neural  network  for  fast  object  detec-\r\ntion,” inThe European Conference on Computer Vision (ECCV), 2016. [71]    J. Li, X. Liang, S. Shen, T. Xu, J. Feng, and S. Yan, “Scale-aware fastr-cnn for pedestrian detection,”IEEE Transactions on Multimedia,\r\nvol. 20, no. 4, pp. 985–996, 2018. [72]    Y. Pang, T. Wang, R. M. Anwer, F. S. Khan, and L. Shao, “Efficientfeaturized  image  pyramid  network  for  single  shot  detector,”  in\r\nThe  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition(CVPR), 2019. [73]    J. Noh,  W. Bae,  W. Lee,  J. Seo,  and  G. Kim,  “Better  to  follow,follow to be better: Towards precise supervision of feature super-\r\nresolution  for  small  object  detection,”  inThe  IEEE  InternationalConference on Computer Vision (ICCV), 2019. [74]    Y. Li, Y. Chen, N. Wang, and Z. Zhang, “Scale-aware trident net-works for object detection,” inThe IEEE International Conference\r\non Computer Vision (ICCV), 2019. [75]    S. Liu, L. Qi, H. Qin, J. Shi, and J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HYdCuyXAeGZRVgpyiT7H7WaU4dLKUZB1Gh+0XbegLOU="},"3cf5d84b-6ee0-42ff-9ba2-7af2b349acc9":{"id_":"3cf5d84b-6ee0-42ff-9ba2-7af2b349acc9","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"3a235b69-2ce8-44f7-aebe-9898f85dd1a8","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"HYdCuyXAeGZRVgpyiT7H7WaU4dLKUZB1Gh+0XbegLOU="},"NEXT":{"nodeId":"d5a6a78e-541d-4b60-98b1-8555485dde4f","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"cWyVZ//dHXQLEbpHgJkFh9SSkG57Qp9CyO1+K+Nb8yc="}},"text":"Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation networkfor  instance  segmentation,”  inThe  IEEE  Conference  on  Computer\r\nVision and Pattern Recognition (CVPR), 2018. [76]    P. Zhou,  B. Ni,  C. Geng,  J. Hu,  and  Y. Xu,  “Scale-transferrableobject detection,” inThe IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR), 2018. [77]    S.-W. Kim,  H.-K. Kook,  J.-Y. Sun,  M.-C. Kang,  and  S.-J. Ko,“Parallel  feature  pyramid  network  for  object  detection,”  inThe\r\nEuropean Conference on Computer Vision (ECCV), 2018. [78]    T. Kong, F. Sun, W. Huang, and H. Liu, “Deep feature pyramidreconfiguration for object detection,” inThe European Conference\r\non Computer Vision (ECCV), 2018. [79]    H. Li,  Y. Liu,  W. Ouyang,  and  X. Wang,  “Zoom  out-and-innetwork  with  map  attention  decision  for  region  proposal  and\r\nobject detection,”International Journal of Computer Vision, vol. 127,no. 3, pp. 225–238, 2019. [80]    Q. Zhao, T. Sheng, Y. Wang, Z. Tang, Y. Chen, L. Cai, and H. Ling,“M2det: A single-shot object detector based on multi-level feature\r\npyramid  network,”  inAAAI  Conference  on  Artificial  Intelligence,2019. [81]    G. Ghiasi,  T. Lin,  R. Pang,  and  Q. V.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"rqz1GE0LjHz5Bzx6gjn0RwM2KNwWyGLdTWDbgaFl5sI="},"d5a6a78e-541d-4b60-98b1-8555485dde4f":{"id_":"d5a6a78e-541d-4b60-98b1-8555485dde4f","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"3cf5d84b-6ee0-42ff-9ba2-7af2b349acc9","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"rqz1GE0LjHz5Bzx6gjn0RwM2KNwWyGLdTWDbgaFl5sI="},"NEXT":{"nodeId":"6b9b0982-5baf-4e1d-a0cc-09e539249c08","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"P/8WC967n47/eAs1M/LNmLPs9NZV4x7RQQBXS4gQTz0="}},"text":"Lin,  R. Pang,  and  Q. V. Le,  “NAS-FPN:  learningscalable  feature  pyramid  architecture  for  object  detection,”  in\r\nThe  IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition(CVPR), 2019. [82]    H. Xu,  L. Yao,  W. Zhang,  X. Liang,  and  Z. Li,  “Auto-fpn:Automatic  network  architecture  adaptation  for  object  detection\r\nbeyond  classification,”  inThe  IEEE  International  Conference  onComputer Vision (ICCV), 2019. [83]    J. Yu,  Y. Jiang,  Z. Wang,  Z. Cao,  and  T. Huang,  “Unitbox:  Anadvanced  object  detection  network,”  inThe  ACM  International\r\nConference on Multimedia, 2016. [84]    L. Tychsen-Smith  and  L. Petersson,  “Improving  object  local-ization  with  fitness  nms  and  bounded  iou  loss,”  inThe  IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2018. [85]    H. Rezatofighi,  N. Tsoi,  J. Gwak,  A. Sadeghian,  I. Reid,  andS. Savarese, “Generalized intersection over union: A metric and\r\na  loss  for  bounding  box  regression,”  inThe  IEEE  Conference  onComputer Vision and Pattern Recognition (CVPR), 2019. [86]    Z. Zheng, P. Wang, W. Liu, J. Li, Y. Rongguang, and R. Dongwei,“Distance-iou loss: Faster and better learning for bounding box\r\nregression,” inAAAI Conference on Artificial Intelligence, 2020. [87]    Z. Cai and N.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"cWyVZ//dHXQLEbpHgJkFh9SSkG57Qp9CyO1+K+Nb8yc="},"6b9b0982-5baf-4e1d-a0cc-09e539249c08":{"id_":"6b9b0982-5baf-4e1d-a0cc-09e539249c08","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"d5a6a78e-541d-4b60-98b1-8555485dde4f","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"cWyVZ//dHXQLEbpHgJkFh9SSkG57Qp9CyO1+K+Nb8yc="},"NEXT":{"nodeId":"a5a87ee9-bc1b-4cb2-9af4-ca592fd2998d","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"4XTFy54Z2BgIyikwLX4cF9kzGRIbbN4yCZ9PhIkOMbY="}},"text":"[87]    Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into highquality  object  detection,”  inThe  IEEE  Conference  on  Computer\r\nVision and Pattern Recognition (CVPR), 2018. [88]    J. Cao, Y. Pang, J. Han, and X. Li, “Hierarchical shot detector,” inThe IEEE International Conference on Computer Vision (ICCV), 2019. [89]    Z. Li,  Z. Xie,  L. Liu,  B. Tao,  and  W. Tao,  “Iou-uniform  r-cnn:Breaking through the limitations of rpn,”arXiv, vol. 1912.05190,\r\n2019. [90]    T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,P. Dollár,  and  C. L. Zitnick,  “Microsoft  COCO:  Common  Ob-\r\njects  in  Context,”  inThe  European  Conference  on  Computer  Vision(ECCV), 2014. [91]    A. Kuznetsova,  H. Rom,  N. Alldrin,  J. R. R. Uijlings,  I. Krasin,J. Pont-Tuset,  S. Kamali,  S. Popov,  M. Malloci,  T. Duerig,  and\r\nV. Ferrari, “The open images dataset V4: unified image classifica-tion, object detection, and visual relationship detection at scale,”\r\narXiv, vol. 1811.00982, 2018. [92]    S. Shao,  Z. Li,  T. Zhang,  C. Peng,  G. Yu,  X. Zhang,  J. Li,  andJ.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"P/8WC967n47/eAs1M/LNmLPs9NZV4x7RQQBXS4gQTz0="},"a5a87ee9-bc1b-4cb2-9af4-ca592fd2998d":{"id_":"a5a87ee9-bc1b-4cb2-9af4-ca592fd2998d","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"6b9b0982-5baf-4e1d-a0cc-09e539249c08","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"P/8WC967n47/eAs1M/LNmLPs9NZV4x7RQQBXS4gQTz0="},"NEXT":{"nodeId":"d96a9f2c-6c37-41e0-adf4-cd90efcfa132","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"VUXBgyA916Up5heNFBcGBHvsvzom/Nv4slXLGYJZkgQ="}},"text":"Yu,  X. Zhang,  J. Li,  andJ. Sun, “Objects365: A large-scale, high-quality dataset for object\r\ndetection,” inThe IEEE International Conference on Computer Vision(ICCV), 2019. [93]    K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning forimage recognition,” inThe IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR), 2016. [94]    K.-K. Sung  and  T. Poggio,  “Example-based  learning  for  view-based human face detection,”IEEE Transactions on Pattern Anal-\r\nysis  and  Machine  Intelligence  (TPAMI),  vol. 20,  no. 1,  pp. 39–51,1998. [95]    P. Viola  and  M. Jones,  “Rapid  object  detection  using  a  boostedcascade of simple features,” inThe IEEE Conference on Computer\r\nVision and Pattern Recognition (CVPR), 2001. [96]    N. Dalal  and  B. Triggs,  “Histograms  of  oriented  gradients  forhuman detection,” inThe IEEE Conference on Computer Vision and\r\nPattern Recognition (CVPR), 2005. [97]    Y. Song,  A. Schwing,  Richard,  and  R. Urtasun,  “Training  deepneural networks via direct loss minimization,” inThe International\r\nConference on Machine Learning (ICML), 2016. [98]    P. Henderson and V. Ferrari, “End-to-end training of object classdetectors for mean average precision,” inThe Asian Conference on\r\nComputer Vision (ACCV), 2017. [99]    D. Dwibedi,  I. Misra,  and  M. Hebert,  “Cut,  paste  and  learn:Surprisingly easy synthesis for instance detection,” inThe IEEE\r\nInternational Conference on Computer Vision (ICCV), 2017. [100]  N. Dvornik, J. Mairal, and C.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4XTFy54Z2BgIyikwLX4cF9kzGRIbbN4yCZ9PhIkOMbY="},"d96a9f2c-6c37-41e0-adf4-cd90efcfa132":{"id_":"d96a9f2c-6c37-41e0-adf4-cd90efcfa132","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_31","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0a+AjtpUCrZcvQel9GMiW71kFnwFCBS2rr88Aqt4I3E="},"PREVIOUS":{"nodeId":"a5a87ee9-bc1b-4cb2-9af4-ca592fd2998d","metadata":{"page_number":31,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"4XTFy54Z2BgIyikwLX4cF9kzGRIbbN4yCZ9PhIkOMbY="}},"text":"Dvornik, J. Mairal, and C. Schmid, “Modeling visual contextis key to augmenting object detection datasets,” inThe European\r\nConference on Computer Vision (ECCV), 2018. [101]  C. Szegedy,  W. Liu,  Y. Jia,  P. Sermanet,  S. Reed,  D. Anguelov,\r\nD. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VUXBgyA916Up5heNFBcGBHvsvzom/Nv4slXLGYJZkgQ="},"9bc83470-c2bb-4a99-bd1a-6e3836d61a7b":{"id_":"9bc83470-c2bb-4a99-bd1a-6e3836d61a7b","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"NEXT":{"nodeId":"30091a55-ed0b-4661-b975-12754487daa0","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CPNmWaXXzmZND4rLZTxCduNk+irGZcbDECONtqFN5yY="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW32\r\nconvolutions,”  inThe  IEEE  Conference  on  Computer  Vision  andPattern Recognition (CVPR), 2015. [102]  K. Oksuz, B. C. Cam, E. Akbas, and S. Kalkan, “Localization recallprecision (LRP): A new performance metric for object detection,”\r\ninThe European Conference on Computer Vision (ECCV), 2018. [103]  K. Simonyan  and  A. Zisserman,  “Very  deep  convolutional  net-works  for  large-scale  image  recognition,”  inThe  International\r\nConference on Learning Representations (ICLR), 2015. [104]  A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks forhuman pose estimation,” inThe European Conference on Computer\r\nVision (ECCV), B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds.,2016. [105]  A. G. Howard,  M. Zhu,  B. Chen,  D. Kalenichenko,  W. Wang,T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient\r\nconvolutional  neural  networks  for  mobile  vision  applications,”inThe IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR), 2017. [106]  Z. Li,  C. Peng,  G. Yu,  X. Zhang,  Y. Deng,  and  J. Sun,  “Detnet:Design backbone for object detection,” inThe European Conference\r\non Computer Vision (ECCV), 2018. [107]  L. Zhu, Z. Deng, X. Hu, C.-W. Fu, X. Xu, J. Qin, and P.-A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0Ptebgg1DTcSeqO2+N02YgKHI0F+/H1PxSfts8atxOQ="},"30091a55-ed0b-4661-b975-12754487daa0":{"id_":"30091a55-ed0b-4661-b975-12754487daa0","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"9bc83470-c2bb-4a99-bd1a-6e3836d61a7b","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"0Ptebgg1DTcSeqO2+N02YgKHI0F+/H1PxSfts8atxOQ="},"NEXT":{"nodeId":"b3652b93-f966-4328-9276-a5d3ac586c96","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"QvJswAkul12ctpVFXZz+24KEUQBt7VhaCAW7GUJLjbI="}},"text":"Fu, X. Xu, J. Qin, and P.-A. Heng,“Bidirectional feature pyramid network with recurrent attention\r\nresidual modules for shadow detection,” inThe European Confer-ence on Computer Vision (ECCV), 2018. [108]  Y. Sun,  P. S. K. P,  J. Shimamura,  and  A. Sagata,  “Concatenatedfeature pyramid network for instance segmentation,”arXiv, vol. 1904.00768, 2019. [109]  S. S. Seferbekov, V. I. Iglovikov, A. V. Buslaev, and A. A. Shvets,“Feature  pyramid  network  for  multi-class  land  segmentation,”\r\ninThe IEEE Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, 2018. [110]  A. Kirillov, R. B. Girshick, K. He, and P. Dollár, “Panoptic featurepyramid  networks,”  inThe  IEEE  Conference  on  Computer  Vision\r\nand Pattern Recognition (CVPR), 2019. [111]  E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M.Ogden, “Pyramid methods in image processing,”RCA Engineer,\r\nvol. 29, no. 6, pp. 33–41, 1984. [112]  F. Yu and V. Koltun, “Multi-scale context aggregation by dilatedconvolutions,”  inThe  International  Conference  on  Learning  Repre-\r\nsentations (ICLR), 2016. [113]  X. Wang,  R. Girshick,  A. Gupta,  and  K. He,  “Non-local  neuralnetworks,” inThe IEEE Conference on Computer Vision and Pattern\r\nRecognition (CVPR), 2018. [114]  G. Huang,  Z. Liu,  and  K. Q.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CPNmWaXXzmZND4rLZTxCduNk+irGZcbDECONtqFN5yY="},"b3652b93-f966-4328-9276-a5d3ac586c96":{"id_":"b3652b93-f966-4328-9276-a5d3ac586c96","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"30091a55-ed0b-4661-b975-12754487daa0","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"CPNmWaXXzmZND4rLZTxCduNk+irGZcbDECONtqFN5yY="},"NEXT":{"nodeId":"6e7814b2-0d6f-463e-bbf7-3f09729b5c87","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"PY3F64N/tfhp7wmMnad4Fd7dRdwnUe0/HSigDR5XMiA="}},"text":"Huang,  Z. Liu,  and  K. Q. Weinberger,  “Densely  connectedconvolutional  networks,”  inThe  IEEE  Conference  on  Computer\r\nVision and Pattern Recognition (CVPR), 2017. [115]  K. He,  X. Zhang,  S. Ren,  and  J. Sun,  “Spatial  pyramid  poolingin  deep  convolutional  networks  for  visual  recognition,”  inThe\r\nEuropean Conference on Computer Vision (ECCV), D. Fleet, T. Pajdla,B. Schiele, and T. Tuytelaars, Eds., 2014. [116]  J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,”inThe IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR), 2018. [117]  S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deepnetwork  training  by  reducing  internal  covariate  shift,”  inThe\r\nInternational Conference on Machine Learning (ICML), 2015. [118]  B. Zoph  and  Q. V. Le,  “Neural  architecture  search  with  rein-forcement  learning,”  inThe  International  Conference  on  Learning\r\nRepresentations (ICLR), 2017. [119]  B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning trans-ferable architectures for scalable image recognition,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2018. [120]  E. Real,  A. Aggarwal,  Y. Huang,  and  Q. V. Le,  “Regularizedevolution for image classifier architecture search,” inAAAI Con-\r\nference on Artificial Intelligence, 2019. [121]  M. Tan and Q. V.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QvJswAkul12ctpVFXZz+24KEUQBt7VhaCAW7GUJLjbI="},"6e7814b2-0d6f-463e-bbf7-3f09729b5c87":{"id_":"6e7814b2-0d6f-463e-bbf7-3f09729b5c87","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"b3652b93-f966-4328-9276-a5d3ac586c96","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"QvJswAkul12ctpVFXZz+24KEUQBt7VhaCAW7GUJLjbI="},"NEXT":{"nodeId":"dabe8eb9-392e-43bd-a251-852c5a5e3da5","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"byEYPfczsiu2qXV2yqA0d8+fHtqSbpRI/17SfC2vBO4="}},"text":"[121]  M. Tan and Q. V. Le, “Efficientnet: Rethinking model scaling forconvolutional neural networks,” inThe International Conference on\r\nMachine Learning (ICML), 2019. [122]  Y. Chen, T. Yang, X. Zhang, G. Meng, C. Pan, and J. Sun, “Detnas:Backbone  search  for  object  detection,”  inAdvances  in  Neural\r\nInformation Processing Systems (NIPS), 2019. [123]  P. Sermanet,  D. Eigen,  X. Zhang,  M. Mathieu,  R. Fergus,  andY. Lecun, “Overfeat: Integrated recognition, localization and de-\r\ntection using convolutional networks,” inInternational Conferenceon Learning Representations (ICLR), 2014. [124]  P. J. Huber, “Robust estimation of a location parameter,”Annalsof Statistics, vol. 53, no. 1, pp. 73–101, 1964. [125]  B. Jiang,  R. Luo,  J. Mao,  T. Xiao,  and  Y. Jiang,  “Acquisitionof  localization  confidence  for  accurate  object  detection,”  inThe\r\nEuropean Conference on Computer Vision (ECCV), 2018. [126]  E. Goldman, R. Herzig, A. Eisenschtat, J. Goldberger, and T. Has-sner,  “Precise  detection  in  densely  packed  scenes,”  inThe  IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2019. [127]  J. Choi, D. Chun, H. Kim, and H.-J. Lee, “Gaussian yolov3: Anaccurate  and  fast  object  detector  using  localization  uncertainty\r\nfor autonomous driving,” inThe IEEE International Conference onComputer Vision (ICCV), 2019. [128]  Z. Tan, X. Nie, Q. Qian, N.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"PY3F64N/tfhp7wmMnad4Fd7dRdwnUe0/HSigDR5XMiA="},"dabe8eb9-392e-43bd-a251-852c5a5e3da5":{"id_":"dabe8eb9-392e-43bd-a251-852c5a5e3da5","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"6e7814b2-0d6f-463e-bbf7-3f09729b5c87","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"PY3F64N/tfhp7wmMnad4Fd7dRdwnUe0/HSigDR5XMiA="},"NEXT":{"nodeId":"17d34e12-ded0-4cd9-9c49-d8624f3351b3","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"FWx1ieozq5lx+cmAB9TbRVPcUmkG68fWmLdnruJa78s="}},"text":"Tan, X. Nie, Q. Qian, N. Li, and H. Li, “Learning to rank pro-posals for object detection,” inThe IEEE International Conference\r\non Computer Vision (ICCV), 2019. [129]  S. Gidaris  and  N. Komodakis,  “Object  detection  via  a  multi-region  and  semantic  segmentation-aware  cnn  model,”  inThe\r\nIEEE International Conference on Computer Vision (ICCV), 2015. [130]  ——,  “Attend  refine  repeat:  Active  box  proposal  generationvia  in-out  localization,”  inThe  British  Machine  Vision  Conference\r\n(BMVC), 2016. [131]  J. Dai,  H. Qi,  Y. Xiong,  Y. Li,  G. Zhang,  H. Hu,  and  Y. Wei,“Deformable convolutional networks,” inThe IEEE International\r\nConference on Computer Vision (ICCV), 2017. [132]  K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,Z. Liu,  J. Xu,  Z. Zhang,  D. Cheng,  C. Zhu,  T. Cheng,  Q. Zhao,\r\nB. Li,  X. Lu,  R. Zhu,  Y. Wu,  J. Dai,  J. Wang,  J. Shi,  W. Ouyang,C. C. Loy,  and  D. Lin,  “MMDetection:  Open  mmlab  detection\r\ntoolbox and benchmark,”arXiv preprint arXiv:1906.07155, 2019. [133]  T. Yang,  X. Zhang,  Z. Li,  W. Zhang,  and  J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"byEYPfczsiu2qXV2yqA0d8+fHtqSbpRI/17SfC2vBO4="},"17d34e12-ded0-4cd9-9c49-d8624f3351b3":{"id_":"17d34e12-ded0-4cd9-9c49-d8624f3351b3","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"dabe8eb9-392e-43bd-a251-852c5a5e3da5","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"byEYPfczsiu2qXV2yqA0d8+fHtqSbpRI/17SfC2vBO4="},"NEXT":{"nodeId":"aab790de-85e1-4cb2-b8fa-a6a2f20e26fd","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"irTSC7XRAUXye5DppyB4PNqLn/HjcTJYGQNtLPiCJI8="}},"text":"Li,  W. Zhang,  and  J. Sun,  “Metaanchor:Learning to detect objects with customized anchors,” inAdvances\r\nin Neural Information Processing Systems (NIPS), 2018. [134]  C. Peng,  T. Xiao,  Z. Li,  Y. Jiang,  X. Zhang,  K. Jia,  G. Yu,  andJ. Sun, “Megdet: A large mini-batch object detector,” inThe IEEE\r\nConference  on  Computer  Vision  and  Pattern  Recognition  (CVPR),2018. [135]  M. Guo,  A. Haque,  D.-A. Huang,  S. Yeung,  and  L. Fei-Fei,“Dynamic task prioritization for dynamic task learning,” inThe\r\nEuropean Conference on Computer Vision (ECCV), 2018. [136]  N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,“Smote: Synthetic minority over-sampling technique,”Journal of\r\nArtificial Intelligence Research, vol. 16, pp. 321–357, 2002. [137]  C. Drummond and R. C. Holte, “C4.5, class imbalance, and costsensitivity:  Why  under-sampling  beats  over-sampling,”  inThe\r\nInternational Conference on Machine Learning (ICML), 2003. [138]  H. He, Y. Bai, E. A. Garcia, and S. Li, “Adasyn: Adaptive syntheticsampling approach for imbalanced learning,” inInternational Joint\r\nConference on Neural Networks, 2008. [139]  M. Buda, A. Maki, and M. A. Mazurowski, “A systematic study ofthe class imbalance problem in convolutional neural networks,”\r\nNeural Networks, vol. 106, pp. 249–259, 2018. [140]  S. Li, L. Zhouche, and H.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FWx1ieozq5lx+cmAB9TbRVPcUmkG68fWmLdnruJa78s="},"aab790de-85e1-4cb2-b8fa-a6a2f20e26fd":{"id_":"aab790de-85e1-4cb2-b8fa-a6a2f20e26fd","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_32","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"bpJHndpbWvzjd0BxP7YGhHNvrjCbZOPpjaeDTBkMoao="},"PREVIOUS":{"nodeId":"17d34e12-ded0-4cd9-9c49-d8624f3351b3","metadata":{"page_number":32,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"FWx1ieozq5lx+cmAB9TbRVPcUmkG68fWmLdnruJa78s="}},"text":"[140]  S. Li, L. Zhouche, and H. Qingming, “Relay Backpropagation forEffective Learning of Deep Convolutional Neural Networks,” in\r\nThe European Conference on Computer Vision (ECCV), 2016. [141]  Y.-X. Wang, D. Ramanan, and M. Hebert, “Learning to model thetail,” inAdvances in Neural Information Processing Systems (NIPS),\r\n2017. [142]  Y. Cui, Y. Song, C. Sun, A. G. Howard, and S. J. Belongie, “Largescale  fine-grained  categorization  and  domain-specific  transfer\r\nlearning,” inThe IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2018. [143]  C. Huang, Y. Li, C. C. Loy, and X. Tang, “Learning deep repre-sentation for imbalanced classification,” inThe IEEE Conference on\r\nComputer Vision and Pattern Recognition (CVPR), 2016. [144]  Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie, “Class-balancedloss based on effective number of samples,” inThe IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR), 2019. [145]  M. P. Kumar,  B. Packer,  and  D. Koller,  “Self-paced  learningfor  latent  variable  models,”  inAdvances  in  Neural  Information\r\nProcessing Systems (NIPS), 2010. [146]  K. Kumar Singh and Y. Jae Lee, “Hide-and-seek: Forcing a net-work to be meticulous for weakly-supervised object and action\r\nlocalization,”  inThe  IEEE  International  Conference  on  ComputerVision (ICCV), 2017.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"irTSC7XRAUXye5DppyB4PNqLn/HjcTJYGQNtLPiCJI8="},"aa4524fe-0e68-4aae-8934-7e018941aec2":{"id_":"aa4524fe-0e68-4aae-8934-7e018941aec2","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"NEXT":{"nodeId":"6c7c202c-5be9-42ea-8e73-6f52c93d5da6","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iN1OXSIQBhsZS5hCuRYq35VNXHXha3g8RlUEgE1ge3g="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW33\r\n[147]  Q. Dong, S. Gong, and X. Zhu, “Class rectification hard miningfor imbalanced deep learning,” inThe International Conference on\r\nComputer Vision (ICCV), 2017. [148]  H.-S. Chang, E. Learned-Miller, and A. McCallum, “Active bias:Training  more  accurate  neural  networks  by  emphasizing  high\r\nvariance  samples,”  inAdvances  in  Neural  Information  ProcessingSystems (NIPS), 2017. [149]  K. Wei,  R. Iyer,  and  J. Bilmes,  “Submodularity  in  data  subsetselection and  active learning,” inThe International  Conference  on\r\nMachine Learning (ICML), 2015, pp. 1954–1963. [150]  V. Kaushal,  A. Sahoo,  K. Doctor,  N. R. Uppalapati,  S. Shetty,P. Singh, R. K. Iyer, and G. Ramakrishnan, “Learning from less\r\ndata:  Diversified  subset  selection  and  active  learning  in  imageclassification tasks,”arXiv, vol. 1805.11191, 2018. [151]  O. Sener and S. Savarese, “Active learning for convolutional neu-ral networks: A core-set approach,” inThe International Conference\r\non Learning Representations (ICLR), 2018. [152]  K. Vodrahalli,  K. Li,  and  J. Malik,  “Are  all  training  examplescreated equal? an empirical study,”arXiv, vol. 1811.12569, 2018. [153]  V. Birodkar, H. Mobahi, and S. Bengio, “Semantic redundanciesin image-classification datasets: The 10% you don’t need,”arXiv,\r\nvol. 1901.11409, 2019.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"89YmmKegVHYTw87rLQYInORGVwytPg5wweMuMAQQzvA="},"6c7c202c-5be9-42ea-8e73-6f52c93d5da6":{"id_":"6c7c202c-5be9-42ea-8e73-6f52c93d5da6","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"PREVIOUS":{"nodeId":"aa4524fe-0e68-4aae-8934-7e018941aec2","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"89YmmKegVHYTw87rLQYInORGVwytPg5wweMuMAQQzvA="},"NEXT":{"nodeId":"7d5f98cc-42f3-43b1-9d41-ddf7c10cf24e","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"E8IbtL+4HX3ARaDA+ExsTwnndJl7shuOCn7IMh3sRR8="}},"text":"1901.11409, 2019. [154]  D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li,A. Bharambe, and L. van der Maaten, “Exploring the Limits of\r\nWeakly  Supervised  Pretraining,”  inThe  European  Conference  onComputer Vision (ECCV), 2018. [155]  Y. Liu, T. Gao, and H. Yang, “Selectnet: Learning to sample fromthe  wild  for  imbalanced  data  training,”arXiv,  vol. 1905.09872,\r\n2019. [156]  S. S. Mullick,  S. Datta,  and  S. Das,  “Generative  AdversarialMinority Oversampling,”arXiv, vol. 1903.09730, 2019. [157]  X. Zhu,  Y. Liu,  Z. Qin,  and  J. Li,  “Data  augmentation  in  emo-tion classification using generative adversarial networks,”arXiv\r\npreprint arXiv:1711.00648, 2017. [158]  C. Bowles,  L. Chen,  R. Guerrero,  P. Bentley,  R. Gunn,  A. Ham-mers, D. A. Dickie, M. V. Hernández, J. Wardlaw, and D. Rueck-\r\nert, “Gan augmentation: augmenting training data using genera-tive adversarial networks,”arXiv preprint arXiv:1810.10863, 2018. [159]  X. Zhang,  Z. Fang,  Y. Wen,  Z. Li,  and  Y. Qiao,  “Range  loss  fordeep face recognition with long-tailed training data,” inThe IEEE\r\nInternational Conference on Computer Vision (ICCV), 2016. [160]  X. Yin, X. Yu, K.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"iN1OXSIQBhsZS5hCuRYq35VNXHXha3g8RlUEgE1ge3g="},"7d5f98cc-42f3-43b1-9d41-ddf7c10cf24e":{"id_":"7d5f98cc-42f3-43b1-9d41-ddf7c10cf24e","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"PREVIOUS":{"nodeId":"6c7c202c-5be9-42ea-8e73-6f52c93d5da6","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"iN1OXSIQBhsZS5hCuRYq35VNXHXha3g8RlUEgE1ge3g="},"NEXT":{"nodeId":"b9571ecd-88fe-42fd-a23b-30f10ba93148","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"F2pHd6mmeEuxHN3IAvaq/3kQenTqbbVIJfzmTATunls="}},"text":"[160]  X. Yin, X. Yu, K. Sohn, X. Liu, and M. K. Chandraker, “Featuretransfer learning for deep face recognition with long-tail data,”\r\narXiv, vol. 1803.09014, 2018. [161]  C. Huang,  Y. Li,  C. L. Chen,  and  X. Tang,  “Deep  imbalancedlearning  for  face  recognition  and  attribute  prediction,”IEEE\r\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI),2019. [162]  J. Hu, J. Lu, and Y. Tan, “Discriminative deep metric learning forface verification in the wild,” inThe IEEE Conference on Computer\r\nVision and Pattern Recognition (CVPR), 2014. [163]  S. Bell and K. Bala, “Learning visual similarity for product designwith  convolutional  neural  networks,”ACM  Trans. on  Graphics\r\n(SIGGRAPH), vol. 34, no. 4, 2015. [164]  F. Schroff,  D. Kalenichenko,  and  J. Philbin,  “Facenet:  A  unifiedembedding for face recognition and clustering,” inConference on\r\nComputer Vision and Pattern Recognition (CVPR), 2015. [165]  E. Hoffer and N. Ailon, “Deep metric learning using triplet net-work,” inThe International Conference on Learning Representations\r\n(ICLR), 2015. [166]  W. Ge,  “Deep  metric  learning  with  hierarchical  triplet  loss,”  inThe European Conference on Computer Vision (ECCV), 2018. [167]  Y. Yuan, K. Yang, and C. Zhang, “Hard-aware deeply cascadedembedding,”  inThe  IEEE  International  Conference  on  Computer\r\nVision (ICCV), 2017. [168]  B. Harwood, G. K. B. VijayKumarB., G. Carneiro, I. D. Reid, andT.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"E8IbtL+4HX3ARaDA+ExsTwnndJl7shuOCn7IMh3sRR8="},"b9571ecd-88fe-42fd-a23b-30f10ba93148":{"id_":"b9571ecd-88fe-42fd-a23b-30f10ba93148","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"PREVIOUS":{"nodeId":"7d5f98cc-42f3-43b1-9d41-ddf7c10cf24e","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"E8IbtL+4HX3ARaDA+ExsTwnndJl7shuOCn7IMh3sRR8="},"NEXT":{"nodeId":"d1b706d3-ea21-4721-be42-1d412e6c30c3","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"fYxnPOUyze5X4I8cxX62pwhG+nKLCF87ijJANIGV1eI="}},"text":"Carneiro, I. D. Reid, andT. Drummond, “Smart mining for deep metric learning,” inThe\r\nIEEE International Conference on Computer Vision (ICCV), 2017. [169]  Y. Cui, F. Zhou, Y. Lin, and S. Belongie, “Fine-grained categoriza-tion and dataset bootstrapping using deep metric learning with\r\nhumans in the loop,” inThe IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2016. [170]  H. O. Song, Y. Xiang, S. Jegelka, and S. Savarese, “Deep metriclearning  via  lifted  structured  feature  embedding,”  inThe  IEEE\r\nComputer Vision and Pattern Recognition (CVPR), 2016. [171]  C. Huang, C. C. Loy, and X. Tang, “Local similarity-aware deepfeature embedding,” inAdvances in Neural Information Processing\r\nSystems (NIPS), 2016.[172]  Y. Duan, W. Zheng, X. Lin, J. Lu, and J. Zhou, “Deep adversarial\r\nmetric learning,” inThe IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2018. [173]  Y. Zhao,  Z. Jin,  G.-j. Qi,  H. Lu,  and  X.-s. Hua,  “An  adversarialapproach to hard triplet generation,” inThe European Conference\r\non Computer Vision (ECCV), 2018.[174]  I. Goodfellow,  J. Pouget-Abadie,  M. Mirza,  B. Xu,  D. Warde-\r\nFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-sarial nets,” inAdvances in Neural Information Processing Systems\r\n(NIPS), 2014.[175]  W. Zheng,  Z. Chen,  J. Lu,  and  J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"F2pHd6mmeEuxHN3IAvaq/3kQenTqbbVIJfzmTATunls="},"d1b706d3-ea21-4721-be42-1d412e6c30c3":{"id_":"d1b706d3-ea21-4721-be42-1d412e6c30c3","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"PREVIOUS":{"nodeId":"b9571ecd-88fe-42fd-a23b-30f10ba93148","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"F2pHd6mmeEuxHN3IAvaq/3kQenTqbbVIJfzmTATunls="},"NEXT":{"nodeId":"4334f373-11fb-42b7-bc47-5607a7c331a1","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"QjTVu/ox5ixTCKtM+BParPoCOFq3lCTMqr92vSRyyt0="}},"text":"Chen,  J. Lu,  and  J. Zhou,  “Hardness-aware  deep\r\nmetric learning,” inThe IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2019. [176]  A. Yao, “Interactive object detection,” inThe IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2012. [177]  C. Li,  J. Yan,  F. Wei,  W. Dong,  Q. Liu,  and  H. Zha,  “Self-pacedmulti-task learning,” inAAAI Conference on Artificial Intelligence,\r\n2017.[178]  A. Kendall,  Y. Gal,  and  R. Cipolla,  “Multi-task  learning  using\r\nuncertainty to weigh losses for scene geometry and semantics,”inThe IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR), 2018.[179]  S. Liu, E. Johns, and A. J. Davison, “End-to-end multi-task learn-\r\ning with attention,” inThe IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2019. [180]  C.-Y. L. Zhao  Chen,  Vijay  Badrinarayanan  and  A. Rabinovich,“Gradnorm: Gradient normalization for adaptive loss balancing\r\nin  deep  multitask  networks,”  inInternational  Conference  on  Ma-chine Learning (ICML), 2018. Kemal  Oksuzreceived B.Sc. in System Engi-neering from Land Forces Academy of Turkey in\r\n2008 and his M.Sc. in Computer Engineering in2016 from Bogazici University with high honor\r\ndegree. He is currently pursuing his Ph.D. inMiddle East Technical University, Ankara, Turkey. His research interests include computer visionwith a focus on object detection. Baris  Can  Camreceived his B.Sc. degree inElectrical and Electronics Engineering from Es-\r\nkisehir Osmangazi University, Turkey in 2016.He is currently pursuing his M.Sc. in Middle\r\nEast Technical University, Ankara, Turkey. Hisresearch interests include computer vision with\r\na focus on object detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"fYxnPOUyze5X4I8cxX62pwhG+nKLCF87ijJANIGV1eI="},"4334f373-11fb-42b7-bc47-5607a7c331a1":{"id_":"4334f373-11fb-42b7-bc47-5607a7c331a1","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_33","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"jMTZREOHvvetaugl8Zk6NtkUqtVJrJO/Lg+YE4ABJGE="},"PREVIOUS":{"nodeId":"d1b706d3-ea21-4721-be42-1d412e6c30c3","metadata":{"page_number":33,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"fYxnPOUyze5X4I8cxX62pwhG+nKLCF87ijJANIGV1eI="}},"text":"Hisresearch interests include computer vision with\r\na focus on object detection. Sinan  Kalkanreceived his M.Sc. degree inComputer Engineering from Middle East Tech-\r\nnical University (METU), Turkey in 2003, and hisPh.D. degree in Informatics from the University\r\nof Göttingen, Germany in 2008. After workingas a postdoctoral researcher at the University of\r\nGottingen and at METU, he became an assis-tant professor at METU in 2010. Since 2016, he\r\nhas been working as an associate professor onproblems within Computer Vision, and Develop-\r\nmental Robotics.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QjTVu/ox5ixTCKtM+BParPoCOFq3lCTMqr92vSRyyt0="},"ecc8d4d8-83c1-4f29-acd8-03911ff8368d":{"id_":"ecc8d4d8-83c1-4f29-acd8-03911ff8368d","metadata":{"page_number":34,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf_34","metadata":{"page_number":34,"file_path":"/usr/src/app/RAG_storage/data/Imbalance Problems in Object Detection- A Review.pdf","file_name":"Imbalance Problems in Object Detection- A Review.pdf"},"hash":"IokLNs2EdzwS411A+ABhmXIS6QR1m+MAEdXh2QT+4wY="}},"text":"OKSUZet al.: IMBALANCE PROBLEMS IN OBJECT DETECTION: A REVIEW34\r\nEmre Akbasis an assistant professor at the De-partment of Computer Engineering, Middle East\r\nTechnical University (METU), Ankara, Turkey. Hereceived his Ph.D. degree from the Department\r\nof Electrical and Computer Engineering, Univer-sity of Illinois at Urbana-Champaign in 2011. His\r\nM.Sc. and B.Sc. degrees in computer scienceare both from METU. Prior to joining METU, he\r\nwas a postdoctoral research associate at the De-partment of Pyschological and Brain Sciences,\r\nUniversity of California Santa Barbara. His re-search interests are in computer vision and deep learning with a focus\r\non object detection and human pose estimation.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"iMoLelpVEqbkRtiCH9/Rj31wON9TzKZrOuTvsFpkB9o="},"a0667aed-6e45-4382-af1d-8160078c64ff":{"id_":"a0667aed-6e45-4382-af1d-8160078c64ff","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"Z294tAjFkc81KUx47N+cdgkdXqmHDNFe8WRVYw5ihS4="},"NEXT":{"nodeId":"cdbd2a55-9e36-433c-a19d-b0d17d01f449","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"XkxDFIiV5RRltXbZmPBm2CBx6v8ktiRTsL3es5Hgdsk="}},"text":"Citation:Lee, H; Ahn, S Improving\r\nperformance of object detection by\r\npreserving label distribution.Preprints\r\n2023,1, 0. https://doi.org/\r\nCopyright:©2023  by  the  authors. Submitted toPreprintsfor possible open\r\naccess publication under the terms and\r\nconditions of the Creative Commons\r\nAttri- bution (CC BY) license (https://\r\ncreativecommons.org/licenses/by/\r\n4.0/). Article\r\nImproving the performance of object detection by preserving\r\nlabel distribution\r\nHeewon Lee, Sangtae Ahn*\r\nSchool of Electronic and Electrical Engineering, Kyungpook National University, 80 Daehak-ro, Buk-gu, Daegu,\r\nSouth Korea, 41566\r\n*Correspondence: Sangtae Ahn (stahn@knu.ac.kr)\r\nAbstract:Object detection is a task that performs position identification and label classification of\r\nobjects in images or videos. The information obtained through this process plays an essential role in\r\nvarious tasks in the field of computer vision. In object detection, the data utilized for training and\r\nvalidation typically originate from public datasets that are well-balanced in terms of the number\r\nof objects ascribed to each class in an image. However, in real-world scenarios, handling datasets\r\nwith much greater class imbalance, i.e., very different numbers of objects for each class , is much\r\nmore common, and this imbalance may reduce the performance of object detection when predicting\r\nunseen test images. In our study, thus, we propose a method that evenly distributes the classes in\r\nan image for training and validation, solving the class imbalance problem in object detection. Our\r\nproposed method aims to maintain a uniform class distribution through multi-label stratification. We tested our proposed method not only on public datasets that typically exhibit balanced class\r\ndistribution but also on custom datasets that may have imbalanced class distribution. We found that\r\nour proposed method was more effective on datasets containing severe imbalance and less data. Our\r\nfindings indicate that the proposed method can be effectively used on datasets with substantially\r\nimbalanced class distribution. Keywords:object detection; imbalanced class distribution;\r\n1. Introduction\r\nComputer vision is a field of artificial intelligence that enables machines to under-\r\nstand and interpret visual information [1].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"qYWg4RT9HiVozAeqa0mfRcmEU50d9cLM2ocbAJpvKKI="},"cdbd2a55-9e36-433c-a19d-b0d17d01f449":{"id_":"cdbd2a55-9e36-433c-a19d-b0d17d01f449","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"Z294tAjFkc81KUx47N+cdgkdXqmHDNFe8WRVYw5ihS4="},"PREVIOUS":{"nodeId":"a0667aed-6e45-4382-af1d-8160078c64ff","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"qYWg4RT9HiVozAeqa0mfRcmEU50d9cLM2ocbAJpvKKI="}},"text":"Computer vision involves the recognition and\r\nextraction of patterns from images or videos, and it has been applied in various fields\r\n[2–17]. Examples of its usage include object detection [18], image classification [19], face\r\nrecognition [20], or image generation [21]. Among these, object detection is an important\r\ntask in the field of computer vision, which aims to identify and localize multiple objects in\r\nimages or videos. To develop a model for object detection,  the first step is to collect data and then\r\nappropriately split them into training and validation datasets. When splitting the original\r\ndata, maintaining a similar class distribution between the training and validation datasets\r\nis crucial because datasets for object detection are presented as multi-label problems in\r\nthat multiple objects can be present in a single image. In particular, the number of objects\r\nfor each label could be imbalanced (Figure 1). Maintaining a similar class balance when\r\ndividing the dataset could be difficult. For image classification problems, a strategy called\r\nstratification can be used to maintain similar class distributions. However, it is unknown\r\nhow stratification is applied to an object detection problem and how stratification influences\r\nthe perofmrance of object detection. Here, we present a study that proposes a new method called stratification for object\r\ndetection (SOD), which applies stratification to the object detection problem. This method\r\nuses a multi-label stratification technique to preprocess labeled data in object detection and\r\nthen applies stratification. This method facilitates the preservation of the class distribution\r\namong the split datasets, yielding better performance on public and custom datasets. Our\r\nproposed method can solve the problem of class imbalance better than existing methods,\r\nimproving the performance of trained models. Our method was applied to object detection\r\narXiv:2308.14466v1  [cs.CV]  28 Aug 2023","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"XkxDFIiV5RRltXbZmPBm2CBx6v8ktiRTsL3es5Hgdsk="},"4dbe9481-0739-43a5-90ac-7878972afcd0":{"id_":"4dbe9481-0739-43a5-90ac-7878972afcd0","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"01+k17N0OKLvJ+EzFW/YAZYyATLKPaGKgbguscvM4rM="},"NEXT":{"nodeId":"a68ecdfb-ed1e-4b5f-8cfc-9e2fde420cef","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ypc2xbClBrTVD9Y78xPw8rEkf/jEswPqjlWMHPJnkLE="}},"text":"2 of 13\r\nFigure 1.Each image has a different number of bounding boxes for each class. For example, the\r\nnumber of bounding boxes for a person may be high while the number of bounding boxes for a chair\r\nmay be low (left) or vice versa (right). Images are taken from the Pascal Visual Object Classes(VOC)\r\n2012 dataset. problems, particularly implemented in the YOLO format because the YOLO algorithm is\r\na widely used platform in object detection studies. Therefore, our proposed method is\r\nexpected to be a useful tool that solves an important problem in the field of object detection\r\nand guarantees improved performance. Furthermore, it can contribute to improving the\r\naccuracy and reliability of the object detection model. The main contributions of this research are as follows:\r\n•      We propose a method for preserving class distribution in object detection tasks. •We experimentally demonstrate the effectiveness of our proposed method on both\r\npublic datasets and custom datasets. 2. Related Work\r\n2.1. Real-time object detection\r\nObject detection is a computer vision task that involves detecting multiple objects in\r\nimages or videos and classifying their positions and types. It is a key technological tool that\r\nenables computers to understand the real world and has been applied in various fields such\r\nas autonomous driving[22], surveillance systems[23], robotics[24], augmented reality[25],\r\nand medical image analysis[26]. Fundamentally, object recognition involves representing\r\nspecific objects in an image as rectangular bounding boxes and characterizing them into\r\ndifferent classes. While there are various algorithms for this task, we mainly employ the\r\nYOLO algorithm in this study. YOLO[27], first proposed in 2015, is an algorithm that enables real-time object recogni-\r\ntion. YOLO divides an image into a grid and applies a method to simultaneously predict\r\nbounding boxes and class probabilities for each grid cell. Unlike two-stage detectors[28],\r\nYOLO adopts a one-stage detector approach, whereby it considers the entire image at once\r\nand performs predictions. This unique feature of YOLO allows for real-time processing. Since 2015, YOLO has undergone many improvements. In YOLOv2[29], the first update of\r\nYOLO, the ability to detect objects of various sizes was enhanced.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KkTNplMH/6qS0brq2VMrvTDg+nYFHWBhdISVxA4lHPY="},"a68ecdfb-ed1e-4b5f-8cfc-9e2fde420cef":{"id_":"a68ecdfb-ed1e-4b5f-8cfc-9e2fde420cef","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"01+k17N0OKLvJ+EzFW/YAZYyATLKPaGKgbguscvM4rM="},"PREVIOUS":{"nodeId":"4dbe9481-0739-43a5-90ac-7878972afcd0","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"KkTNplMH/6qS0brq2VMrvTDg+nYFHWBhdISVxA4lHPY="}},"text":"The concept of anchor\r\nboxes[30] was introduced in YOLOv2, and multi-scale training methods were used to\r\ntrain on images with different resolutions, resulting in improved detection of objects of\r\ndifferent sizes. Additionally, YOLOv2 utilized the WordTree model to jointly train on the\r\nMSCOCO and ImageNet datasets, enabling the detection of over 9,000 different classes. YOLOv3[31] improved the detection of objects of various sizes by predicting bounding\r\nboxes in three different-sized feature maps. YOLOv3 also introduced a method to predict\r\nmulti-labels for each box, facilitating the handling of more complex classification problems.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ypc2xbClBrTVD9Y78xPw8rEkf/jEswPqjlWMHPJnkLE="},"f3281fb0-9483-454e-9dea-d0d384d5db35":{"id_":"f3281fb0-9483-454e-9dea-d0d384d5db35","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"5nHRXxi56gCNI8h2Kb+Qgduf4uhInQG6qjGOtl+ztbE="},"NEXT":{"nodeId":"2ece4101-bd73-43d6-b4b2-80a7612b0c2b","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"UPXEgz5GwN2WNtIhEIFMaICUX5auyN9cBY5uj/KXT9M="}},"text":"3 of 13\r\nIn YOLOv4[32], several optimizations were introduced to improve both performance and\r\nspeed over those of previous versions. For YOLOv4, several features were introduced,\r\nnamely, a new backbone network called CSPDarknet53, a new neck structure using PANet\r\nand SAM blocks, and the Mish activation function. These new structures and features\r\nmade YOLOv4 more efficient and capable of accurately recognizing objects. The improved\r\ntraining speed, inference time, and model size were improved in YOLOv5[33], while user-\r\nfriendly features were added using the PyTorch framework. This enabled faster and more\r\nefficient object recognition, expanding the practical application of object detection. Recently,\r\nthe trainable bag-of-freebies method was introduced in YOLOv7[34,35] to significantly\r\nimprove detection accuracy without increasing the inference cost. In this version, methods\r\nwere also proposed to address issues arising from re-parameterized modules replacing\r\noriginal modules and apply dynamic label assignment strategies for different output layer\r\nassignments. Furthermore, extended and compound scaling methods were formulated to\r\neffectively utilize parameters and computations, reduce parameters and computational\r\ncost, and improve inference speed and accuracy. In this manner, YOLO-based models\r\nhave continued to evolve and become the standard in real-time object detection. By exam-\r\nining these research trends, we can observe that object detection algorithms are steadily\r\nadvancing to become faster, more accurate, and more applicable in diverse environments. 2.2. Stratification\r\nStudies applying stratification to datasets have emphasized the significance of the class\r\ndistribution of the dataset, which is particularly important in the treatment of classification\r\nproblems in the field of machine learning[36]. Stratification is a type of data sampling\r\ntechnique that seeks to maintain the class distribution of the entire dataset in the training\r\nand validation sub-datasets. The key advantage of this technique is that it ensures that all\r\nclasses are represented in the training and validation subsets as they are over the entire\r\ndataset, allowing the model to earn each class fairly. The utilization of stratification in deep learning is mainly focused on addressing class\r\nimbalance issues[37]. In scenarios in which the number of samples for a specific class is\r\nsignificantly larger than that for other classes, i.e., class imbalance, the performance of the\r\nmodel can be excessively influenced by the dominant class.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KyXFC4hzDEFK512yX1Fs7Hc6AgEiIF+dyg1STeFngqQ="},"2ece4101-bd73-43d6-b4b2-80a7612b0c2b":{"id_":"2ece4101-bd73-43d6-b4b2-80a7612b0c2b","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"5nHRXxi56gCNI8h2Kb+Qgduf4uhInQG6qjGOtl+ztbE="},"PREVIOUS":{"nodeId":"f3281fb0-9483-454e-9dea-d0d384d5db35","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"KyXFC4hzDEFK512yX1Fs7Hc6AgEiIF+dyg1STeFngqQ="}},"text":"Stratification can alleviate such\r\nproblems and ensure that all classes are fairly represented. Furthermore, applying stratification to cross-validation is a highly effective strategy. Cross-validation[38] is a technique used to evaluate the generalization performance of a\r\nmodel by dividing the data into multiple folds and using each fold alternately as a training\r\nset and a validation set. By applying stratification to this process, the class distribution of\r\nthe data in each fold can accurately reflect the distribution of the entire dataset. This can\r\nhelp verify whether each model performs equally well for all classes. Various methods for applying stratification in deep learning have been utilized, and\r\nthey can be adjusted depending on the characteristics of a specific problem and dataset. Stratification may be necessary in some cases but non-essential in others. However, in\r\ngeneral, stratification is recognized as an important step for improving model training and\r\nenhancing generalization performance. 2.3. Multi-label stratification\r\nMulti-label classification[39], unlike conventional classification, allows each data point\r\nto be classified with multiple labels simultaneously. Stratified sampling is a technique that\r\nselects samples evenly from each category, ensuring the representativeness of the entire\r\ndataset while maintaining the importance of each category. However, this technique is\r\nonly applicable to single-label classification problems. Multi-label stratification extends\r\nthis technique to problems in which each data point can have multiple labels. This ensures\r\nthat each label combination is evenly distributed in the training and validation datasets,\r\nallowing the model to optimize generalizability for each label combination during training.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"UPXEgz5GwN2WNtIhEIFMaICUX5auyN9cBY5uj/KXT9M="},"e7415340-4fcf-4818-aac5-4b3f294b4e26":{"id_":"e7415340-4fcf-4818-aac5-4b3f294b4e26","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"uT4ArRohhav+x+4MN1CM3DwHoqyFm9NO3xN9LPyDFk0="},"NEXT":{"nodeId":"de66ca57-9b02-4fce-87fd-e2ca4c328bf5","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"EFTOFzZC3ReNUZi8HxGjeqkfMSEvzCu5Rp6ocpEEFa0="}},"text":"4 of 13\r\nThere  are  various  use  cases  for  multi-label  stratification. For  example,  in  music\r\nclassification[40],  as a song can belong to multiple genres,  the corresponding training\r\nand validation datasets should represent each genre combination correctly to achieve\r\naccurate model learning. Multi-label stratification can be used to meet these requirements. Similarly, in medical imaging[41], an image can display multiple diseases. In this case, the\r\ncombinations involving each disease label need to be well represented in the training and\r\nvalidation datasets, and multi-label stratification can be used for this purpose. Additionally,\r\nin text classification[42], text data such as news articles, research papers, and blog posts\r\ncan be simultaneously classified multiple topics or categories. In this case, multi-label\r\nstratification can be applied to ensure that combinations involving each topic are evenly\r\ndistributed in the training and validation datasets. In summary, multi-label stratification\r\nplays an important role in enabling more accurate model training in various fields to\r\naddress real-world problems effectively. 3. Proposed algorithm\r\nIn this section,  we introduce a new algorithm that applies stratification to object\r\ndetection datasets using the YOLO format. The main goal is to improve existing dataset\r\npartitioning methods, enabling a more precise and fairer training process. The detailed\r\noperation of the algorithm is as follows. This algorithm requires three inputs: the paths to the folders where the image files\r\nand text files are stored, denoted asFimgandFtxtrespectively, and the number of subsets\r\nwithin the dataset to be created, denoted ask. The algorithm, illustrated below, operates as follows. Lines 1 to 5 generate a list using\r\nthe names of the image files. Similarly, lines 6 to 10 construct a list using the names of the\r\ntext files. Lines 11 to 27 convert the label data from the text files into Dataframe format. In this process, if the dataset contains background images to prevent false positives, there\r\nmay be no corresponding text file for that image or the text file may be empty to prevent\r\nthe presence of false positives. To account for such background images in the partitioning\r\nprocess, we insert -1 in the class column of the Dataframe.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"54lbYazQ0ad1SsX0g6/cmsOs7FCyDANK8Vj4DnAfnCs="},"de66ca57-9b02-4fce-87fd-e2ca4c328bf5":{"id_":"de66ca57-9b02-4fce-87fd-e2ca4c328bf5","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"uT4ArRohhav+x+4MN1CM3DwHoqyFm9NO3xN9LPyDFk0="},"PREVIOUS":{"nodeId":"e7415340-4fcf-4818-aac5-4b3f294b4e26","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"54lbYazQ0ad1SsX0g6/cmsOs7FCyDANK8Vj4DnAfnCs="}},"text":"Once this step is completed, the\r\nDataframe contains the paths of the text files, number of objects per class, and the x and y\r\ncoordinates, width (w), and height (h) of each object. Lines 29 to 34 are preprocessing steps\r\nneeded before the multi-label stratified KFold procedure is performed. In this process, we\r\nperform one-hot encoding on the classes and concatenate the original Dataframe with the\r\none-hot encoded Dataframe. We then multiply thewandhvalues by 1000 to prevent loss of\r\nvalues when calculating their averages. Next, we remove the \"class\", \"x\", and \"y\" columns\r\nfrom the Dataframe and group them based on filename. This creates a Dataframe that\r\nindicates how many objects of each class exist within each text file. For background images,\r\nthe total object count is 0; thus, we change it to 1 to avoid division by 0 when calculating the\r\naverages ofwandh. We then calculate w, h, and the ratio of h to w. Then, we remove the\r\n\"w\" and \"h\" columns to improve computational efficiency during the partitioning process. Finally, lines 35 to 38 perform the multi-label stratified KFold procedure, dividing the\r\ndataset into training and validation sets. We discuss the selection of the labels for the Dataframe in the multi-label stratified\r\nKFold processclass0∼classn,avg_w,avg_h, andavg_ratio.class0∼classnindicates the\r\npresence and quantity of each class within an image. This parameter serves to ensure\r\ndiversity in the training data by considering the various class labels within the dataset. We\r\nalso selected average width (avg_w), average height (avg_h), and average ratio (avg_ratio)\r\nas labels for the following reason. Object detection models such as Faster R-CNN[43]\r\nand YOLO use predefined anchor boxes with specific aspect ratios. Anchor boxes are\r\none of the elements used in object detection, in which frames with specific positions and\r\nsizes are set within an image. This allows the model to detect objects of various sizes\r\nand shapes, resulting in improved accuracy. If the aspect ratio distribution of bounding\r\nboxes in the training set and validation set differs, model performance can be negatively\r\naffected. If the aspect ratio distribution is not aligned, it becomes difficult to appropriately","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EFTOFzZC3ReNUZi8HxGjeqkfMSEvzCu5Rp6ocpEEFa0="},"2a09b840-9607-404e-9e93-77a4467a498a":{"id_":"2a09b840-9607-404e-9e93-77a4467a498a","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"aqr/ybTyQqCPWrrnJCvkRPK5YTobRUZNkmSe/qaeL3w="},"NEXT":{"nodeId":"48e7328a-90b4-451d-957c-0a82e90d0019","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"0W0iQ3VJ8mWPZEJxXonOZNdLvYu8sd25c0JIuF5zu0Q="}},"text":"5 of 13\r\nAlgorithm 1YOLOstratifiedKFold\r\nInput:Fimg,Ftxt,k\r\n1:Limg←empty list▷List of image file names\r\n2:forfinlistdir(Fimg)with .jpgextensiondo\r\n3:appendsplitext(f)0toLimg\r\n4:end for\r\n5:Limg←remove duplicates fromLimg\r\n6:Ltxt←empty list▷List of txt file names\r\n7:forfinlistdir(Ftxt)with .txtextensiondo\r\n8:appendsplitext(f)0toLtxt\r\n9:end for\r\n10:Ltxt←remove duplicates fromLtxt\r\n11:Ldata←empty list▷List of files data\r\n12:foreachFnameinLimgdo\r\n13:ifFnameis not inLtxtthen\r\n14:Ldata←append[Fname+′.jpg′,−1,None,None,None,None]\r\n15:else\r\n16:txt_f ile_path←joinFtxt,Fname+′.txt′\r\n17:f←opentxt_f ile_path\r\n18:lines←read all lines fromf\r\n19:iflinesis not emptythen\r\n20:Ldata←append[Fname+′.jpg′,−1,None,None,None,None]\r\n21:else\r\n22:foreachlineinlinesdo\r\n23:Ldata←append[Fname+′.jpg′,line0,line1,line2,line3,line4]\r\n24:end for\r\n25:end if\r\n26:end if\r\n27:end for\r\n28:data←create Dataframe fromLdata\r\n29:one_hot←convertdata[′class′]to one-hot encoding\r\n30:data←concatenatedataandone_hotand multiply 1000 to ’w’, ’h’ columns\r\n31:new_d f←drop ’class’, ’x’, ’y’ columns fromdataand group by ’filename’ and sum\r\n32:new_d fcnt←replace 0 with 1 in count of box\r\n33:new_d favg_w,new_d favg_h,new_d favg_ratio←Calculate average width, height, ratio\r\n34:drop ’w’,’h’ columns fromnew_d f\r\n35:msk f←initializeMultilabelStratifiedKFoldwithk\r\n36:foreach(train_idx,val_idx)inmsk f.split(new_d ff ilename,new_d f.iloc[:, 1 :])do\r\n37:X_train,X_val←select rows fromnew_d fbytrain_idx,val_idx\r\n38:end for\r\nset the position and size of the anchor boxes, which ultimately affects the accuracy of object\r\ndetection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"PouO8cNknRVi73corJSxEy5Qq0lOKZGia6nlqS9eC4o="},"48e7328a-90b4-451d-957c-0a82e90d0019":{"id_":"48e7328a-90b4-451d-957c-0a82e90d0019","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"aqr/ybTyQqCPWrrnJCvkRPK5YTobRUZNkmSe/qaeL3w="},"PREVIOUS":{"nodeId":"2a09b840-9607-404e-9e93-77a4467a498a","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"PouO8cNknRVi73corJSxEy5Qq0lOKZGia6nlqS9eC4o="}},"text":"Therefore, by selecting the average width (avg_w), average height (avg_h), and\r\naverage ratio (avg_ratio) as labels, we aim to characterize the aspect ratio distribution of\r\nthe bounding boxes in each dataset and optimize their size and aspect ratios, improving\r\nmodel performance. This allows the model to effectively detect objects with various aspect\r\nratios.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0W0iQ3VJ8mWPZEJxXonOZNdLvYu8sd25c0JIuF5zu0Q="},"90d86539-9b70-436b-8b83-7e2e82b4794e":{"id_":"90d86539-9b70-436b-8b83-7e2e82b4794e","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"E+nqdziCjvHKMl8/QYwkAllGau1ECRnG96MwbBOSQR4="},"NEXT":{"nodeId":"7b3510f1-e82b-4dad-9ff0-565f631e34dd","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"t8lUtej4cwsU/PBSR/xcoEOWWzOmgoEjHEznwjUgFho="}},"text":"6 of 13\r\n4. Experiments\r\n4.1. Datasets\r\nWe present the results of experiments conducted on public datasets to demonstrate\r\nthe efficiency of the proposed algorithm. The datasets are used for object detection pur-\r\nposes, with each image indicating the location and type of object through a bounding box. Across the datasets, the number of samples is generally far greater than the number of\r\nclasses, which enhances the training process by providing a wide range of samples for each\r\nclass. In addition, we selected a dataset that includes background images to minimize the\r\nphenomenon of background false positives. This is crucial in preventing errors in object\r\ndetection owing to features in the background. We used entropy (Equation 1) to assess the distribution of each dataset. Higher entropy\r\ncorresponds to higher uncertainty in the data and vice versa. Thus, entropy can be used as\r\na measure of how well the classes in the data are distributed. In the equation,pirepresents\r\nthe occurrence probability of each class over the entire dataset. Entro py=−n∑\r\ni=1\r\npilog(pi)(1)\r\nTable 1 shows a detailed analysis of the datasets. In datasets with a large number of\r\nclasses, entropy tends to be relatively high. If the class label distribution of a dataset is\r\ndiverse or uncertain, the stratification process may not be as effective. As stratification\r\ninvolves extracting samples while maintaining the class ratio of the original dataset, the\r\nproposed algorithm is recommended for use in scenarios in which the number of classes is\r\nless than or equal to 20. Table 1.Statistical analysis of datasets for object detection. Public datasets: COCOval2017, Pascal\r\nVOC 2012 val, and PlantDoc. Private datasets[47]: Website screenshot, Aquarium, and BCCD.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"IOQGhAOEqcKziNm3h5dev599Uor1VEzbys0+FFT7vrc="},"7b3510f1-e82b-4dad-9ff0-565f631e34dd":{"id_":"7b3510f1-e82b-4dad-9ff0-565f631e34dd","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"E+nqdziCjvHKMl8/QYwkAllGau1ECRnG96MwbBOSQR4="},"PREVIOUS":{"nodeId":"90d86539-9b70-436b-8b83-7e2e82b4794e","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"IOQGhAOEqcKziNm3h5dev599Uor1VEzbys0+FFT7vrc="},"NEXT":{"nodeId":"e565faf0-5c77-48e3-b030-233a31c26218","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"drvsi0xu4ufipQj12/pnI3VRg+aPqqtM0XSX2jyyCp0="}},"text":"Private datasets[47]: Website screenshot, Aquarium, and BCCD. Category     Dataset                                   Classes     SamplesSamples      Samples per Image         Class per Image        Entropyper ClassMin      Avg      Max      Min      Avg      Max\r\nPublic\r\nCOCO val2017[44]                    80              5000              62.5             0          7.9         96           0          2.9         14           3.39\r\nPascal VOC 2012 val[45]          20              3422             171.1            0          2.3         23           0          1.4          5            2.31\r\nPlantDoc[46]                              30              2569              85.6             0          3.4         42           0            1            3            3.17\r\nPrivate\r\nWebsite screenshot                     8               1206             150.8            2           45        2023         2          5.3          8            1.61\r\nAquarium                                    7                638               91.1             0          7.6         56           0          1.4          3            1.42\r\nBCCD                                           3                364              121.3            1         13.4        30           1          2.5          3            0.53\r\n4.2. Class distribution\r\nWe applied 10-fold cross-validation to the datasets and compared the class distribution\r\nof the original dataset with that of the split datasets. To quantify the differences in distribu-\r\ntion between them, we used the mean absolute error(MAE) (Equation 2). This calculation\r\nwas used to measure the difference in class ratios between the original dataset and split\r\ndataset. Through this, we quantitatively evaluated how accurately the split dataset reflects\r\nthe class distribution of the original dataset. In the MAE metric below,yirepresents the\r\nclass ratio of the original dataset,ˆyirepresents the class ratio of the divided dataset, andn\r\nrepresents the number of classes in the existing dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"t8lUtej4cwsU/PBSR/xcoEOWWzOmgoEjHEznwjUgFho="},"e565faf0-5c77-48e3-b030-233a31c26218":{"id_":"e565faf0-5c77-48e3-b030-233a31c26218","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"E+nqdziCjvHKMl8/QYwkAllGau1ECRnG96MwbBOSQR4="},"PREVIOUS":{"nodeId":"7b3510f1-e82b-4dad-9ff0-565f631e34dd","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"t8lUtej4cwsU/PBSR/xcoEOWWzOmgoEjHEznwjUgFho="}},"text":"MAE=1nn∑\r\ni=1\r\n|yi−ˆyi|(2)\r\nWe compared the MAE of the traditional KFold cross-validation method and that of\r\nour proposed algorithm. As shown in Table 2, in the majority of datasets in which our\r\nproposed algorithm was applied, the median MAE was lower compared to that of the\r\ntraditional KFold cross-validation. This demonstrates that our proposed algorithm was","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"drvsi0xu4ufipQj12/pnI3VRg+aPqqtM0XSX2jyyCp0="},"de2be68c-5eae-4e7a-8739-415c9c464488":{"id_":"de2be68c-5eae-4e7a-8739-415c9c464488","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"80mb/u1hzlzUdmyAjyw5M2pwVh7fBgBGEtk0Y2qgB9E="},"NEXT":{"nodeId":"9919087c-3d22-4791-96dd-8830ae8fd88d","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"t46BZdx2xhp7dmDGq2ZuawVEBJQ9O8GjOhs6U1dx+9I="}},"text":"7 of 13\r\nmore effective in preserving the class ratios than the traditional method. However, some of\r\ndatasets with an entropy of 2 or higher have found that KFold preserves class distribution\r\nbetter than our proposed algorithm. This indicates that such a phenomenon occurs when\r\nthere is high uncertainty in the label distribution. In contrast, for datasets with an entropy\r\nof 2 or lower, our algorithm consistently showed lower median MAE and variance values\r\ncompared to KFold in all cases. These results demonstrate that our algorithm provides a\r\nmore stable and consistent performance. Based on the experimental results, our proposed algorithm has been confirmed to\r\neffectively preserve class ratios while reducing variance, surpassing the commonly used\r\nKFold method. Table 2.Statistical characteristics of the subsets generated through 10-fold cross-validation. A\r\ncomparison between KFold and our proposed algorithm is presented, listing the name of each dataset,\r\nentropy of each dataset, class distribution of the training set (9 folds), and class distribution of the\r\nvalidation set (1 fold). The unit of the MAE is 1e-7. CategoryDatasetEntropyTrainValidationKFold                           OursKFold                           Ours\r\nCOCO val20173.39165±127168±1281466.5±1126.51506.5±1144.5\r\nPublicPascal VOC 2012 val2.31591.5±408.5618±3915299±37125279±3330\r\nPlantDoc3.17463.5±321.5301.5±255.54097±28032614.5±2205.5\r\nWebsite screenshot1.614864±38804092.5±3428.542897±3400935538.5±29582.5\r\nPrivateAquarium1.425172.5±4075.53943±220244031.5±33452.538541±22795\r\nBCCD0.531973.5±1417.51224±86217683.5±12738.511459±8186\r\n4.3. Training and testing\r\nTo analyze the difference in performance between KFold and our proposed algorithm,\r\nwe applied 10-fold cross-validation using the two methods in question to split the dataset. The dataset used in our study was divided according to the following procedure (Figure 2). 1.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"auK6cF1+rjGXhWpqCNfsM1aYI4sPlzw7MucwxF4bD7k="},"9919087c-3d22-4791-96dd-8830ae8fd88d":{"id_":"9919087c-3d22-4791-96dd-8830ae8fd88d","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"80mb/u1hzlzUdmyAjyw5M2pwVh7fBgBGEtk0Y2qgB9E="},"PREVIOUS":{"nodeId":"de2be68c-5eae-4e7a-8739-415c9c464488","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"auK6cF1+rjGXhWpqCNfsM1aYI4sPlzw7MucwxF4bD7k="}},"text":"1. The original dataset was split into 10 folds using our proposed algorithm. 2. The last fold (10th fold) was set as the test dataset. 3.The remaining 9 folds (k-1 folds) were combined and further split into 9 folds using\r\nKFold. 4. Similarly, the k-1 folds were split into 9 folds using our proposed algorithm. 5. Training was performed iteratively for each of the 9 fold datasets. 6. The trained models were used for inference on the fixed test dataset. 7.Finally, the inference results using KFold and our proposed algorithm were compared\r\nusing MAE. The training was conducted with a 320-pixel image as the base model of YOLOv7 on a\r\ncomputing system consisting of three NVIDIA RTX 3090 Ti GPUs. Figure 2.10-fold cross-validation split method. The yellow box represents the fold used for validation,\r\nand the green boxes represent the fold used for training. When splitting the Train/Validation data\r\nand Test data, YOLOstratifiedKFold is used, and when dividing the data onto folds, either KFold or\r\nYOLOstratifiedKFold is used.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"t46BZdx2xhp7dmDGq2ZuawVEBJQ9O8GjOhs6U1dx+9I="},"a4b645a7-51f4-445a-824d-f0f080cc0543":{"id_":"a4b645a7-51f4-445a-824d-f0f080cc0543","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"wR0Rk/xVcZYwSBFoen1xeQjAWxdfkQT0OC6Uy40SIvE="},"NEXT":{"nodeId":"151f5445-7ab8-4c04-a6fe-e854a4732112","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22MWwX4weA4EZW6v5ReXsYrTwFm6jHgEzuPKcYnnRlA="}},"text":"8 of 13\r\nFor datasets in which the entropy value is 2 or less, the training on the dataset applied\r\nwith our proposed method consistently showed higher performance in themAP_0.5metric\r\nas compared to when the KFold method was used, illustrated in Table 3. In the custom\r\ndatasets, which contain relatively few classes, the class distribution of the datasets applied\r\nwith our proposed method was more similar to the original class distribution (refer to Table\r\n2) than the public dataset, and we confirmed that the performance of our model was also\r\nhigher than that of the original KFold method. These results suggest that in training with\r\ndata that exhibit relatively low complexity, i.e., a low number of classes, our method can\r\nachieve higher performance based on themAP_0.5 metric. Table 3.Results of training by 10-fold cross validation\r\nCategoryDatasetEntropymAP_0.5mAP_0.5:0.95KFold                           OursKFold                           Ours\r\nPublic\r\nCOCO val20173.3923.50%±0.80%23.75%±1.25%14.00%±0.80%14.10%±0.70%\r\nPascal VOC 2012 val2.3146.05%±2.25%44.95%±2.85%27.95%±1.05%27.45%±1.55%\r\nPlantDoc3.1748.80%±2.60%48.20%±2.00%32.90%±1.90%33.30%±1.30%\r\nPrivate\r\nWebsite screenshot1.6145.85%±1.65%46.10%±1.50%28.00%±1.20%27.75%±1.45%\r\nAquarium1.4251.60%±3.20%52.25%±4.25%23.10%±2.10%23.25%±2.25%\r\nBCCD0.5387.90%±1.70%88.55%±1.35%55.55%±1.25%55.45%±1.55%\r\nIn contrast, it was difficult to confirm a significant improvement in themAP_0.5:0.95\r\nmetric from the training results.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"DpI41aSxy3pniWHFRcYccQBkAqy6K4cGXyfNcweJhNI="},"151f5445-7ab8-4c04-a6fe-e854a4732112":{"id_":"151f5445-7ab8-4c04-a6fe-e854a4732112","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"wR0Rk/xVcZYwSBFoen1xeQjAWxdfkQT0OC6Uy40SIvE="},"PREVIOUS":{"nodeId":"a4b645a7-51f4-445a-824d-f0f080cc0543","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"DpI41aSxy3pniWHFRcYccQBkAqy6K4cGXyfNcweJhNI="},"NEXT":{"nodeId":"b08f85cf-2f0b-4ac5-9df5-673defc915de","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"WdsqmF2BU0g/rqPcEBCfG4kTyGw+Luvxk3fWUwj70Vk="}},"text":"For our method, the datasets demonstrating better perfor-\r\nmance inmAP_0.5:0.95had less than 100 samples per class. These results suggest that our\r\nmethod effectively works even with small datasets, demonstrating good performance even\r\nfor low numbers of samples per class. This implies that when the dataset is split using our\r\nmethod, the model can recognize the rough location of the object more accurately; however,\r\nit does not reflect the detailed shape and exact location of the object during the splitting\r\nprocess. One possible way to solve this problem is to perform stratification by referring\r\nto the features within the bounding box in the image; however, this method significantly\r\nincreases the computational load required for stratification, which could greatly increase\r\nthe amount of time to perform the routine. Therefore, in this study, we minimized the time\r\nburden and verified the performance of the model by excluding image references and only\r\nusing label data stored in text files. 4.4. Inference results\r\nIn Figure 3, the inference results for the public dataset are presented. When comparing\r\nKFold and our method on the COCO dataset, the classification error for our method is\r\nless than that using KFold. While the tennis racket located in the center of the image\r\nis not accurately classified with KFold, it is correctly classified using our method. This\r\ndemonstrates that using our method to split the dataset results in a smaller classification\r\nerror as the class ratio is maintained throughout the dataset partitioning. A comparison\r\nconducted on the Pascal VOC 2012 dataset shows that the bounding box appearing in the\r\nupper left of the image is drawn incorrectly when using KFold but is drawn closer to the\r\ncorrect answer when using our method. This is also due to the reduction in localization\r\nerror[48] as the aspect ratio of the bounding box is maintained with the split of the dataset. For the PlantDoc dataset, using KFold resulted in two bounding boxes with different classes\r\ndrawn in a similar location even though there is only one ground truth, leading to multiple\r\ndetections. This occurs because the class ratios between the subsets are not maintained in\r\nthe random splitting of the dataset with this method. Conversely, this phenomenon did not\r\nappear using our method. In Figure 4, the inference results on the private dataset are shown.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"22MWwX4weA4EZW6v5ReXsYrTwFm6jHgEzuPKcYnnRlA="},"b08f85cf-2f0b-4ac5-9df5-673defc915de":{"id_":"b08f85cf-2f0b-4ac5-9df5-673defc915de","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"wR0Rk/xVcZYwSBFoen1xeQjAWxdfkQT0OC6Uy40SIvE="},"PREVIOUS":{"nodeId":"151f5445-7ab8-4c04-a6fe-e854a4732112","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22MWwX4weA4EZW6v5ReXsYrTwFm6jHgEzuPKcYnnRlA="}},"text":"In Figure 4, the inference results on the private dataset are shown. In the Website\r\nscreenshot dataset, there is a True Negative[49] owing to the complexity of features within\r\na class, but our method shows a higher IoU than does KFold. In this case, the localization\r\nerror is reduced by maintaining the aspect ratio of the bounding box similar as before\r\nafter splitting the dataset. In the Aquarium dataset, the True Negative phenomenon of","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WdsqmF2BU0g/rqPcEBCfG4kTyGw+Luvxk3fWUwj70Vk="},"437d9177-0152-4efb-b7b1-5631b8329030":{"id_":"437d9177-0152-4efb-b7b1-5631b8329030","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"L//6idPf0jT+IekX2DQ2xtdgANmKYMwphFcZ+hBbPoc="}},"text":"9 of 13\r\n(a)(COCO val2017) KFold(b)(COCO val2017) Our method\r\n(c)(Pascal VOC 2012) KFold(d)(Pascal VOC 2012) Our method\r\n(e)(Plant) KFold(f)(Plant) Our method\r\nFigure 3.Comparing KFold to our proposed method on the Public dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EcSh7y91F1XftTfe2CmvNUdSyu3egg/jv30ue7gCRdU="},"217e1611-85b6-44df-a502-bfa9fc6dc3e5":{"id_":"217e1611-85b6-44df-a502-bfa9fc6dc3e5","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"84e6gAdRDhbVJTzATJ7tvjuPrQSPYYLRLHksjhDQnyM="}},"text":"10 of 13\r\n(a)(Website Screenshot) KFold(b)(Website Screenshot) Our method\r\n(c)(Aquarium) KFold(d)(Aquarium) Our method\r\n(e)(BCCD) KFold(f)(BCCD) Our method\r\nFigure 4.Comparing KFold to our proposed method on the private dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WHlxLavYdWYNdMbao7rjA4I2OiOreOYSDHgvETQ0hYI="},"42c4ae33-0243-490a-89e2-8f35011da2e8":{"id_":"42c4ae33-0243-490a-89e2-8f35011da2e8","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ZxL9+z9mcUDQQQ9E2OFHOM+fDWKuLHS0rqwYIY832rg="},"NEXT":{"nodeId":"1a33ea67-572c-4ca7-88d9-18527cc7ccdb","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"Dz0/nv2hUhCJYwEHy2rW3+6wEA04YLN/UQcSPS9kufY="}},"text":"11 of 13\r\nno detection occurred when using KFold; however, this phenomenon was reduced when\r\nusing our method. In the BCCD dataset, by optimizing the anchor box considering the\r\nclass ratio, width, height, and ratio of the object, both KFold and our method achieved\r\nresults closer to the ground truth of the bounding box and realized a higher IoU. 5. Discussion&Conclusion\r\nThis study focuses on the importance of applying stratification in the preprocessing\r\nstage of object detection tasks. This is the first study, to our knowledge, in the field of object\r\ndetection that proposes a stratification method in dataset splitting, demonstrating the effect\r\nof alleviating class imbalance in the generation of training and validation sets. Additionally,\r\nstratification has been demonstrated to enhance the performance of an object detection\r\nmodel for a 2D image object detection dataset. However, stratification did not yield the\r\nbest results under all experimental conditions. For high-dimensional datasets with a large\r\nnumber of classes, the application of stratification was ineffective, indicating that it may\r\nnot be suitable for all types of datasets. Nevertheless, research involving classification tasks\r\nhas shown that class balancing is essential and can significantly contribute to performance\r\nimprovement for unbalanced datasets. This aspect is applicable to the object detection\r\ntask in this study, emphasizing the need for an integrated approach to apply stratification\r\nin classification and object detection tasks. The results of this study confirm that the\r\napplication of stratification must be carefully performed based on the characteristics of the\r\ndataset and distribution of the detection targets. As the features present in image data can\r\ninfluence the effect of stratification, further research in needed to improve and optimize the\r\nstratification method for use on images. Based on the results of this study, greater attention\r\ntoward stratification utilization and its role in enhancing performance of object detection\r\nmodels is expected. Author Contributions:Conceptualization, H.L., S.A.; methodology, H.L.; software, H.L.; validation,\r\nH.L., S.A.;  formal analysis, H.L.;  data curation, H.-L.;  writing—original draft preparation, H.L.;\r\nwriting—review and editing, H.L., S.A.; visualization, H.L.; funding acquisition, S.A. All authors\r\nhave read and agreed to the published version of the manuscript. Institutional Review Board Statement:Not applicable.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"yZstq298emuBT9AbnC4aegj91LW6bNAwpaPMXv4sFSI="},"1a33ea67-572c-4ca7-88d9-18527cc7ccdb":{"id_":"1a33ea67-572c-4ca7-88d9-18527cc7ccdb","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ZxL9+z9mcUDQQQ9E2OFHOM+fDWKuLHS0rqwYIY832rg="},"PREVIOUS":{"nodeId":"42c4ae33-0243-490a-89e2-8f35011da2e8","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"yZstq298emuBT9AbnC4aegj91LW6bNAwpaPMXv4sFSI="},"NEXT":{"nodeId":"55d50aef-0c6d-4d05-9e86-1a09a8152387","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"OjwSNbRUvx+cSDxbrHmOzzSiLrZNB4M+qIy2ASqrel8="}},"text":"Institutional Review Board Statement:Not applicable. Data Availability Statement:Datasets used in this study are publicly available on the web(https:\r\n//public.roboflow.com/). Acknowledgments:This work was supported by the National Research Foundation of Korea(NRF)\r\ngrant funded by the Korea government(MSIT)(No. NRF-2022R1A4A1023248)\r\nConflicts of Interest:The authors declare no conflict of interest. References\r\n1.Voulodimos, Athanasios, et al. \"Deep learning for computer vision: A brief review. \" Computational intelligence and neuroscience\r\n2018 (2018). 2. Li, Zhuoling, et al. \"CLU-CNNs: Object detection for medical images. \" Neurocomputing 350 (2019): 53-59. 3.Mao, Qi-Chao, et al. \"Finding every car: a traffic surveillance multi-scale vehicle object detection method. \" Applied Intelligence\r\n50 (2020): 3125-3136. 4.Yin, Jiale, et al. \"The infrared moving object detection and security detection related algorithms based on W4 and frame difference. \"\r\nInfrared Physics & Technology 77 (2016): 302-315. 5. Cheng, Wen-Huang, et al. \"Fashion meets computer vision: A survey. \" ACM Computing Surveys (CSUR) 54.4 (2021): 1-41. 6.Zhao, Jiawei, Rahat Masood, and Suranga Seneviratne. \"A review of computer vision methods in network security. \" IEEE\r\nCommunications Surveys & Tutorials 23.3 (2021): 1838-1878. 7.Sood, Shivani, and Harjeet Singh. \"Computer vision and machine learning based approaches for food security:  A review. \"\r\nMultimedia Tools and Applications 80.18 (2021): 27973-27999. 8.Lu, Yuzhen, and Sierra Young. \"A survey of public datasets for computer vision tasks in precision agriculture. \" Computers and\r\nElectronics in Agriculture 178 (2020): 105760. 9.Tian, Hongkun, et al. \"Computer vision technology in agricultural automation—A review.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Dz0/nv2hUhCJYwEHy2rW3+6wEA04YLN/UQcSPS9kufY="},"55d50aef-0c6d-4d05-9e86-1a09a8152387":{"id_":"55d50aef-0c6d-4d05-9e86-1a09a8152387","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ZxL9+z9mcUDQQQ9E2OFHOM+fDWKuLHS0rqwYIY832rg="},"PREVIOUS":{"nodeId":"1a33ea67-572c-4ca7-88d9-18527cc7ccdb","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"Dz0/nv2hUhCJYwEHy2rW3+6wEA04YLN/UQcSPS9kufY="}},"text":"\"Computer vision technology in agricultural automation—A review. \" Information Processing in Agriculture\r\n7.1 (2020): 1-19.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"OjwSNbRUvx+cSDxbrHmOzzSiLrZNB4M+qIy2ASqrel8="},"c5b506f8-b797-4a13-b7b9-67d5e2bd7702":{"id_":"c5b506f8-b797-4a13-b7b9-67d5e2bd7702","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22iBDjl7Ga0Ors76FpIaeMMGwko6LW+Whdlewje1SKE="},"NEXT":{"nodeId":"d014b102-0c39-4c01-ba57-46b4976adcf0","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"pqQFYDz+5/Z870q5Zr3x6V/s1OE/k4ICxRpcIt/Sj6A="}},"text":"12 of 13\r\n10.Scime, Luke, and Jack Beuth. \"Anomaly detection and classification in a laser powder bed additive manufacturing process using\r\na trained computer vision algorithm. \" Additive Manufacturing 19 (2018): 114-126. 11.Qiu,  Weichao,  and Alan Yuille. \"Unrealcv:  Connecting computer vision to unreal engine. \" Computer Vision–ECCV 2016\r\nWorkshops:  Amsterdam,  The Netherlands,  October 8-10 and 15-16,  2016,  Proceedings,  Part III 14. Springer International\r\nPublishing, 2016. 12.Xu, Yanling, et al. \"Computer vision technology for seam tracking in robotic GTAW and GMAW. \" Robotics and computer-\r\nintegrated manufacturing 32 (2015): 25-36. 13.Kazemian, Ali, et al. \"Computer vision for real-time extrusion quality monitoring and control in robotic construction. \" Automation\r\nin Construction 101 (2019): 92-98. 14.Demir, Ilke, et al. \"Deepglobe 2018: A challenge to parse the earth through satellite images. \" Proceedings of the IEEE Conference\r\non Computer Vision and Pattern Recognition Workshops. 2018. 15.Rad, Mohammad Saeed, et al. \"A computer vision system to localize and classify wastes on the streets. \" Computer Vision Systems:\r\n11th International Conference, ICVS 2017, Shenzhen, China, July 10-13, 2017, Revised Selected Papers 11. Springer International\r\nPublishing, 2017. 16.Grys, Ben T., et al. \"Machine learning and computer vision approaches for phenotypic profiling. \" Journal of Cell Biology 216.1\r\n(2017): 65-71. 17.Kremer, Jan, et al. \"Big universe, big data: Machine learning and image analysis for astronomy. \" IEEE Intelligent Systems 32.2\r\n(2017): 16-22. 18. Zou, Zhengxia, et al. \"Object detection in 20 years: A survey. \" Proceedings of the IEEE (2023). 19.Rawat, Waseem, and Zenghui Wang.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HTpICOqQ1E7lbCV7irBnEA6QoaI5e8Fi7nKEVDW6ok4="},"d014b102-0c39-4c01-ba57-46b4976adcf0":{"id_":"d014b102-0c39-4c01-ba57-46b4976adcf0","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22iBDjl7Ga0Ors76FpIaeMMGwko6LW+Whdlewje1SKE="},"PREVIOUS":{"nodeId":"c5b506f8-b797-4a13-b7b9-67d5e2bd7702","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"HTpICOqQ1E7lbCV7irBnEA6QoaI5e8Fi7nKEVDW6ok4="},"NEXT":{"nodeId":"5bc09840-be95-4c98-b887-dbdbdf567d40","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ZfT+lvnxZoCor6FH1YSj+OgULGlEbZWIJs4LEJFiou0="}},"text":"19.Rawat, Waseem, and Zenghui Wang. \"Deep convolutional neural networks for image classification: A comprehensive review. \"\r\nNeural computation 29.9 (2017): 2352-2449. 20. Kortli, Yassin, et al. \"Face recognition systems: A survey. \" Sensors 20.2 (2020): 342. 21. Elasri, Mohamed, et al. \"Image generation: A review. \" Neural Processing Letters 54.5 (2022): 4609-4646. 22.Feng, Di, et al. \"A review and comparative study on probabilistic object detection in autonomous driving. \" IEEE Transactions on\r\nIntelligent Transportation Systems 23.8 (2021): 9961-9980. 23.Elhoseny, Mohamed. \"Multi-object detection and tracking (MODT) machine learning model for real-time video surveillance\r\nsystems. \" Circuits, Systems, and Signal Processing 39 (2020): 611-630. 24.Xu, Ge, et al. \"The Object Detection, Perspective and Obstacles In Robotic: A Review. \" EAI Endorsed Transactions on AI and\r\nRobotics 1.1 (2022). 25.Liu, Luyang, Hongyu Li, and Marco Gruteser. \"Edge assisted real-time object detection for mobile augmented reality. \" The 25th\r\nannual international conference on mobile computing and networking. 2019. 26. Li, Zhuoling, et al. \"CLU-CNNs: Object detection for medical images. \" Neurocomputing 350 (2019): 53-59. 27.Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection. \" Proceedings of the IEEE conference on computer\r\nvision and pattern recognition. 2016. 28.Du, Lixuan, Rongyu Zhang, and Xiaotian Wang. \"Overview of two-stage object detection algorithms. \" Journal of Physics:\r\nConference Series. Vol. 1544. No. 1. IOP Publishing, 2020. 29.Redmon, Joseph, and Ali Farhadi. \"YOLO9000: better, faster, stronger.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"pqQFYDz+5/Z870q5Zr3x6V/s1OE/k4ICxRpcIt/Sj6A="},"5bc09840-be95-4c98-b887-dbdbdf567d40":{"id_":"5bc09840-be95-4c98-b887-dbdbdf567d40","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22iBDjl7Ga0Ors76FpIaeMMGwko6LW+Whdlewje1SKE="},"PREVIOUS":{"nodeId":"d014b102-0c39-4c01-ba57-46b4976adcf0","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"pqQFYDz+5/Z870q5Zr3x6V/s1OE/k4ICxRpcIt/Sj6A="},"NEXT":{"nodeId":"cd8017bc-fae4-4419-b3d8-3567fa4f46fc","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"8WqHiQeY5AhRhohjqYyOaFdj/kGEiSvsIJyNXhn7MS0="}},"text":"\"YOLO9000: better, faster, stronger. \" Proceedings of the IEEE conference on computer vision\r\nand pattern recognition. 2017. 30.Zhong, Yuanyi, et al. \"Anchor box optimization for object detection. \" Proceedings of the IEEE/CVF Winter Conference on\r\nApplications of Computer Vision. 2020. 31. Redmon, Joseph, and Ali Farhadi. \"Yolov3: An incremental improvement. \" arXiv preprint arXiv:1804.02767 (2018). 32.Bochkovskiy, Alexey, Chien-Yao Wang, and Hong-Yuan Mark Liao. \"Yolov4: Optimal speed and accuracy of object detection. \"\r\narXiv preprint arXiv:2004.10934 (2020). 33. Jocher, Glenn, et al. \"ultralytics/yolov5: v3. 0. \" Zenodo (2020). 34.Wang, Chien-Yao, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. \"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art\r\nfor real-time object detectors. \" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. 35. Zhang, Zhi, et al. \"Bag of freebies for training object detection neural networks. \" arXiv preprint arXiv:1902.04103 (2019). 36.Prusty, Sashikanta, Srikanta Patnaik, and Sujit Kumar Dash. \"SKCV: Stratified K-fold cross-validation on ML classifiers for\r\npredicting cervical cancer. \" Frontiers in Nanotechnology 4 (2022): 972421. 37.Wu, Qingyao, et al. \"ForesTexter: An efficient random forest algorithm for imbalanced text categorization. \" Knowledge-Based\r\nSystems 67 (2014): 105-116. 38. Arlot, Sylvain, and Alain Celisse. \"A survey of cross-validation procedures for model selection. \" (2010): 40-79. 39.Tsoumakas, Grigorios, and Ioannis Katakis.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ZfT+lvnxZoCor6FH1YSj+OgULGlEbZWIJs4LEJFiou0="},"cd8017bc-fae4-4419-b3d8-3567fa4f46fc":{"id_":"cd8017bc-fae4-4419-b3d8-3567fa4f46fc","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"22iBDjl7Ga0Ors76FpIaeMMGwko6LW+Whdlewje1SKE="},"PREVIOUS":{"nodeId":"5bc09840-be95-4c98-b887-dbdbdf567d40","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"ZfT+lvnxZoCor6FH1YSj+OgULGlEbZWIJs4LEJFiou0="}},"text":"39.Tsoumakas, Grigorios, and Ioannis Katakis. \"Multi-label classification: An overview. \" International Journal of Data Warehousing\r\nand Mining (IJDWM) 3.3 (2007): 1-13. 40. Trohidis, Konstantinos, et al. \"Multi-label classification of music into emotions. \" ISMIR. Vol. 8. 2008. 41.Nigam, Priyanka. Applying deep learning to ICD-9 multi-label classification from medical records. Technical report, Stanford\r\nUniversity, 2016. 42.Yang, Bishan, et al. \"Effective multi-label active learning for text classification. \" Proceedings of the 15th ACM SIGKDD international\r\nconference on Knowledge discovery and data mining. 2009.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"8WqHiQeY5AhRhohjqYyOaFdj/kGEiSvsIJyNXhn7MS0="},"9034dcd5-c42c-4a07-9829-7ddde5e5ef24":{"id_":"9034dcd5-c42c-4a07-9829-7ddde5e5ef24","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf","file_name":"Improving_the_performance_of_object_detection_by_preserving_label_distribution.pdf"},"hash":"M31eDADWMvwPytQzLlad5DomjiqmeumMYTKDehHxREU="}},"text":"13 of 13\r\n43.Ren, Shaoqing, et al. \"Faster r-cnn:  Towards real-time object detection with region proposal networks. \" Advances in neural\r\ninformation processing systems 28 (2015). 44.Lin, Tsung-Yi, et al. \"Microsoft coco: Common objects in context. \" Computer Vision–ECCV 2014: 13th European Conference,\r\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer International Publishing, 2014. 45.Everingham, Mark, et al. \"The pascal visual object classes challenge: A retrospective. \" International journal of computer vision\r\n111 (2015): 98-136. 46.Singh, Davinder, et al. \"PlantDoc: A dataset for visual plant disease detection. \" Proceedings of the 7th ACM IKDD CoDS and 25th\r\nCOMAD. 2020. 249-253. 47.Ciaglia, Floriana, et al. \"Roboflow 100: A Rich, Multi-Domain Object Detection Benchmark. \" arXiv preprint arXiv:2211.13523\r\n(2022). 48.Hoiem, Derek, Yodsawalai Chodpathumwan, and Qieyun Dai. \"Diagnosing error in object detectors. \" European conference on\r\ncomputer vision. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012. 49.Padilla, Rafael, Sergio L. Netto, and Eduardo AB Da Silva. \"A survey on performance metrics for object-detection algorithms. \"\r\n2020 international conference on systems, signals and image processing (IWSSIP). IEEE, 2020.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"h815ZhbkHVzR6lygNMyWv2kwnhgnzramJX2pIBh+1eI="},"5ea31394-9794-42c4-a386-bbb5ec59bf04":{"id_":"5ea31394-9794-42c4-a386-bbb5ec59bf04","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"N+RsVZNtTzx8fNmXOdCDO3S14gDsvkaxtccBLnlwiNE="},"NEXT":{"nodeId":"8b7a9770-3861-40e8-b055-880f323a9658","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Ws8XCsV1BkvAOGpy3aD20HN4tYi4e/B06BVT/B4UPm4="}},"text":"Learning Data Augmentation Strategies for\r\nObject Detection\r\nBarret Zoph\u0003, Ekin D. Cubuk\u0003, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon\r\nShlens, and Quoc V. Le\r\nGoogle Research, Brain Team\r\nAbstract.Much research on object detection focuses on building bet-\r\nter model architectures and detection algorithms. Changing the model\r\narchitecture, however, comes at the cost of adding more complexity to\r\ninference, making models slower. Data augmentation, on the other hand,\r\ndoesn't add any inference complexity, but is insu\u000eciently studied in ob-\r\nject detection for two reasons. First it is more di\u000ecult to design plausible\r\naugmentation strategies for object detection than for classi\fcation, be-\r\ncause  one  must  handle  the  complexity  of  bounding  boxes  if  geometric\r\ntransformations are applied. Secondly, data augmentation attracts less\r\nresearch attention perhaps because it is believed to add less value and\r\nto transfer poorly compared to advances in network architectures. This paper serves two main purposes. First, we propose to use AutoAug-\r\nment [3] to design better data augmentation strategies for object detec-\r\ntion because it can address the di\u000eculty of designing them. Second, we\r\nuse the method to assess the value of data augmentation in object detec-\r\ntion and compare it against the value of architectures. Our investigation\r\ninto data augmentation for object detection identi\fes two surprising re-\r\nsults. First, by changing the data augmentation strategy to our method,\r\nAutoAugment for detection, we can improve RetinaNet with a ResNet-50\r\nbackbone from 36.7 to 39.0 mAP on COCO, a di\u000berence of +2.3mAP. This  gain  exceeds  the  gain  achieved  by  switching  the  backbone  from\r\nResNet-50 to ResNet-101 (+2.1mAP), which incurs additional training\r\nand inference costs. The second surprising \fnding is that our strategies\r\nfound  on  the  COCO  dataset  transfer  well  to  the  PASCAL  dataset  to\r\nimprove accuracy by +2.7mAP.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"XCwwBTyXNrF3r5zrEmQeSHafijoZ9roO/ILLKBAGLqc="},"8b7a9770-3861-40e8-b055-880f323a9658":{"id_":"8b7a9770-3861-40e8-b055-880f323a9658","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"N+RsVZNtTzx8fNmXOdCDO3S14gDsvkaxtccBLnlwiNE="},"PREVIOUS":{"nodeId":"5ea31394-9794-42c4-a386-bbb5ec59bf04","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"XCwwBTyXNrF3r5zrEmQeSHafijoZ9roO/ILLKBAGLqc="}},"text":"These results together with our system-\r\natic studies of data augmentation call into question previous assumptions\r\nabout the role and transferability of architectures versus data augmenta-\r\ntion. In particular, changing the augmentation may lead to performance\r\ngains that are equally transferable as changing the underlying architec-\r\nture. 1  Introduction\r\nMuch  work  in  object  detection  was  devoted  to  building  better  model  archi-\r\ntectures or detection algorithms [11, 35, 22, 13, 10, 39, 34, 33, 32]. Although these\r\n\u0003Equal contribution. yCode  and  models  are  available  athttps://github.com/tensorflow/tpu/tree/\r\nmaster/models/official/detection","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Ws8XCsV1BkvAOGpy3aD20HN4tYi4e/B06BVT/B4UPm4="},"12a01bcb-1840-4900-b7a5-4bd3ca431ee5":{"id_":"12a01bcb-1840-4900-b7a5-4bd3ca431ee5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Nkp41IB3ddk/oJxe20u1qrRSymj14Rekh7kC1/Z9VIU="},"NEXT":{"nodeId":"e27b9a0a-cc31-4a26-839d-9ce99a4b1871","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"IcIZkdXS2vLsbYDt/t2koNahGPV/I+v+0Dq7M8AjLJc="}},"text":"2           Zoph et al. changes often lead to more accurate models, they also add complexity that po-\r\ntentially slows down the detection system at both training and inference. Data augmentation, on the other hand, is relatively understudied in object\r\ndetection. So  far  the  most  common  distortions  are  horizontal  \rips  and  scale\r\njittering  [23]. We  suspect  that  the  lack  of  research  in  this  area  is  due  to  two\r\nreasons. First, object detection often comes with bounding boxes, so transferring\r\nstrategies from image classi\fcation to object detection will be sub-optimal. These\r\nadded degrees of freedom from the bounding boxes suggest some automation is\r\nneeded to \fnd a very good augmentation policy. Secondly, it is also a common\r\nbelief that custom data augmentation adds smaller performance improvements\r\nthan architectures and transfers poorly from one detection dataset to another. Here we investigate the value of data augmentation in object detection. As\r\naforementioned, since it can be di\u000ecult to design good data augmentation strate-\r\ngies for object detection, and inspired by the recent success of AutoAugment for\r\nclassi\fcation [3], we use AutoAugment [3] to \fnd good combinations of trans-\r\nformations  for  detection. To  tailor  AutoAugment  for  detection,  we  add  novel\r\noperations  that  handle  bounding  boxes  di\u000berently  from  the  image  which  im-\r\nproves results. Our experiments with AutoAugment for detection identify two key surprising\r\n\fndings. First,  data  augmentation  is  more  valuable  than  commonly  believed. In particular, by changing the data augmentation, we can improve RetinaNet\r\nwith a ResNet-50 backbone from 36.7 to 39.0 mAP on COCO, a di\u000berence of\r\n2.3mAP. This  gain  is  even  slightly  better  than  changing  the  backbone  from\r\nResNet-50 to ResNet-101 which only gives +2.1mAP improvement but with a\r\nhigher cost of training and inference.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"M0da9miJRZkdSBqtw89WApUjPopIKDZXzycj7kCrMAk="},"e27b9a0a-cc31-4a26-839d-9ce99a4b1871":{"id_":"e27b9a0a-cc31-4a26-839d-9ce99a4b1871","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Nkp41IB3ddk/oJxe20u1qrRSymj14Rekh7kC1/Z9VIU="},"PREVIOUS":{"nodeId":"12a01bcb-1840-4900-b7a5-4bd3ca431ee5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"M0da9miJRZkdSBqtw89WApUjPopIKDZXzycj7kCrMAk="}},"text":"Secondly, it is also common wisdom that\r\nintricate data augmentation can \\over\ft\" to the dataset of interest and does not\r\ntransfer well to other datasets. Our experiments show that data augmentation is\r\ntransferable between detection datasets just like architectures. For example, the\r\nbest data augmentation found on COCO transfers well to PASCAL to improve\r\nthe  accuracy  by  +2.7mAP. This  means  that  the  augmentation  strategies  we\r\nfound on COCO can be used directly on future object detection datasets without\r\nchanging any parameters. In summary, our main contributions are as follows:\r\n{We propose a novel set of data augmentation operations that uniquely act\r\nupon the content of bounding boxes and even sometimes change their loca-\r\ntion. {We implement a search method based on AutoAugment [3] to combine and\r\noptimize data augmentation policies for object detection problems by utiliz-\r\ning novel operations speci\fc to bounding box annotations. {We  show  surprising  results  that  improving  data  augmentation  can  be  as\r\ne\u000bective  as  improving  architectures  while  adding  no  cost  to  the  inference\r\nand minimal cost to training. {We show surprising results that data augmentation strategies are transfer-\r\nable across di\u000berent detection datasets, architectures and algorithms.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"IcIZkdXS2vLsbYDt/t2koNahGPV/I+v+0Dq7M8AjLJc="},"b948d7ec-c2f2-4d10-a5fe-107121cd371d":{"id_":"b948d7ec-c2f2-4d10-a5fe-107121cd371d","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"1k3zS8Mnuur4hu/H/rLj9FqX1KGEgXmDgbhYHyBzpZ8="},"NEXT":{"nodeId":"93ddac08-4560-4efa-b682-391c200066b2","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"qhtODE+cDWkKpAqU/UHTY7JllEo4ZSLgAzreckjKV1w="}},"text":"Learning Data Augmentation Strategies for Object Detection           3\r\n2  AutoAugment for Object Detection\r\n2.1    Search Space De\fnition\r\nAs  mentioned  above,  commonly  used  data  augmentation  methods  for  object\r\ndetection are quite simple: horizontal \ripping and scale jittering. Here we would\r\nlike to add more augmentation operations to object detection to further boost\r\nthe value of data augmentation. Hence we turn our attention to a search method\r\nto compose basic image transformations into sophisticated distortions to improve\r\ngeneralization performance. The search method expands on our previous work,\r\nAutoAugment  [3],  where  a  reinforcement  learning  method  is  used  to  learn  to\r\ncompose  image  processing  operations  mainly  from  the  Python  Image  Library\r\n(PIL). Here we adapt the method to perform well on object detection. Our key ob-\r\nservation is that object detection introduces many additional complications such\r\nas maintaining consistency between a bounding box location and a distorted im-\r\nage. To handle this complication we explore how to change the bounding box\r\nlocations when geometric transformations are applied to the image. To further\r\nadapt our method to object detection, we notice that bounding box annotations\r\nopen  up  the  possibility  of  introducing  augmentation  operations  that  uniquely\r\nact upon the contents within each bounding box. Using this observation, we add\r\nmany new augmentation operations that work on each bounding box indepen-\r\ndently in addition to existing image augmentation operations. For  our  data  augmentation  policy1we  use  the  following  parameterization. We de\fne an augmentation policy as a unordered set ofKsub-policies. During\r\ntraining one of theKsub-policies will be selected at random and then applied\r\nto the current image. Each sub-policy hasNimage transformations which are\r\napplied sequentially. We turn this problem of searching for a data augmentation\r\npolicy  into  a  discrete  optimization  problem  by  creating  a  search  space. This\r\nspace gives us the \rexibility to have a diversity of operations in a single policy,\r\nwhile  having  a  constraint  on  how  large  the  space  can  be.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"hrCT/SPiE9jelcxF9rI18HGyV50riLmB5/bRjpOobcw="},"93ddac08-4560-4efa-b682-391c200066b2":{"id_":"93ddac08-4560-4efa-b682-391c200066b2","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"1k3zS8Mnuur4hu/H/rLj9FqX1KGEgXmDgbhYHyBzpZ8="},"PREVIOUS":{"nodeId":"b948d7ec-c2f2-4d10-a5fe-107121cd371d","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"hrCT/SPiE9jelcxF9rI18HGyV50riLmB5/bRjpOobcw="}},"text":"We  also  want  our\r\naugmentation  policy  to  bene\ft  from  augmentation  diversity,  which  has  been\r\nfound to be useful in the classi\fcation domain [3]. In  our  implementation,  our  search  space  consistsK=  5  sub-policies  with\r\neach sub-policy consisting ofN= 2 operations applied in sequence to a single\r\nimage. Additionally, each operation is also associated with two hyperparameters\r\nspecifying the probability of applying the operation, and the magnitude of the\r\noperation. Figure 1 (bottom text) demonstrates 5 of the learned sub-policies. The\r\nprobability parameter introduces a notion of stochasticity into the augmentation\r\npolicy where the selected augmentation operation is applied to the image with\r\nthe speci\fed probability. Our goal was to include as many augmentation transformations as possible\r\nto get the best understanding of what operations would be useful for object de-\r\ntection. To limit the complexity of including every operation, we identi\fed 26\r\n1In this paper, we use \\policy\" and \\strategy\" interchangeably.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"qhtODE+cDWkKpAqU/UHTY7JllEo4ZSLgAzreckjKV1w="},"7896d840-84ff-4fe7-a1a4-8d7a4354d2d5":{"id_":"7896d840-84ff-4fe7-a1a4-8d7a4354d2d5","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"ACQGHl3+oLoyj+mk0vZ6HNFVpP5S2HKqJRRLSI62rAw="},"NEXT":{"nodeId":"858842fc-a9ec-40ac-924c-8f0b754980ef","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"AHUWd3lVkaVD7iYhq0HcWNTIN6E1PhFuH6HqIKH/Ygo="}},"text":"4           Zoph et al.Batch 1Batch 2Batch 3Batch 4\r\nSub-policy 1\r\nSub-policy 2\r\nSub-policy 3\r\nSub-policy 4\r\nSub-policy 5\r\nSub-policy 1.(Color, 0.2, 8), (Rotate, 0.8, 10)\r\nSub-policy 2.(BBoxOnlyShearY, 0.8, 5)\r\nSub-policy 3.(SolarizeAdd, 0.6, 8), (Brightness, 0.8, 10)\r\nSub-policy 4.(ShearY, 0.6, 10), (BBoxOnlyEqualize,0.6, 8)\r\nSub-policy 5.(Equalize, 0.6, 10), (TranslateX, 0.2, 2)\r\nFig. 1. Examples of data augmentation sub-policies.5 examples of learned sub-\r\npolicies applied to one example image. Each column corresponds to a di\u000berent random\r\nsample of the corresponding sub-policy. Each step of an augmentation sub-policy con-\r\nsists of a triplet corresponding to the operation, the probability of application and a\r\nmagnitude measure. The bounding box is adjusted to maintain consistency with the\r\napplied augmentation. Note the probability and magnitude are discretized values (see\r\ntext for details). unique operations for the search space, 13 of which are novel to object detec-\r\ntion, that appear to cover the widest range of available transformations. These\r\noperations  were  implemented  in  TensorFlow  [1]. We  brie\ry  summarize  these\r\noperations, but reserve the details for the Appendix A.1:\r\n{  Color operations. Distort color channels, without impacting the locations\r\nof the bounding boxes (e.g.,Equalize,Contrast,Brightness).2\r\n{  Geometric operations. Geometrically distort the image, which correspond-\r\ningly  alters  the  location  and  size  of  the  bounding  box  annotations  (e.g.,\r\nRotate,ShearX,TranslationY, etc.). Note that for any operations that ef-\r\nfect the geometry of an image, we likewise modify the bounding box size and\r\nlocation to maintain consistency.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"z1hqe8k7nOFuqoJtOlSHnckA+uVh3vPEGXsNvLf0F2k="},"858842fc-a9ec-40ac-924c-8f0b754980ef":{"id_":"858842fc-a9ec-40ac-924c-8f0b754980ef","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"ACQGHl3+oLoyj+mk0vZ6HNFVpP5S2HKqJRRLSI62rAw="},"PREVIOUS":{"nodeId":"7896d840-84ff-4fe7-a1a4-8d7a4354d2d5","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"z1hqe8k7nOFuqoJtOlSHnckA+uVh3vPEGXsNvLf0F2k="}},"text":"2The color transformations largely derive from transformation in the Python Image\r\nLibrary (PIL).https://pillow.readthedocs.io/en/5.1.x/","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"AHUWd3lVkaVD7iYhq0HcWNTIN6E1PhFuH6HqIKH/Ygo="},"1da85bdd-ff24-484e-bca1-944ee0fbe0d4":{"id_":"1da85bdd-ff24-484e-bca1-944ee0fbe0d4","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"exbdbgEoyVkElYmfBqLOC/QhrnllmWFBOeFKTeUCfjs="},"NEXT":{"nodeId":"2e037603-8c47-482a-9297-d430b7789d2b","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"SncgxNH0GEjhibvuhqLK4jMJk0HgaNzylP4XIp2L5Dc="}},"text":"Learning Data Augmentation Strategies for Object Detection           5\r\n{  Bounding box operations. Only distort the pixel content contained within\r\nthe bounding box annotations (e.g.,BBoxOnlyEqualize,BBoxOnlyRotate,\r\nBBoxOnlyFlipLR). Since many augmentation operations have a \\strength\" parameter, such as\r\nhow many degrees to rotate an image, we associate with each operation a cus-\r\ntom range of parameter values. We map this range onto a standardized range\r\nfrom  0  to  10  for  all  operations. We  discretize  the  range  of  magnitude  intoL\r\nuniformly-spaced values so that these parameters are amenable to discrete opti-\r\nmization. Similarly, we discretize the probability of applying an operation into\r\nMuniformly-spaced values. In preliminary experiments we found that setting\r\nL= 6 andM= 6 provides a good balance between computational tractability\r\nand  learning  performance  with  an  RL  algorithm. Thus,  \fnding  a  good  sub-\r\npolicy becomes a search in a discrete space containing a cardinality of (26LM)2. In  particular,  to  search  over  5  sub-policies,  the  search  space  contains  roughly\r\n(26\u00026\u00026)2\u00025\u00195:2\u00021029possibilities and requires an e\u000ecient search technique\r\nto navigate this space. This number comes from the fact that each operation in\r\na subpolicy has 26 transformation options and there are also 6 options for the\r\nprobability  and  6  options  for  the  magnitude. This  number  gets  raised  to  the\r\n(2\u00025) as there are 2 operations per subpolicy and 5 di\u000berent subpolicies. 2.2    Controller Settings\r\nNow  that we  have our  search space  setup, we  want  to optimize it  to \fnd the\r\naugmentation  policy  that  allows  the  model  to  achieve  the  best  validation  set\r\nperformance. As done in other work we will have a controller that will predict an\r\naugmentation policy, which will be used to train a neural network (child model)\r\non  a  detection  dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wMp8L1Bs8lUcodCM2ZHYyH22RUgYRegnIy/ATASx7P4="},"2e037603-8c47-482a-9297-d430b7789d2b":{"id_":"2e037603-8c47-482a-9297-d430b7789d2b","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"exbdbgEoyVkElYmfBqLOC/QhrnllmWFBOeFKTeUCfjs="},"PREVIOUS":{"nodeId":"1da85bdd-ff24-484e-bca1-944ee0fbe0d4","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"wMp8L1Bs8lUcodCM2ZHYyH22RUgYRegnIy/ATASx7P4="}},"text":"After  training  the  network  we  evaluate  its  validation\r\naccuracy to judge how well the augmentation policy performed. Using this signal\r\nwe update the controller to generate better and better augmentation policies over\r\ntime according to the validation set. Many  methods  exist  for  addressing  the  discrete  optimization  problem  of\r\ntraining the controller including reinforcement learning [47], evolutionary meth-\r\nods [31] and sequential model-based optimization [25]. In this work, we choose\r\nto build on previous work by structuring the discrete optimization problem as\r\nthe output space of an RNN and employ reinforcement learning to update the\r\nweights  of  the  model  [47]. The  training  setup  for  the  RNN  is  similar  to  [47,\r\n48, 4, 3]. We employ the proximal policy optimization (PPO) [37] for the search\r\nalgorithm. The RNN is unrolled 30 steps to predict a single augmentation policy. The\r\nnumber of unrolled steps, 30, corresponds to the number of discrete predictions\r\nthat must be made in order to enumerate 5 sub-policies. Each sub-policy consists\r\nof 2 operations and each operation consists of 3 predictions corresponding to the\r\nselected image transformation, probability of application and magnitude of the\r\ntransformation.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"SncgxNH0GEjhibvuhqLK4jMJk0HgaNzylP4XIp2L5Dc="},"a18fb187-5635-4f9e-98cc-03a79ac60cff":{"id_":"a18fb187-5635-4f9e-98cc-03a79ac60cff","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"APJZov96PgRMP9KcbcEK0f3Ct0IcTNQXojhw4agS60U="},"NEXT":{"nodeId":"3288e014-46d6-49a9-ac00-c6f3cdd67450","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"F6kNvi9rjw27AT4EHlDeA08zVlbk2qo7mDB0v4E3EGI="}},"text":"6           Zoph et al. In order to train each child model, we selected 5K images from the COCO\r\ntraining  set  as  we  found  that  searching  directly  on  the  full  COCO  dataset  to\r\nbe  prohibitively  expensive. We  found  that  policies  identi\fed  with  this  subset\r\nof data generalize to the full dataset while providing signi\fcant computational\r\nsavings. Brie\ry,  we  trained  each  child  model3from  scratch  on  the  5K  COCO\r\nimages with the ResNet-50 backbone [14] and RetinaNet detector [23] using a\r\ncosine learning rate decay [27]. The reward signal for the controller is the mAP\r\non a custom held-out validation set of 7392 images created from a subset of the\r\nCOCO training set. The RNN controller is trained over 20K augmentation policies. The search\r\nemployed  400  TPU's  [18]  over  48  hours  with  identical  hyper-parameters  for\r\nthe controller as [48]. The search can be sped up using the recently developed,\r\nmore e\u000ecient search methods based on population based training [15] or density\r\nmatching [21]. Since our learned augmentation method is being used as a method\r\nto study and evaluate the performance of data augmentation on COCO, we leave\r\nthe algorithmic speedup to future work. The learned augmentation policy can\r\nbe seen in Table 7 in the Appendix. 3  Experiments\r\nWe applied our search method to the COCO dataset with a ResNet-50 [14] back-\r\nbone with RetinaNet [23]. We are mainly interested in answering the following\r\ntwo questions:\r\n{How important is data augmentation for object detection? {How generalizable are the found data augmentation policies? To answer the \frst question, we compare the improvement of our AutoAug-\r\nment data augmentation policy to changing the model architecture across various\r\nsizes. We additionally show that the augmentation policy can push the state-of-\r\nthe-art using a much simpler system than previous results on COCO.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"aFBc4TaOsQCzfHvVi8NCPOYoIjqSVbs7BydDWzmayM4="},"3288e014-46d6-49a9-ac00-c6f3cdd67450":{"id_":"3288e014-46d6-49a9-ac00-c6f3cdd67450","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"APJZov96PgRMP9KcbcEK0f3Ct0IcTNQXojhw4agS60U="},"PREVIOUS":{"nodeId":"a18fb187-5635-4f9e-98cc-03a79ac60cff","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"aFBc4TaOsQCzfHvVi8NCPOYoIjqSVbs7BydDWzmayM4="}},"text":"To answer\r\nthe  second  question,  we  use  the  top  policy  found  on  COCO  and  apply  it  to\r\ndi\u000berent datasets, dataset sizes and architecture con\fgurations to examine gen-\r\neralizability. Finally, we study properties of what kinds of operations are needed\r\nfor a good augmentation policy on an object detection dataset. 3.1    Understanding the policies found by AutoAugment\r\nSearching  for  the  data  augmentation  strategy  on  5K  COCO  training  images\r\nresulted in the \fnal augmentation policy that will be used in all of our results. Before  diving  into  the  results,  we  would  like  to  inspect  the  best  policy  found\r\nduring the search to gain a better understanding of what operations are used. 3We employed a base learning rate of 0.08 over 150 epochs; image size was 640\u0002640;\r\n\u000b= 0:25 and\r= 1:5 for the focal loss parameters; weight decay of 1e\u00004; batch\r\nsize was 64","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"F6kNvi9rjw27AT4EHlDeA08zVlbk2qo7mDB0v4E3EGI="},"28af6010-36d5-4c6f-8686-00166266ae4d":{"id_":"28af6010-36d5-4c6f-8686-00166266ae4d","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"8RnwWCCu1ACf+VxKy6e46PZCeR0qyoO+CP6KGJfYFr8="},"NEXT":{"nodeId":"9191587b-f3b9-49f8-9bab-f627cb09c491","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"+nAinyVp9kAbJjdGOQ/f/kBxSFX/jwSsv8iekH2PKS0="}},"text":"Learning Data Augmentation Strategies for Object Detection           7\r\nUpon inspection, the most commonly used operation in good policies isRotate,\r\nwhich  rotates  the  whole  image  and  the  bounding  boxes. The  bounding  boxes\r\nend up larger after the rotation, to include all of the rotated objects. Despite\r\nthis  e\u000bect  of  theRotateoperation,  it  seems  to  be  very  bene\fcial:  it  is  the\r\nmost frequently used operation in good policies. Two other operations that are\r\ncommonly used areEqualizeandBBoxOnlyTranslateY.Equalize\rattens the\r\nhistogram of the pixel values, and does not modify the location or size of each\r\nbounding box.BBoxOnlyTranslateYtranslates only the objects in bounding\r\nboxes vertically, up or down with equal probability. This is quite encouraging as\r\nsome operations, such asBBoxOnlyTranslateY, uniquely act on the bounding\r\nboxes. Utilizing  a  combination  of  color,  geometric  and  bounding  box  speci\fc\r\noperations appears to be crucial to creating an optimal data augmentation policy\r\nfor object detection. 3.2    Data  augmentation  policy  found  by  AutoAugment\r\nsystematically improves object detection\r\nWe assess the quality of the data augmentation policy found by AutoAugment\r\non the competitive COCO dataset [24] on di\u000berent backbone architectures and\r\ndetection algorithms. We start with the competitive RetinaNet object detector4\r\nemploying the same training protocol as [9]. Brie\ry, we train from scratch with\r\na global batch size of 64, images are resized to 640\u0002640, learning rate of 0.08,\r\nweight  decay  of  1e\u00004,\u000b=  0:25  and\r=  1:5  for  the  focal  loss  parameters,\r\ntrain the models for 150 epochs, use stepwise decay with the learning rate being\r\nreduced by a factor  of 10 at  epochs  120 and  140. All models  were trained on\r\nTPUs  [18]. The baseline RetinaNet architecture used in this and subsequent sections em-\r\nploys standard data augmentation techniques typically used for object detection\r\ntraining [23].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"vXK1hVEiya73AnQsVQ4bO6VOFnprFuyOJMwalvNzA7s="},"9191587b-f3b9-49f8-9bab-f627cb09c491":{"id_":"9191587b-f3b9-49f8-9bab-f627cb09c491","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"8RnwWCCu1ACf+VxKy6e46PZCeR0qyoO+CP6KGJfYFr8="},"PREVIOUS":{"nodeId":"28af6010-36d5-4c6f-8686-00166266ae4d","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"vXK1hVEiya73AnQsVQ4bO6VOFnprFuyOJMwalvNzA7s="}},"text":"This consists of doing horizontal \ripping with 50% probability and\r\nmulti-scale  jittering  where  images  are  randomly  resized  between  512  and  786\r\nduring training and then cropped to 640x640. Our results using our augmentation policy found by AutoAugment using the\r\nabove procedures are shown in Tables 1 and 2. In Table 1, the data augmenta-\r\ntion policy achieves systematic gains across several backbone architectures with\r\nsurprising  improvements  ranging  from  +1.6  mAP  to  +2.3  mAP. In  compar-\r\nison,  a  previous  state-of-the-art  regularization  technique  (DropBlock)  applied\r\nto  ResNet-50  [9]  only  achieves  a  gain  of  +1.7  mAP. Additionally,  going  from\r\na ResNet-50 model to ResNet-101 achieves a 2.1 mAP gain and going from a\r\nResNet-101  to  ResNet-200  achieves  a  1.1  mAP  gain. Our  data  augmentation\r\npolicy  achieves  a  2.3  gain  on  ResNet-50,  which  is  a  larger  improvement  than\r\nsubstantially increasing the architecture size, while incurring no additional in-\r\nference cost. Clearly we see that changing augmentation can be as, if not more,\r\npowerful than changing around the underlying architectural components. 4https://github.com/tensorflow/tpu","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+nAinyVp9kAbJjdGOQ/f/kBxSFX/jwSsv8iekH2PKS0="},"379f3e7a-985e-42fc-a862-6a6386885b9a":{"id_":"379f3e7a-985e-42fc-a862-6a6386885b9a","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"XSg1AeQrxfv6zUM6pbw6JIb3uLo3LCQUoMOozieyklo="},"NEXT":{"nodeId":"44f7362b-5b79-4fc9-a7dc-e65d3666986c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"LdHFx3g5tlzim4R1iMZdTDZJb+gRVgQKCPmx+L8v0Dg="}},"text":"8           Zoph et al. Table 1. Improvements with AutoAugment data augmentation policy across\r\ndi\u000berent  ResNet  backbones.All  results  employ  RetinaNet  detector  [23]  on  the\r\nCOCO dataset [24]\r\nBackboneBaselineAutoAugmentDi\u000berence\r\nResNet-5036.739.0+2.3\r\nResNet-10138.840.4+1.6\r\nResNet-20039.942.1+2.2\r\nTo better understand where augmentation bene\fts, we break the data aug-\r\nmentation policies applied to ResNet-50 into three parts: color operations, ge-\r\nometric  operations,  and  bbox-only-operations  (Table  2). Employing  color  op-\r\nerations  only  boosts  performance  by  +0.8  mAP. Combining  the  search  with\r\ngeometric operations increases the boost in performance by +1.9 mAP. Finally,\r\nadding  bounding  box-speci\fc  operations  yields  the  best  results  when  used  in\r\nconjunction with the previous operations and provides +2.3 mAP improvement\r\nover the baseline. Table  2. Improvements  in  object  detection  with  the  data  augmentation\r\npolicy.All results employ RetinaNet detector with ResNet-50 backbone [23] on COCO\r\ndataset [24]. MethodmAP\r\nbaseline36.7\r\nbaseline + DropBlock [9]38.4\r\nAutoAugment with color operations37.5\r\n+ geometric operations38.6\r\n+ bbox-only operations39.0\r\nInterestingly, we observe that the custom operations designed for object de-\r\ntection (geometric operations and bbox-only operations) contributes 1.5 mAP\r\nof the 2.3 mAP gain from this data augmentation policy. Using object detection\r\nspeci\fc operations is clearly bene\fcial when trying to \fnd good augmentation\r\npolicies. This further con\frms our result that a diversity of augmentation oper-\r\nations spanning color, geometric and unique bounding box only operations are\r\nneeded to make a high performing augmentation policy. Also note that the policy\r\nfound was only searched using 5K COCO training examples and still generalizes\r\nwell when trained on the full COCO dataset.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"x/dWVzulA+kDpseQZMmuCucp7hJHGzoiMemsAwBnNt0="},"44f7362b-5b79-4fc9-a7dc-e65d3666986c":{"id_":"44f7362b-5b79-4fc9-a7dc-e65d3666986c","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"XSg1AeQrxfv6zUM6pbw6JIb3uLo3LCQUoMOozieyklo="},"PREVIOUS":{"nodeId":"379f3e7a-985e-42fc-a862-6a6386885b9a","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"x/dWVzulA+kDpseQZMmuCucp7hJHGzoiMemsAwBnNt0="}},"text":"3.3    Data augmentation policy found by AutoAugment push the\r\nstate-of-the-art on object detection models\r\nA good data augmentation policy is one that can transfer between models, be-\r\ntween datasets and work well for models trained on di\u000berent image sizes. Here\r\nwe experiment with the AutoAugment data augmentation policy on a di\u000berent","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"LdHFx3g5tlzim4R1iMZdTDZJb+gRVgQKCPmx+L8v0Dg="},"c97a59bb-570f-4f7e-bec8-95cd50c46f74":{"id_":"c97a59bb-570f-4f7e-bec8-95cd50c46f74","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Lof5yyWUIDMLRd81t4Cq/ETMFYmHGIB8wWYfCMfHnVc="},"NEXT":{"nodeId":"442ad558-9500-40ff-a72b-9d8b9a86ddb6","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"U5GGRgw4FtiBVb7siLtLr6leNjadERKZLMmW7CkgBQM="}},"text":"Learning Data Augmentation Strategies for Object Detection           9\r\nbackbone architecture and detection model. To test how the data augmentation\r\npolicy transfers to a state-of-the-art detection model, we replace the ResNet-50\r\nbackbone  with  the  AmoebaNet-D  architecture  [31]. The  feature-pyramid  net-\r\nwork  [22]  was  changed  to  NAS-FPN  [10]. Additionally,  we  use  ImageNet  pre-\r\ntraining for the AmoebaNet-D backbone as we found we are not able to achieve\r\ncompetitive results when training from scratch. The model was trained for 150\r\nepochs using a cosine learning rate decay with a learning rate of 0.08. The rest\r\nof the setup was identical to the ResNet-50 backbone model except the image\r\nsize was increased from 640\u0002640 to 1280\u00021280. Table  3  indicates  that  the  data  augmentation  policy  improves  +1.5  mAP\r\non  top  of  a  competitive  detection  architecture  and  setup. These  experiments\r\nshow  that  the  augmentation  policy  transfers  well  across  a  di\u000berent  backbone\r\narchitecture, feature pyramid network, image sizes (i.e. 640!1280 pixels), and\r\ntraining procedure (training from scratch!using ImageNet pre-training). This\r\nis a surprising result that shows our data augmentation policy is quite general. We can extend these results even further by increasing the image resolution from\r\n1280  to  1536  pixels  and  likewise  increasing  the  number  of  detection  anchors5\r\nfollowing [44]. Table 3. Exceeding state-of-the-art detection with the AutoAugment data\r\naugmentation policy.Reporting mAP for COCO validation set. Previous state-of-\r\nthe-art results for COCO detection evaluated a single image at multiple spatial scales\r\nto perform detection at test time [29]. Our current results only require a single inference\r\ncomputation at a single spatial scale. The backbone model is AmoebaNet-D [31] with\r\nNAS-FPN as the feature pyramid network [10].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"sD3HWo1Fr28EcQlUf3uinGBG7Vvx2pevyGypD0OfLSA="},"442ad558-9500-40ff-a72b-9d8b9a86ddb6":{"id_":"442ad558-9500-40ff-a72b-9d8b9a86ddb6","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Lof5yyWUIDMLRd81t4Cq/ETMFYmHGIB8wWYfCMfHnVc="},"PREVIOUS":{"nodeId":"c97a59bb-570f-4f7e-bec8-95cd50c46f74","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"sD3HWo1Fr28EcQlUf3uinGBG7Vvx2pevyGypD0OfLSA="}},"text":"For the50.7result, in addition to using\r\nthe data augmentation policy, we increase the image size from 1280 to 1536 and the\r\nnumber of detection anchors from 3x3 to 9x9. ArchitectureChange# ScalesmAPmAPSmAPMmAPL\r\nMegDet [29]multiple50.5-         -         -\r\nAmoebaNet + NAS-FPNbaseline [10]147.030.6    50.9    61.3\r\n+ AutoAugment policy148.632.0    53.4    62.7\r\n+\"anchors,\"image size150.734.2   55.5   64.5\r\nThis  result  of  these  simple  modi\fcations  is  the  \frst  single-stage  detection\r\nsystem to achieve state-of-the-art, single-model results of 50.7 mAP on COCO. We note that this result only requires a single pass of the image, where as the\r\nprevious  results  required  multiple  evaluations  of  the  same  image  at  di\u000berent\r\nspatial  scales  at  test  time  [29]. Additionally,  these  results  were  arrived  at  by\r\nincreasing  the  image  resolution  and  increasing  the  number  of  anchors  -  both\r\n5Speci\fcally, we increase the number of anchors from 3\u00023 to 9\u00029 by changing the\r\naspect ratios fromf1/2, 1, 2gtof1/5, 1/4, 1/3, 1/2, 1, 2, 3, 4, 5g. When making this\r\nchange we increased the strictness in the IoU thresholding from 0.5/0.5 to 0.6/0.5\r\ndue  to  the  increased  number  of  anchors  following  [44]. The  anchor  scale  was  also\r\nincreased from 4 to 5 to compensate for the larger image size.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"U5GGRgw4FtiBVb7siLtLr6leNjadERKZLMmW7CkgBQM="},"4d29baf6-a4ef-4ab9-9751-8d48024a74da":{"id_":"4d29baf6-a4ef-4ab9-9751-8d48024a74da","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"guoosDK54yNsCiw1evpbR8gmDF6FsaQFLlNIyuZUQBc="},"NEXT":{"nodeId":"58742e8f-fcef-4dcb-995a-c98f1bc2a4aa","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"pCXI2RfCdgVYKK3y3rru0TwDCsn2KoyYYzAD5Opj1uE="}},"text":"10         Zoph et al. simple and well known techniques for improving object detection performance\r\n[44, 16]. In contrast, previous state-of-the-art results relied on multiple, custom\r\nmodi\fcations of the model architecture and regularization methods in order to\r\nachieve these results [29]. Our method largely relies on a more modern network\r\narchitecture paired with a learned data augmentation policy. 3.4    Data augmentation policy found by AutoAugment transfers to\r\nother detection datasets\r\nTo evaluate the transferability of the data augmentation policy to an entirely dif-\r\nferent dataset and a di\u000berent detection algorithm, we train a Faster R-CNN [35]\r\nmodel with a ResNet-101 backbone on PASCAL VOC dataset [8]. We combine\r\nthe training sets of PASCAL VOC 2007 and PASCAL VOC 2012, and test our\r\nmodel on the PASCAL VOC 2007 test set (4952 images). Our evaluation met-\r\nric is the mean average precision at an IoU threshold of 0.5 (mAP50). For the\r\nbaseline model, we use the Tensor\row Object Detection API [16] with the de-\r\nfault  hyperparameters:  9  GPU  workers  are  utilized  for  asynchronous  training\r\nwhere each worker processes a batch size of 1. Initial learning rate is set to be\r\n3\u000210\u00004, which is decayed by 0.1 after 500K steps. Training is started from a\r\nCOCO detection model checkpoint. When training with our data augmentation\r\npolicy,  we  do  not  change  any  of  the  training  details,  and  just  add  our  policy\r\nfound  on  COCO  to  the  pre-processing. This  leads  to  a  2.7%  improvement  on\r\nmAP50 (Table 4). Table 4. Data augmentation policy transfers to other object detection tasks. Mean average precision (%) at IoU threshold 0.5 on a Faster R-CNN detector [35] with\r\na ResNet-101 backbone trained and evaluated on PASCAL VOC 2007 [8].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oZHEmo0gfZ/DTKWPbR3Y53bp1YF8zeYEuCSshLd2hvg="},"58742e8f-fcef-4dcb-995a-c98f1bc2a4aa":{"id_":"58742e8f-fcef-4dcb-995a-c98f1bc2a4aa","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"guoosDK54yNsCiw1evpbR8gmDF6FsaQFLlNIyuZUQBc="},"PREVIOUS":{"nodeId":"4d29baf6-a4ef-4ab9-9751-8d48024a74da","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"oZHEmo0gfZ/DTKWPbR3Y53bp1YF8zeYEuCSshLd2hvg="}},"text":"Note that\r\nthe augmentation policy was learned from the policy search on the COCO dataset\r\nplanebike bird boatbottlebus car cat chaircow tabledog horsembikepersonplantsheepsofa traintvmeanbaseline86.6 82.2 75.9 63.4 62.3  84.7 86.8 92.0 55.5 83.3 63.1 89.2 89.4 85.0  85.6  50.7 76.2 73.0 86.6 76.376.0\r\nours88.0 83.3 78.0 65.9 63.5  85.5 87.4 93.1 58.5 83.9 65.2 90.1 90.2 85.9  86.6  55.2 78.6 76.6 88.6 80.378.7\r\nThis result is surprising because the best policy found on COCO may appear\r\nto be too intricate to generalize to other datasets. But the result con\frms that\r\njust like architectures, data augmentation policies transfer well across datasets. This  means  that  the  augmentations  learned  on  the  COCO  dataset  are  very\r\ngeneric and can be used for many other object detection datasets in the future. 4  Analysis\r\nIn this section, we analyze the impact of training with the augmentation policy\r\nin more detail. We \fnd that:\r\n{Relative  improvement  of  AP  due  to  the  augmentation  policy  is  larger  for\r\nsmaller  datasets. This  is  good  news  since  data  augmentation  policies  are\r\nneeded mostly for models that have small amount of data available.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"pCXI2RfCdgVYKK3y3rru0TwDCsn2KoyYYzAD5Opj1uE="},"60b84c71-8535-4a16-935e-981208a403ad":{"id_":"60b84c71-8535-4a16-935e-981208a403ad","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"d3o6JUP8btLGnvHrOMQkJTmI/Ms6gLmo+Ik/X+HBzfM="},"NEXT":{"nodeId":"fd8fd06d-8fb5-478c-8a9e-add7ceb4b3e7","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"epCgFjbbFww9yj3Fy0IDxUVF+FSrYGbP43f+QSOuwCE="}},"text":"Learning Data Augmentation Strategies for Object Detection         11\r\n{Relative  improvement  is  larger  for  more  di\u000ecult  detection  tasks. Average\r\nprecision for small objects as well as average precision at more strict thresh-\r\nolds bene\ft more from the learned augmentation. {Data augmentation regularizes the detection model, which can be seen ei-\r\nther by the increased training loss or decreased magnitude of the trainable\r\nweights. {Having data augmentation operations that modify the locations and the sizes\r\nof the objects in the search space is important for achieving good results with\r\nthe augmentation policy. Below we describe these points in detail. 4.1    Data augmentation policy found by AutoAugment mimics the\r\nperformance of larger annotated datasets\r\nIn this section we conducted experiments to determine how the data augmenta-\r\ntion policy will perform if there is more or less training data. To conduct these\r\nexperiments we took subsets of the COCO dataset to make datasets with the\r\nfollowing number of images: 5000, 9000, 14000, 23000 (see Table 5). All models\r\ntrained in this experiment are using a ResNet-50 backbone with RetinaNet and\r\nare trained for 150 epochs without using ImageNet pretraining. Table 5. Data augmentation policy is especially bene\fcial for small datasets\r\nand small objects.Mean average precision (mAP) for RetinaNet model trained on\r\nCOCO with varying subsets of the original training set. mAPS, mAPMand mAPLdenote\r\nthe mean average precision for small, medium and large examples. Note the complete\r\nCOCO training set consists of 118K examples. The same policy found on the 5K COCO\r\nimages was used in all of the experiments. The models in the \frst row were trained on\r\nthe same 5K images that the policies were searched on.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"UpuB1T5PnqKcHNbBVpnJY7CL0aMozGftK5O7uMy/3Ic="},"fd8fd06d-8fb5-478c-8a9e-add7ceb4b3e7":{"id_":"fd8fd06d-8fb5-478c-8a9e-add7ceb4b3e7","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"d3o6JUP8btLGnvHrOMQkJTmI/Ms6gLmo+Ik/X+HBzfM="},"PREVIOUS":{"nodeId":"60b84c71-8535-4a16-935e-981208a403ad","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"UpuB1T5PnqKcHNbBVpnJY7CL0aMozGftK5O7uMy/3Ic="}},"text":"trainingBaselineOur results\r\nset sizemAPSmAPMmAPLmAPmAPSmAPMmAPLmAP\r\n50001.9      7.1      9.7     6.53.2      9.8    12.7     8.7\r\n90004.3    12.3    17.6   11.87.1    16.8    22.3   15.1\r\n140006.8    17.5    23.9   16.49.5    22.1    29.8   19.9\r\n2300010.0    24.3    33.3   22.611.9    27.8    36.8   25.3\r\nAs we expected, the improvements due to the data augmentation policy is\r\nlarger when the model is trained on smaller datasets, which can be seen in Fig. 2\r\nand in Table 5. We show that for models trained on 5000 training samples, the\r\ndata augmentation policy can improve mAP by more than 70% relative to the\r\nbaseline. As  the  training  set  size  is  increased,  the  e\u000bect  of  the  data  augmen-\r\ntation policy is decreased, although the improvements are still signi\fcant. It is\r\ninteresting to note that models trained with the data augmentation policy seem\r\nto do especially well on detecting smaller objects, especially when fewer images\r\nare present in the training dataset. For example, for small objects, applying the","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"epCgFjbbFww9yj3Fy0IDxUVF+FSrYGbP43f+QSOuwCE="},"2ead68e2-610d-4855-b7c1-c4581325a36f":{"id_":"2ead68e2-610d-4855-b7c1-c4581325a36f","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Y74Qe175cDRw7T9SA1HQqkvGDsABK528gG6C+uf0YXg="}},"text":"12         Zoph et al. data augmentation policy seems to be better than increasing the dataset size by\r\n50%, as seen in Table 5. This is quite a striking \fnding as in many detection\r\napplications detecting small objects is of great importance. For small objects,\r\ntraining with the data augmentation policy with 9000 examples results in better\r\nperformance than the baseline when using 15000 images. In this scenario using\r\nour augmentation policy is almost as e\u000bective as doubling your dataset size. Fig. 2.Percentage improvement in mAP for objects of di\u000berent sizes due to the data\r\naugmentation policy. Another interesting behavior of models trained with the data augmentation\r\npolicy  is  that  they  do  relatively  better  on  the  harder  task  of  AP75  (average\r\nprecision  IoU=0.75). In  Fig. 3,  we  plot  the  percentage  improvement  in  mAP,\r\nAP50, and AP75 for models trained with the data augmentation policy (rela-\r\ntive to baseline augmentation). The relative improvement of AP75 is larger than\r\nthat of AP50 for all training set sizes. The data augmentation policy is particu-\r\nlarly bene\fcial at AP75 indicating that the augmentation policy helps with more\r\nprecisely aligning the bounding box prediction. This suggests that the augmen-\r\ntation policy particularly helps with learned \fne spatial details in bounding box\r\nposition { which is consistent with the gains observed with small objects. Fig. 3.Percentage improvement due to the data augmentation policy on mAP, AP50,\r\nand AP75, relative to models trained with baseline augmentation.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"kNpq67dJ24fWnAUy8QwM2czvuPxceVnM+pKnE8jdH8E="},"8e76050f-70be-41cd-95b1-1db9c9ead619":{"id_":"8e76050f-70be-41cd-95b1-1db9c9ead619","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"en3l9x380f/h7ixMPNdgsen6NYwNoH8krPPIxvr2ZBc="},"NEXT":{"nodeId":"9e76a358-1723-423d-99c2-69f14bd4717e","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"3Lm/cxY2IzbCtiCQylpfVjUoPTpnjAU6DHl1jLJhgJE="}},"text":"Learning Data Augmentation Strategies for Object Detection         13\r\n4.2    Data augmentation improves model regularization\r\nFig. 4.Two plots showing data augmentation regularizes detection models. On the left\r\nis training loss vs. number of training examples for baseline model (black) and with\r\nthe  data  augmentation  policy  (red). One  the  right  is  L2norm  of  the  weights  of  the\r\nbaseline (black) and our (red) models at the end of training. Note that the L2norm of\r\nthe weights decrease with increasing training set size. The data augmentation policy\r\nfurther decreases the norm of the weights. In this section, we study the regularization e\u000bect of the data augmentation\r\npolicy. We \frst notice that the \fnal training loss of a detection model is lower\r\nwhen trained on a larger training set (see black curve in the left plot in Fig. 4). When  we  apply  the  data  augmentation  policy,  the  training  loss  is  increased\r\nsigni\fcantly for all dataset sizes (red curve). The regularization e\u000bect can also\r\nbe seen by looking at the L2norm of the weights of the trained models. The L2\r\nnorm of the weights is smaller for models trained on larger datasets, and models\r\ntrained with the data augmentation policy have a smaller L2norm than models\r\ntrained with baseline augmentation (see right plot in Fig. 4). 5  Related Work\r\nData augmentation strategies for vision models are often focused on the image\r\nclassi\fcation  domain  [21, 15, 41, 3, 6, 17, 28]. For  example,  state-of-the-art  clas-\r\nsi\fcation  models  trained  on  MNIST  use  elastic  distortions  which  e\u000bect  scale,\r\ntranslation, and rotation [38, 2, 42, 36]. Random cropping and image mirroring\r\nare commonly used in classi\fcation models trained on natural images [45, 19]. Among  the  limited  data  augmentation  strategies  for  object  detection,  image\r\nmirror  and  multi-scale  training  are  the  most  widely  used  [12]. Object-centric\r\ncropping is also a popular augmentation approach [26].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"fLqaiTXmwekbaBZs1pvPUIeidGXPnBkv2U7ogGrf80k="},"9e76a358-1723-423d-99c2-69f14bd4717e":{"id_":"9e76a358-1723-423d-99c2-69f14bd4717e","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"en3l9x380f/h7ixMPNdgsen6NYwNoH8krPPIxvr2ZBc="},"PREVIOUS":{"nodeId":"8e76050f-70be-41cd-95b1-1db9c9ead619","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"fLqaiTXmwekbaBZs1pvPUIeidGXPnBkv2U7ogGrf80k="}},"text":"Object-centric\r\ncropping is also a popular augmentation approach [26]. Instead of cropping to\r\nfocus on parts of the image, some methods randomly erase image contents for\r\naugmentation  [46, 9]. In  the  same  vein,    [43]  learns  an  occlusion  pattern  for\r\neach object to create adversarial examples. In addition to cropping and erasing,\r\n[7] adds new objects on training images by cut-and-paste. While these object-\r\ndetection  approaches  work  decently  well,  there  is  a  real  lack  of  studying  how","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"3Lm/cxY2IzbCtiCQylpfVjUoPTpnjAU6DHl1jLJhgJE="},"9678be46-f6d6-41e7-8a01-403604cc24a6":{"id_":"9678be46-f6d6-41e7-8a01-403604cc24a6","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"uUfTDSEwxJTwJKZSQjWfloymHrPXYFWfYOeyQ81x65Q="},"NEXT":{"nodeId":"2888a452-207a-4b11-ae83-c7e0ab0575d4","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"a5wy6Ju0uQUQumnEoPX41WgduqHLXHoVFTWnY5hsyPM="}},"text":"14         Zoph et al. truly transferable they are, doing compositions of many di\u000berent augmentation\r\nmethods at once and not typically performing as well as modeling changes [10]. To avoid the overwhelming amount of options when designing a data augmen-\r\ntation policy, recent work has focused on learning data augmentation strategies\r\ndirectly from data itself. For example, Smart Augmentation uses a network that\r\ngenerates new data by merging two or more samples from the same class [20]. Tran et al. generate augmented data, using a Bayesian approach, based on the\r\ndistribution  learned  from  the  training  set  [40]. DeVries  and  Taylor  used  sim-\r\nple transformations like noise, interpolations and extrapolations in the learned\r\nfeature space to augment data [5]. Ratner et al., used generative adversarial net-\r\nworks to generate sequences of data augmentation operations [30]. More recently,\r\nseveral papers use the AutoAugment [3] search space with improved optimization\r\nalgorithms to \fnd AutoAugment policies more e\u000eciently [15, 21]. The above learned augmentation approaches were found to be quite e\u000bective\r\nin the classi\fcation domain due to the complexity of designing a good augmen-\r\ntation  procedure. When  designing  augmentation  policies  for  object  detection\r\nthe complexity only increases. Unlike classi\fcation, labeled data for object de-\r\ntection  is  more  scarce  because  it  is  more  costly  to  annotate  detection  data. Compared to image classi\fcation, developing a data augmentation strategy for\r\nobject  detection  is  harder  because  there  are  more  complexities  introduced  by\r\ndistorting  the  image,  bounding  box  locations,  and  the  sizes  of  the  objects  in\r\ndetection datasets. Furthermore, it is much less clear that augmentation policies\r\nare transferable due to images having a richer label structure and the models\r\nand detection algorithms being more complex. Our goal is to show that these\r\nadded complexities are handle-able using learned augmentation procedures and\r\nthat high performing data augmentation policies can be found. We surprisingly\r\n\fnd that these policies are highly generalizable across di\u000berence datasets, models\r\nand detection algorithms.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"1mjY5VcbKi3VqDC9s9eD9uxW86CIoymvMiSOY/ztyp8="},"2888a452-207a-4b11-ae83-c7e0ab0575d4":{"id_":"2888a452-207a-4b11-ae83-c7e0ab0575d4","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"uUfTDSEwxJTwJKZSQjWfloymHrPXYFWfYOeyQ81x65Q="},"PREVIOUS":{"nodeId":"9678be46-f6d6-41e7-8a01-403604cc24a6","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"1mjY5VcbKi3VqDC9s9eD9uxW86CIoymvMiSOY/ztyp8="}},"text":"6  Discussion\r\nIn  this  work,  we  challenge  the  common  belief  that  focusing  on  changing  the\r\ndetection  model  is  the  most  promising  research  direction. Our  augmentation\r\nprocedure gets larger improvements than increasing the model size, while incur-\r\nring no additional inference cost and minimal training cost. And although data\r\naugmentation strategies can be intricate, they can be as transferable as archi-\r\ntectures. Our augmentation policy learned on COCO transfers to PASCAL with\r\ngreat performance. Additionally, we are able to further improve the state-of-the-\r\nart on COCO using our learned augmentation policy found on a small 5K subset\r\nof the COCO dataset with much smaller model, a di\u000berent image resolution and\r\ndetection algorithm. AcknowledgementsWe  thank  the  larger  teams  at  Google  Brain  for  their\r\nhelp and support. We also thank Dumitru Erhan for detailed comments on the\r\nmanuscript.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"a5wy6Ju0uQUQumnEoPX41WgduqHLXHoVFTWnY5hsyPM="},"be8b572e-ec9d-45dc-b9dc-421badb8282d":{"id_":"be8b572e-ec9d-45dc-b9dc-421badb8282d","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"EICoS6CnXHoqRPfzlwnBmIBczRnYsWmQhtCtkeROzWM="},"NEXT":{"nodeId":"daf63ded-011c-49f9-8615-b72a406a998a","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"X9kZOdjAKADFw1r/LJVFQWtAzvmEEDS6IKIMpbLUuHg="}},"text":"Learning Data Augmentation Strategies for Object Detection         15\r\nReferences\r\n1. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe-\r\nmawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore,\r\nS., Murray, D.G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M.,\r\nYu, Y., Zheng, X.: Tensor\row: A system for large-scale machine learning. In: Pro-\r\nceedings of the 12th USENIX Conference on Operating Systems Design and Im-\r\nplementation. pp. 265{283. OSDI'16,  USENIX  Association,  Berkeley,  CA,  USA\r\n(2016)\r\n2. Ciregan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for\r\nimage classi\fcation. In: Proceedings of IEEE Conference on Computer Vision and\r\nPattern Recognition. pp. 3642{3649. IEEE (2012)\r\n3. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning\r\naugmentation policies from data. arXiv preprint arXiv:1805.09501 (2018)\r\n4. Cubuk, E.D., Zoph, B., Schoenholz, S.S., Le, Q.V.: Intriguing properties of adver-\r\nsarial examples. arXiv preprint arXiv:1711.02846 (2017)\r\n5. DeVries, T., Taylor, G.W.: Dataset augmentation in feature space. arXiv preprint\r\narXiv:1702.05538 (2017)\r\n6. DeVries,  T.,  Taylor,  G.W.:  Improved  regularization  of  convolutional  neural  net-\r\nworks with cutout. arXiv preprint arXiv:1708.04552 (2017)\r\n7.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"bq7cDhWfi9gXZygNMHgpUdPhYKTtBf6kFvfchDOx42s="},"daf63ded-011c-49f9-8615-b72a406a998a":{"id_":"daf63ded-011c-49f9-8615-b72a406a998a","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"EICoS6CnXHoqRPfzlwnBmIBczRnYsWmQhtCtkeROzWM="},"PREVIOUS":{"nodeId":"be8b572e-ec9d-45dc-b9dc-421badb8282d","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"bq7cDhWfi9gXZygNMHgpUdPhYKTtBf6kFvfchDOx42s="},"NEXT":{"nodeId":"8858a816-f34a-46a0-941b-4732c093d97a","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"e1ms0/XOsUaWr+/lcrrD7bnBpsSkgcsKpEJ+0ze2i/k="}},"text":"arXiv preprint arXiv:1708.04552 (2017)\r\n7. Dwibedi, D., Misra, I., Hebert, M.: Cut, paste and learn: Surprisingly easy synthesis\r\nfor instance detection. In: Proceedings of the IEEE International Conference on\r\nComputer Vision. pp. 1301{1310 (2017)\r\n8. Everingham,  M.,  Van  Gool,  L.,  Williams,  C.K.,  Winn,  J.,  Zisserman,  A.:  The\r\npascal visual object classes (voc) challenge. International journal of computer vision\r\n88(2), 303{338 (2010)\r\n9. Ghiasi, G., Lin, T.Y., Le, Q.V.: DropBlock: A regularization method for convo-\r\nlutional  networks. In:  Advances  in  Neural  Information  Processing  Systems. pp. 10750{10760 (2018)\r\n10. Ghiasi,  G.,  Lin,  T.Y.,  Pang,  R.,  Le,  Q.V.:  NAS-FPN:  Learning  scalable  feature\r\npyramid architecture for object detection. In: The IEEE Conference on Computer\r\nVision and Pattern Recognition (CVPR) (June 2019)\r\n11. Girshick,  R.:  Fast  r-cnn. In:  The  IEEE  International  Conference  on  Computer\r\nVision (ICCV) (December 2015)\r\n12. Girshick, R., Radosavovic, I., Gkioxari, G., Doll\u0013ar, P., He, K.: Detectron (2018)\r\n13. He, K., Gkioxari, G., Doll\u0013ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the\r\nIEEE international conference on computer vision. pp. 2961{2969 (2017)\r\n14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\r\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\r\n(CVPR). pp. 770{778 (2016)\r\n15.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"X9kZOdjAKADFw1r/LJVFQWtAzvmEEDS6IKIMpbLUuHg="},"8858a816-f34a-46a0-941b-4732c093d97a":{"id_":"8858a816-f34a-46a0-941b-4732c093d97a","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"EICoS6CnXHoqRPfzlwnBmIBczRnYsWmQhtCtkeROzWM="},"PREVIOUS":{"nodeId":"daf63ded-011c-49f9-8615-b72a406a998a","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"X9kZOdjAKADFw1r/LJVFQWtAzvmEEDS6IKIMpbLUuHg="}},"text":"pp. 770{778 (2016)\r\n15. Ho,  D.,  Liang,  E.,  Stoica,  I.,  Abbeel,  P.,  Chen,  X.:  Population  based  aug-\r\nmentation:  E\u000ecient  learning  of  augmentation  policy  schedules. arXiv  preprint\r\narXiv:1905.05393 (2019)\r\n16. Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I.,\r\nWojna, Z., Song, Y., Guadarrama, S., et al.: Speed/accuracy trade-o\u000bs for modern\r\nconvolutional object detectors. In: Proceedings of the IEEE conference on computer\r\nvision and pattern recognition. pp. 7310{7311 (2017)\r\n17. Inoue, H.: Data augmentation by pairing samples for images classi\fcation. arXiv\r\npreprint arXiv:1801.02929 (2018)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"e1ms0/XOsUaWr+/lcrrD7bnBpsSkgcsKpEJ+0ze2i/k="},"76db2229-e2a7-4ea4-8f61-d19ae703d0ae":{"id_":"76db2229-e2a7-4ea4-8f61-d19ae703d0ae","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"2pbbdvW9bgHfSgb2wdSDC7KfyDdIu5mjbpbFO/9IjAU="},"NEXT":{"nodeId":"81d52562-ed86-4e77-b597-8b75f2ca2c50","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Ombwygba6dI8wdK6w8jvfQgdUh6BCSXcb0cNAOJwDzA="}},"text":"16         Zoph et al. 18. Jouppi, N.P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S.,\r\nBhatia, S., Boden, N., Borchers, A., et al.: In-datacenter performance analysis of a\r\ntensor processing unit. In: 2017 ACM/IEEE 44th Annual International Symposium\r\non Computer Architecture (ISCA). pp. 1{12. IEEE (2017)\r\n19. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\fcation with deep con-\r\nvolutional neural networks. In: Advances in Neural Information Processing Systems\r\n(2012)\r\n20. Lemley, J., Bazrafkan, S., Corcoran, P.: Smart augmentation learning an optimal\r\ndata augmentation strategy. IEEE Access5, 5858{5869 (2017)\r\n21. Lim,  S.,  Kim,  I.,  Kim,  T.,  Kim,  C.,  Kim,  S.:  Fast  autoaugment. arXiv  preprint\r\narXiv:1905.00397 (2019)\r\n22. Lin,  T.Y.,  Doll\u0013ar,  P.,  Girshick,  R.,  He,  K.,  Hariharan,  B.,  Belongie,  S.:  Feature\r\npyramid networks for object detection. In: Proceedings of the IEEE conference on\r\ncomputer vision and pattern recognition. pp. 2117{2125 (2017)\r\n23. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u0013ar, P.: Focal loss for dense object\r\ndetection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980{2988 (2017)\r\n24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u0013ar, P.,\r\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\r\non computer vision. pp. 740{755. Springer (2014)\r\n25.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"iHezP4Sy14gsdOe5gxSEH32Tl2752DhzOfB5gBOJX0c="},"81d52562-ed86-4e77-b597-8b75f2ca2c50":{"id_":"81d52562-ed86-4e77-b597-8b75f2ca2c50","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"2pbbdvW9bgHfSgb2wdSDC7KfyDdIu5mjbpbFO/9IjAU="},"PREVIOUS":{"nodeId":"76db2229-e2a7-4ea4-8f61-d19ae703d0ae","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"iHezP4Sy14gsdOe5gxSEH32Tl2752DhzOfB5gBOJX0c="},"NEXT":{"nodeId":"818644c5-16df-4c23-b707-2ff3e5020317","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"x87Ezjez6KGmainCcfzSlpg6lLWu6Y3ltUBbVUpyFsg="}},"text":"pp. 740{755. Springer (2014)\r\n25. Liu,   C.,   Zoph,   B.,   Shlens,   J.,   Hua,   W.,   Li,   L.J.,   Fei-Fei,   L.,   Yuille,   A.,\r\nHuang,  J.,  Murphy,  K.:  Progressive  neural  architecture  search. arXiv  preprint\r\narXiv:1712.00559 (2017)\r\n26. Liu,  W.,  Anguelov,  D.,  Erhan,  D.,  Szegedy,  C.,  Reed,  S.,  Fu,  C.Y.,  Berg,  A.C.:\r\nSsd: Single shot multibox detector. In: European conference on computer vision. pp. 21{37. Springer (2016)\r\n27. Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983 (2016)\r\n28. Miyato, T., Maeda, S.i., Koyama, M., Ishii, S.: Virtual adversarial training: a reg-\r\nularization method for supervised and semi-supervised learning. In: International\r\nConference on Learning Representations (2016)\r\n29. Peng, C., Xiao, T., Li, Z., Jiang, Y., Zhang, X., Jia, K., Yu, G., Sun, J.: Megdet:\r\nA large mini-batch object detector. In: The IEEE Conference on Computer Vision\r\nand Pattern Recognition (CVPR) (June 2018)\r\n30. Ratner, A.J., Ehrenberg, H., Hussain, Z., Dunnmon, J., R\u0013e, C.: Learning to com-\r\npose domain-speci\fc transformations for data augmentation. In: Advances in Neu-\r\nral Information Processing Systems. pp. 3239{3249 (2017)\r\n31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-\r\nsi\fer architecture search. In: Thirty-Third AAAI Conference on Arti\fcial Intelli-\r\ngence (2019)\r\n32.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Ombwygba6dI8wdK6w8jvfQgdUh6BCSXcb0cNAOJwDzA="},"818644c5-16df-4c23-b707-2ff3e5020317":{"id_":"818644c5-16df-4c23-b707-2ff3e5020317","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"2pbbdvW9bgHfSgb2wdSDC7KfyDdIu5mjbpbFO/9IjAU="},"PREVIOUS":{"nodeId":"81d52562-ed86-4e77-b597-8b75f2ca2c50","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"Ombwygba6dI8wdK6w8jvfQgdUh6BCSXcb0cNAOJwDzA="}},"text":"Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Uni\fed,\r\nreal-time  object  detection. In:  The  IEEE  Conference  on  Computer  Vision  and\r\nPattern Recognition (CVPR) (June 2016)\r\n33. Redmon, J., Farhadi, A.: Yolo9000: Better, faster, stronger. In: The IEEE Confer-\r\nence on Computer Vision and Pattern Recognition (CVPR) (July 2017)\r\n34. Redmon,   J.,   Farhadi,   A.:   Yolov3:   An   incremental   improvement. CoRR\r\nabs/1804.02767(2018),http://arxiv.org/abs/1804.02767\r\n35. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-\r\ntion with region proposal networks. In: Advances in neural information processing\r\nsystems. pp. 91{99 (2015)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"x87Ezjez6KGmainCcfzSlpg6lLWu6Y3ltUBbVUpyFsg="},"274ee22d-1367-45c5-bb19-68d35367d182":{"id_":"274ee22d-1367-45c5-bb19-68d35367d182","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"ucj6IDgenkOjlZqPUmHtMCZBC1BdaV54RvMxkWTCN8I="},"NEXT":{"nodeId":"ef105de1-b6b1-4b40-a5a6-1d2c54d1138c","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"kEES7FB7I26XEugtJ8DrKjFJBpxgtNw37VT73iZjeY8="}},"text":"Learning Data Augmentation Strategies for Object Detection         17\r\n36. Sato,  I.,  Nishimura,  H.,  Yokoi,  K.:  Apac:  Augmented  pattern  classi\fcation  with\r\nneural networks. arXiv preprint arXiv:1505.03229 (2015)\r\n37. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\r\noptimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\r\n38. Simard, P.Y., Steinkraus, D., Platt, J.C., et al.: Best practices for convolutional\r\nneural networks applied to visual document analysis. In: Proceedings of Interna-\r\ntional Conference on Document Analysis and Recognition (2003)\r\n39. Tan, M., Pang, R., Le, Q.V.: E\u000ecientdet: Scalable and e\u000ecient object detection. arXiv preprint arXiv:1911.09070 (2019)\r\n40. Tran, T., Pham, T., Carneiro, G., Palmer, L., Reid, I.: A bayesian data augmen-\r\ntation  approach  for  learning  deep  models. In:  Advances  in  Neural  Information\r\nProcessing Systems. pp. 2794{2803 (2017)\r\n41. Verma, V., Lamb, A., Beckham, C., Courville, A., Mitliagkis, I., Bengio, Y.: Man-\r\nifold  mixup:  Encouraging  meaningful  on-manifold  interpolation  as  a  regularizer. arXiv preprint arXiv:1806.05236 (2018)\r\n42. Wan, L., Zeiler, M., Zhang, S., Le Cun, Y., Fergus, R.: Regularization of neural\r\nnetworks  using  dropconnect. In:  International  Conference  on  Machine  Learning. pp. 1058{1066 (2013)\r\n43. Wang, X., Shrivastava, A., Gupta, A.: A-fast-rcnn: Hard positive generation via ad-\r\nversary for object detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4O389wGCdFIuj62jV7Vcy2Lf2NypPWmJzTCIB7tVSVU="},"ef105de1-b6b1-4b40-a5a6-1d2c54d1138c":{"id_":"ef105de1-b6b1-4b40-a5a6-1d2c54d1138c","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"ucj6IDgenkOjlZqPUmHtMCZBC1BdaV54RvMxkWTCN8I="},"PREVIOUS":{"nodeId":"274ee22d-1367-45c5-bb19-68d35367d182","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"4O389wGCdFIuj62jV7Vcy2Lf2NypPWmJzTCIB7tVSVU="}},"text":"In: Proceedings of the IEEE Conference on Computer\r\nVision and Pattern Recognition. pp. 2606{2615 (2017)\r\n44. Yang, T., Zhang, X., Li, Z., Zhang, W., Sun, J.: Metaanchor: Learning to detect\r\nobjects with customized anchors. In: Advances in Neural Information Processing\r\nSystems. pp. 318{328 (2018)\r\n45. Zagoruyko, S., Komodakis, N.: Wide residual networks. In: British Machine Vision\r\nConference (2016)\r\n46. Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmenta-\r\ntion. arXiv preprint arXiv:1708.04896 (2017)\r\n47. Zoph,  B.,  Le,  Q.V.:  Neural  architecture  search  with  reinforcement  learning. In:\r\nInternational Conference on Learning Representations (2017)\r\n48. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures\r\nfor scalable image recognition. In: Proceedings of IEEE Conference on Computer\r\nVision and Pattern Recognition (2017)","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"kEES7FB7I26XEugtJ8DrKjFJBpxgtNw37VT73iZjeY8="},"569a3dfd-474b-4546-a11f-e329d10f73a3":{"id_":"569a3dfd-474b-4546-a11f-e329d10f73a3","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"pzeEiJiQvFDdZM3cse3RNmpQh6xwu+fm4GhTvbk3AyY="},"NEXT":{"nodeId":"9071c79e-da7a-4398-a85e-cf405b86e146","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"mxy0+4BhAWpWw7i5Hk5zSgyqy0B0FMZSke0CM6DgJ4M="}},"text":"18         Zoph et al. A  Appendix\r\nA.1    AutoAugment Controller Training Details\r\nTable 6.Table of all the possible transformations that can be applied to an image. These  are  the  transformations  that  are  available  to  the  controller  during  the  search\r\nprocess. The range of magnitudes that the controller can predict for each of the trans-\r\nforms  is  listed  in  the  third  column. Some  transformations  do  not  have  a  magnitude\r\nassociated with them (e.g. Equalize). Operation Name                                    Description                                      Range of\r\nmagnitudes\r\nShearX(Y)      Shear the image and the corners of the bounding boxes\r\nalong the horizontal (vertical) axis with ratemagnitude. [-0.3,0.3]\r\nTranslateX(Y)   Translate the image and the bounding boxes in the hori-\r\nzontal (vertical) direction bymagnitudenumber of pixels. [-150,150]\r\nRotate         Rotate the image and the bounding boxesmagnitudede-\r\ngrees. [-30,30]\r\nEqualize        Equalize the image histogram. Solarize        Invert all pixels above a threshold value ofmagnitude. [0,256]\r\nSolarizeAdd     For each pixel in the image that is less than 128, add an\r\nadditional amount to it decided by the magnitude. [0,110]\r\nContrast       Control the contrast of the image. Amagnitude=0 gives\r\na  gray  image,  whereasmagnitude=1  gives  the  original\r\nimage. [0.1,1.9]\r\nColor          Adjust the color balance of the image, in a manner similar\r\nto the controls on a colour TV set. Amagnitude=0 gives\r\na black & white image, whereasmagnitude=1 gives the\r\noriginal image. [0.1,1.9]\r\nBrightness      Adjust the brightness of the image. Amagnitude=0 gives\r\na black image, whereasmagnitude=1 gives the original\r\nimage. [0.1,1.9]\r\nSharpness      Adjust the sharpness of the image. Amagnitude=0 gives\r\na blurred image, whereasmagnitude=1 gives the original\r\nimage.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"aCiLUi82I8AUnB29PZ9IuOIlYNLU5QVhkOzJHkPpfgA="},"9071c79e-da7a-4398-a85e-cf405b86e146":{"id_":"9071c79e-da7a-4398-a85e-cf405b86e146","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"pzeEiJiQvFDdZM3cse3RNmpQh6xwu+fm4GhTvbk3AyY="},"PREVIOUS":{"nodeId":"569a3dfd-474b-4546-a11f-e329d10f73a3","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"aCiLUi82I8AUnB29PZ9IuOIlYNLU5QVhkOzJHkPpfgA="}},"text":"[0.1,1.9]\r\nCutout [6, 46]   Set a random square patch of side-lengthmagnitudepix-\r\nels to gray. [0,60]\r\nBBoxOnlyX   Apply  X  to  each  bounding  box  content  with  indepen-\r\ndent probability, and magnitude that was chosen for X\r\nabove. Location and the size of the bounding box are not\r\nchanged.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"mxy0+4BhAWpWw7i5Hk5zSgyqy0B0FMZSke0CM6DgJ4M="},"7f14d27e-35be-4cfc-affd-ecc078d6db4c":{"id_":"7f14d27e-35be-4cfc-affd-ecc078d6db4c","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf","file_name":"Learning_Data_Augmentation_Strategies_for_Object_Detection.pdf"},"hash":"qmBV/LfZgDZjERTzG6O7XfKV2ZYVak3GQJduB9ye6Oc="}},"text":"Learning Data Augmentation Strategies for Object Detection         19\r\nTable 7.The sub-policies used in our learned augmentation policy. P and M correspond\r\nto the probability and magnitude with which the operations were applied in the sub-\r\npolicy. Note that for each image in each mini-batch, one of the sub-policies is picked\r\nuniformly  at  random. TheNo  operationis  listed  when  an  operation  has  a  learned\r\nprobability or magnitude of 0\r\nOperation 1                   P   M Operation 2                   P   M\r\nSub-policy 1 TranslateX                    0.6 4   Equalize                        0.8 10\r\nSub-policy 2 BBoxOnlyTranslateY 0.2 2   Cutout                          0.8 8\r\nSub-policy 3 ShearY                          1.0 2   BBoxOnlyTranslateY 0.6 6\r\nSub-policy 4 Rotate                           0.6 10 Color                             1.0 6\r\nSub-policy 5 No operation                           No operation","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"0UJ7B+mJkGhBXIp+o+kM6yOgBh1INGYV/EpohJhity8="},"eda5ba7e-c509-4d41-96bf-28e899110c9b":{"id_":"eda5ba7e-c509-4d41-96bf-28e899110c9b","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"i9i4CLlAkBhHtSgzHpD3ZXOJBXupQxHdjL5vUDfXBcg="},"NEXT":{"nodeId":"e9585fc0-54f5-4293-be05-eea543b480e0","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+OUKucU72LT/lF2HLEx0TBMGpQFsphO7imQMKoVIYak="}},"text":"Object Detection in 20 Years:\r\nA Survey\r\nThis survey seeks to provide the novice reader with a complete grasp of object detection\r\ntechnology from many viewpoints, with an emphasis on its evolution. ByZHENGXIAZOU, KEYANCHEN, ZHENWEISHI,Member IEEE,\r\nYUHONGGUO,ANDJIEPINGYE,Fellow IEEE\r\nABSTRACT|Object detection, as of one the most fundamental\r\nand  challenging  problems  in  computer  vision,  has  received\r\ngreat attention in recent years. Over the past two decades, we\r\nhave seen a rapid technological evolution of object detection\r\nand its profound impact on the entire computer vision field. If\r\nwe consider today’s object detection technique as a revolution\r\ndriven by deep learning, then, back in the 1990s, we would\r\nsee the ingenious thinking and long-term perspective design of\r\nearly computer vision. This article extensively reviews this fast-\r\nmoving research field in the light of technical evolution, span-\r\nning over a quarter-century’s time (from the 1990s to 2022). A number of topics have been covered in this article, including\r\nthe milestone detectors in history, detection datasets, metrics,\r\nfundamental building blocks of the detection system, speedup\r\ntechniques, and recent state-of-the-art detection methods. KEYWORDS|Computer vision; convolutional neural networks\r\n(CNNs); deep learning; object detection; technical evolution. Manuscript received 31 October 2022; revised 5 January 2023; accepted17 January 2023. Date of publication 27 January 2023; date of current version\r\n7 March 2023.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"bCvNfecMzklAaAS4A8ayQ0Te1BF110pS+k9v71ZMrLc="},"e9585fc0-54f5-4293-be05-eea543b480e0":{"id_":"e9585fc0-54f5-4293-be05-eea543b480e0","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"i9i4CLlAkBhHtSgzHpD3ZXOJBXupQxHdjL5vUDfXBcg="},"PREVIOUS":{"nodeId":"eda5ba7e-c509-4d41-96bf-28e899110c9b","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"bCvNfecMzklAaAS4A8ayQ0Te1BF110pS+k9v71ZMrLc="},"NEXT":{"nodeId":"8b31b7f1-55fc-47e2-af7c-9558e28bd942","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"KK/XkUr97I7MiFeXUuV8sx330HKlEMRPwcJ1mjY7Ahw="}},"text":"This work was supported in part by the National Natural ScienceFoundation of China under Grant 62125102, in part by the National Key\r\nResearch and Development Program of China (titled: Brain-Inspired GeneralVision Models and Applications), and in part by the Fundamental Research Funds\r\nfor the Central Universities.(Corresponding authors: Zhengxia Zou; Jieping Ye.)Zhengxia Zouis with the Department of Guidance, Navigation and Control,\r\nSchool of Astronautics, Beihang University, Beijing 100191, China, and also withthe Shanghai Artificial Intelligence Laboratory, Shanghai 200232, China (e-mail:\r\nzhengxiazou@buaa.edu.cn).Keyan ChenandZhenwei Shiare with the Image Processing Center, School\r\nof Astronautics, the Beijing Key Laboratory of Digital Media, and the State KeyLaboratory of Virtual Reality Technology and Systems, Beihang University,\r\nBeijing 100191, China, and also with the Shanghai Artificial IntelligenceLaboratory, Shanghai 200232, China. Yuhong Guois with the School of Computer Science, Carleton University,Ottawa, ON K1S 5B6, Canada. Jieping Yeis with the Alibaba Group, Hangzhou 310030, China (e-mail:jieping@gmail.com). Digital Object Identifier 10.1109/JPROC.2023.3238524\r\nI. I N T R O D U C T I O N\r\nObject detection is an important computer vision task that\r\ndeals with detecting instances of visual objects of a certain\r\nclass (such as humans, animals, or cars) in digital images. The goal of object detection is to develop computational\r\nmodels and techniques that provide one of the most basic\r\npieces of knowledge needed by computer vision applica-\r\ntions:What  objects  are  where?The  two  most  significant\r\nmetrics for object detection are accuracy (including clas-\r\nsification accuracy and localization accuracy) and speed. Object  detection  serves  as  a  basis  for  many  other\r\ncomputer  vision  tasks,  such  as  instance  segmentation\r\n[1],  [2],  [3],  [4],  image  captioning  [5],  [6],  [7],  and\r\nobject tracking [8].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+OUKucU72LT/lF2HLEx0TBMGpQFsphO7imQMKoVIYak="},"8b31b7f1-55fc-47e2-af7c-9558e28bd942":{"id_":"8b31b7f1-55fc-47e2-af7c-9558e28bd942","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_1","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"i9i4CLlAkBhHtSgzHpD3ZXOJBXupQxHdjL5vUDfXBcg="},"PREVIOUS":{"nodeId":"e9585fc0-54f5-4293-be05-eea543b480e0","metadata":{"page_number":1,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+OUKucU72LT/lF2HLEx0TBMGpQFsphO7imQMKoVIYak="}},"text":"In recent years, the rapid development\r\nof  deep  learning  techniques  [9]  has  greatly  promoted\r\nthe  progress  of  object  detection,  leading  to  remarkable\r\nbreakthroughs  and  propelling  it  to  a  research  hot-spot\r\nwith  unprecedented  attention. Object  detection  has  now\r\nbeen widely used in many real-world applications, such as\r\nautonomous driving, robot vision, and video surveillance. Fig. 1  shows  the  growing  number  of  publications  that\r\nare  associated  with  “object  detection”  over  the  past  two\r\ndecades. As different detection tasks have totally different objec-\r\ntives and constraints, their difficulties may vary from each\r\nother. In  addition  to  some  common  challenges  in  other\r\ncomputer  vision  tasks,  such  as  objects  under  different\r\nviewpoints,  illuminations,  and  intraclass  variations,  the\r\nchallenges in object detection include, but are not limited\r\nto, the following aspects: object rotation and scale changes\r\n(e.g.,  small  objects),  accurate  object  localization,  dense\r\nand occluded object detection, speedup of detection, and\r\nso on. In Section IV, we will give a more detailed analysis\r\nof these topics. This  survey  seeks  to  provide  novices  with  a  com-\r\nplete  grasp  of  object  detection  technology  from  many\r\nviewpoints,  with  an  emphasis  on  its  evolution. The  key\r\n0018-9219 © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See https://www.ieee.org/publications/rights/index.html for more information. Vol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE257\r\nAuthorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KK/XkUr97I7MiFeXUuV8sx330HKlEMRPwcJ1mjY7Ahw="},"f0ab8d2e-cb96-492f-a897-ad96acae65c5":{"id_":"f0ab8d2e-cb96-492f-a897-ad96acae65c5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4UlS3RvTaof5GUQQva4SEBi6qiXs3RhvtpFTdaSjRic="},"NEXT":{"nodeId":"315e0beb-3f98-4b49-9841-09d04c056f3f","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Rw0yXNWQZUT2IoIePibwC1SB7Xsnt7ywcugpbRh0WOw="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 1.Increasing number of publications in object detection from\r\n1998 to 2021. (Data from Google scholar advanced search: allintitle:\r\n“object detection” or “detecting objects. ”)\r\nfeatures are threefolds: a comprehensive review in the light\r\nof technical evolutions, an in-depth exploration of the key\r\ntechnologies and the recent state of the arts, and a com-\r\nprehensive analysis of detection speedup techniques. The\r\nmain clue focuses on the past, present, and future, comple-\r\nmented with some other necessary components in object\r\ndetection, such as datasets, metrics, and acceleration tech-\r\nniques. Standing  on  the  technical  highway,  this  survey\r\naims  to  present  the  evolution  of  related  technologies,\r\nallowing readers to grasp the essential concepts and find\r\npotential future directions, while neglecting their technical\r\nspecifics. The  rest  of  this  article  is  organized  as  follows. In\r\nSection II, we review the 20 years of evolution of object\r\ndetection. In Section III, we review the speedup techniques\r\nin object detection. The state-of-the-art detection methods\r\nof  the  recent  three  years  are  reviewed  in  Section  IV. In\r\nSection  V,  we  conclude  this  article  and  make  a  deep\r\nanalysis of the further research directions. II. O B J E C T  D E T E C T I O N  I N  2 0  Y E A R S\r\nIn  this  section,  we  will  review  the  history  of  object\r\ndetection   from   multiple   views,   including   milestone\r\ndetectors,  datasets,  metrics,  and  the  evolution  of  key\r\ntechniques. A. Road Map of Object Detection\r\nIn the past two decades, it is widely accepted that the\r\nprogress  of  object  detection  has  generally  gone  through\r\ntwo  historical  periods:  the  “traditional  object  detection\r\nperiod (before 2014)” and the “deep learning-based detec-\r\ntion  period  (after  2014),”  as  shown  in  Fig. 2.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+K7CpGLH9H4FDl9t+2TsdpLWyKQMQoocIGFt4mZU7PE="},"315e0beb-3f98-4b49-9841-09d04c056f3f":{"id_":"315e0beb-3f98-4b49-9841-09d04c056f3f","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4UlS3RvTaof5GUQQva4SEBi6qiXs3RhvtpFTdaSjRic="},"PREVIOUS":{"nodeId":"f0ab8d2e-cb96-492f-a897-ad96acae65c5","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+K7CpGLH9H4FDl9t+2TsdpLWyKQMQoocIGFt4mZU7PE="},"NEXT":{"nodeId":"4d238757-2ece-4f05-bc7f-8752cd92c8f6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"VguSNoLLFxVJMsvzfPHBloDD5mGz++MqKq08OCDKdEM="}},"text":"2. In  the\r\nfollowing,  we  will  summarize  the  milestone  detectors  of\r\nthis  period,  with  the  emergence  time  and  performance\r\nserving as the main clue to highlight the behind driving\r\ntechnology, as shown in Fig.3. 1)  Milestones:   Traditional   Detectors:If   we   consider\r\ntoday’s object detection technique as a revolution driven\r\nby  deep  learning,  then,  back  in  the  1990s,  we  would\r\nsee  the  ingenious  design  and  long-term  perspective  of\r\nearly computer vision. Most of the early object detection\r\nalgorithms were built based on handcrafted features. Due\r\nto the lack of effective image representation at that time,\r\npeople have to design sophisticated feature representations\r\nand a variety of speedup skills. Viola   Jones   Detectors:In   2001,   Viola   and   Jones\r\n[10],  [11]  achieved  real-time  detection  of  human  faces\r\nfor the first time without any constraints (e.g., skin color\r\nsegmentation). Running on a 700-MHz Pentium III CPU,\r\nthe detector was tens or even hundreds of times faster than\r\nother  algorithms  in  its  time  under  comparable  detection\r\naccuracy. The VJ detector follows a most straightforward\r\nway of detection, i.e., sliding windows: to go through all\r\npossible  locations  and  scales  in  an  image  to  see  if  any\r\nwindow contains a human face. Although it seems to be\r\na  very  simple  process,  the  calculation  behind  it  was  far\r\nbeyond the computer’s power of its time. The VJ detector\r\nhas dramatically improved its detection speed by incorpo-\r\nrating three important techniques: “integral image,” “fea-\r\nture selection,” and “detection cascades” (to be introduced\r\nin Section III). HOG Detector:In 2005, Dalal and Triggs [12] proposed\r\nthe histogram of oriented gradients (HOG) feature descrip-\r\ntor. HOG can be considered an important improvement of\r\nthe scale-invariant feature transform [29], [30] and shape\r\ncontexts [31] of its time.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Rw0yXNWQZUT2IoIePibwC1SB7Xsnt7ywcugpbRh0WOw="},"4d238757-2ece-4f05-bc7f-8752cd92c8f6":{"id_":"4d238757-2ece-4f05-bc7f-8752cd92c8f6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4UlS3RvTaof5GUQQva4SEBi6qiXs3RhvtpFTdaSjRic="},"PREVIOUS":{"nodeId":"315e0beb-3f98-4b49-9841-09d04c056f3f","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Rw0yXNWQZUT2IoIePibwC1SB7Xsnt7ywcugpbRh0WOw="},"NEXT":{"nodeId":"cb2bd23c-65e8-4e0b-a010-a119396c64c2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5JUP4YAue2Epgjb0YyzWvW50ye3E9Y1ht9rUmktlsLg="}},"text":"To balance the feature invariance\r\n(including translation, scale, illumination, and so on) and\r\nthe  nonlinearity,  the  HOG  descriptor  is  designed  to  be\r\ncomputed on a dense grid of uniformly spaced cells and\r\nuse overlapping local contrast normalization (on “blocks”). Although  HOG  can  be  used  to  detect  a  variety  of  object\r\nclasses,  it  was  motivated  primarily  by  the  problem  of\r\npedestrian detection. To detect objects of different sizes,\r\nthe HOG detector rescales the input image multiple times\r\nwhile keeping the size of a detection window unchanged. The  HOG  detector  has  been  an  important  foundation  of\r\nmany object detectors [13], [14], [32] and a large variety\r\nof computer vision applications for many years. Deformable Part-Based Model (DPM):DPM, as the win-\r\nners  of  VOC-07,  -08,  and  -09  detection  challenges,  was\r\nthe  epitome  of  the  traditional  object  detection  methods. DPM was originally proposed by Felzenszwalb et al. [13]\r\nin  2008  as  an  extension  of  the  HOG  detector. It  follows\r\nthe detection philosophy of “divide and conquer,” where\r\nthe training can be simply considered as the learning of\r\na proper way of decomposing an object, and the inference\r\ncan be considered as an ensemble of detections on different\r\nobject parts. For example, the problem of detecting a “car”\r\ncan be decomposed to the detection of its window, body,\r\nand wheels. This part of the work, a.k.a. “star-model,” was\r\nintroduced by Felzenszwalb et al. [13]. Later on, Girshick\r\n[14], [15], [33], [34] has further extended the star model\r\nto the “mixture models” to deal with the objects in the real\r\nworld under more significant variations and has made a\r\nseries of other improvements.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"VguSNoLLFxVJMsvzfPHBloDD5mGz++MqKq08OCDKdEM="},"cb2bd23c-65e8-4e0b-a010-a119396c64c2":{"id_":"cb2bd23c-65e8-4e0b-a010-a119396c64c2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_2","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4UlS3RvTaof5GUQQva4SEBi6qiXs3RhvtpFTdaSjRic="},"PREVIOUS":{"nodeId":"4d238757-2ece-4f05-bc7f-8752cd92c8f6","metadata":{"page_number":2,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"VguSNoLLFxVJMsvzfPHBloDD5mGz++MqKq08OCDKdEM="}},"text":"Although  today’s  object  detectors  have  far  surpassed\r\nDPM in detection accuracy, many of them are still deeply\r\ninfluenced by its valuable insights, e.g., mixture models,\r\n258PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5JUP4YAue2Epgjb0YyzWvW50ye3E9Y1ht9rUmktlsLg="},"4392db7b-f298-4ba9-8dac-b9de4b3f7c7b":{"id_":"4392db7b-f298-4ba9-8dac-b9de4b3f7c7b","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/N+Oso94MbkbwkbGQNJlcjnSYUl/siGcpkHIh9XyWvw="},"NEXT":{"nodeId":"e69573fe-ffa2-449c-84e8-c8b8b963d5b1","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"HtoQWiTvucd5Qv3+3OReydklCdkXczvHYyUxNytuWOI="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 2.Road map of object detection. Milestone detectors in this figure: VJ Det. [10], [11], HOG Det. [12], DPM [13], [14], [15], RCNN [16],\r\nSPPNet [17], Fast RCNN [18], Faster RCNN [19], YOLO [20], [21], [22], SSD [23], FPN [24], Retina-Net [25], CornerNet [26], CenterNet [27], and\r\nDETR [28]. hard  negative  mining  (HNM),  bounding  box  regression,\r\nand context priming. In 2010, Felzenszwalb and Girshick\r\nwere awarded the “lifetime achievement” by PASCAL VOC. 2)  Milestones:  CNN-Based  Two-Stage  Detectors:As  the\r\nperformance  of  handcrafted  features  became  saturated,\r\nthe  research  of  object  detection  reached  a  plateau  after\r\n2010. In 2012, the world saw the rebirth of convolutional\r\nneural  networks  (CNNs)  [35]. As  a  deep  convolutional\r\nnetwork is able to learn robust and high-level feature rep-\r\nresentations of an image, a natural question arises: can we\r\nintroduce it to object detection? Girshick et al. [16], [36]\r\ntook the lead to break the deadlocks in 2014 by proposing\r\nthe Regions with CNN features (RCNNs). Since then, object\r\ndetection  started  to  evolve  at  an  unprecedented  speed. There are two groups of detectors in the deep learning era:\r\n“two-stage detectors” and “one-stage detectors,” where the\r\nformer frames the detection as a “coarse-to-fine” process,\r\nwhile the latter frames it as to “complete in one step. ”\r\nRCNN:The idea behind RCNN is simple. It starts with\r\nthe extraction of a set of object proposals (object candidate\r\nboxes)  by  selective  search  [45].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5Mt0TyY0cFCt6UG8dGYVc0HItKdZ3ZSngTlemzbEQpc="},"e69573fe-ffa2-449c-84e8-c8b8b963d5b1":{"id_":"e69573fe-ffa2-449c-84e8-c8b8b963d5b1","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/N+Oso94MbkbwkbGQNJlcjnSYUl/siGcpkHIh9XyWvw="},"PREVIOUS":{"nodeId":"4392db7b-f298-4ba9-8dac-b9de4b3f7c7b","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5Mt0TyY0cFCt6UG8dGYVc0HItKdZ3ZSngTlemzbEQpc="},"NEXT":{"nodeId":"287554d3-59ce-47ad-8223-64f5bd00dbcf","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"AdsZpCiww4btM+tfhcghrzjJubNSiWyOKevWmWvwdkw="}},"text":"Then,  each  proposal  is\r\nrescaled to a fixed-size image and fed into a CNN model\r\npretrained on ImageNet (say, AlexNet [35]) to extract fea-\r\ntures. Finally, linear SVM classifiers are used to predict the\r\npresence of an object within each region and to recognize\r\nobject categories. RCNN yields a significant performance\r\nboost on VOC07, with a large improvement of mean Aver-\r\nage Precision (mAP) from 33.7% (DPM-v5 [46]) to 58.5%. Although  RCNN  has  made  great  progress,  its  drawbacks\r\nare  obvious:  the  redundant  feature  computations  on  a\r\nlarge  number  of  overlapped  proposals  (over  2000  boxes\r\nfrom one image) lead to an extremely slow detection speed\r\n(14 s per image with GPU). Later in the same year, SPPNet\r\n[17] was proposed and solved this problem. SPPNet:In 2014, He et al. [17] proposed spatial pyramid\r\npooling networks (SPPNet). Previous CNN models require\r\na  fixed-size  input,  e.g.,  a  224×224  image  for  AlexNet\r\n[35]. The main contribution of SPPNet is the introduction\r\nof  a  spatial  pyramid  pooling  (SPP)  layer,  which  enables\r\na  CNN  to  generate  a  fixed-length  representation  regard-\r\nless  of  the  size  of  the  image/region  of  interest  without\r\nrescaling it. When using SPPNet for object detection, the\r\nfeature maps can be computed from the entire image only\r\nonce,  and  then,  fixed-length  representations  of  arbitrary\r\nregions can be generated for training the detectors, which\r\navoids  repeatedly  computing  the  convolutional  features. SPPNet is more than 20 times faster than R-CNN without\r\nsacrificing any detection accuracy (VOC07 mAP=59.2%).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HtoQWiTvucd5Qv3+3OReydklCdkXczvHYyUxNytuWOI="},"287554d3-59ce-47ad-8223-64f5bd00dbcf":{"id_":"287554d3-59ce-47ad-8223-64f5bd00dbcf","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_3","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/N+Oso94MbkbwkbGQNJlcjnSYUl/siGcpkHIh9XyWvw="},"PREVIOUS":{"nodeId":"e69573fe-ffa2-449c-84e8-c8b8b963d5b1","metadata":{"page_number":3,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"HtoQWiTvucd5Qv3+3OReydklCdkXczvHYyUxNytuWOI="}},"text":"Although  SPPNet  has  effectively  improved  the  detection\r\nspeed,  it  still  has  some  drawbacks:  first,  the  training  is\r\nstill  multistage;  second,  SPPNet  only  fine-tunes  its  fully\r\nFig. 3.Accuracy improvement of object detection on VOC07, VOC12,\r\nand MS-COCO datasets. Detectors in this figure: DPM-v1 [13], DPM-\r\nv5 [37], RCNN [16], SPPNet [17], Fast RCNN [18], Faster RCNN [19],\r\nSSD [23], FPN [24], Retina-Net [25], RefineDet [38], TridentNet [39]\r\nCenterNet [40], FCOS [41], HTC [42], YOLOv4 [22], Deformable DETR\r\n[43], and Swin Transformer [44]. Vol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE259Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"AdsZpCiww4btM+tfhcghrzjJubNSiWyOKevWmWvwdkw="},"64442454-5aab-43b8-8594-767f34113e82":{"id_":"64442454-5aab-43b8-8594-767f34113e82","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"gf834UfOu4r2Es24X7Wq2vkb+6hOqapxAu3yCYsalCE="},"NEXT":{"nodeId":"b1026068-cbc4-4792-aa26-85999e40b5bb","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Vc84Nn0EqCHKx7IQm3x5k+LzLulbjXY4CWRsR87Sj3A="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nconnected layers while simply ignoring all previous layers. Later in the next year, Fast RCNN [18] was proposed and\r\nsolved these problems. Fast RCNN:In 2015, Girshick [18] proposed a Fast RCNN\r\ndetector,  which  is  a  further  improvement  of  R-CNN  and\r\nSPPNet  [16],  [17]. Fast  RCNN  enables  us  to  simultane-\r\nously train a detector and a bounding box regressor under\r\nthe same network configurations. On the VOC07 dataset,\r\nFast  RCNN  increased  the  mAP  from  58.5%  (RCNN)  to\r\n70.0% while with a detection speed over 200 times faster\r\nthan R-CNN. Although Fast-RCNN successfully integrates\r\nthe advantages of R-CNN and SPPNet, its detection speed\r\nis still limited by the proposal detection (see Section II-C1\r\nfor more details). Then, a question naturally arises: “can\r\nwe generate object proposals with a CNN model? ” Later,\r\nFaster R-CNN [19] answered this question. Faster RCNN:In 2015, Ren et al. [19], [47] proposed a\r\nFaster RCNN detector shortly after the Fast RCNN. Faster\r\nRCNN  is  the  first  near-real-time  deep  learning  detector\r\n(COCO mAP@.5=42.7%, VOC07 mAP=73.2%, and 17\r\nfps  with  ZF-Net  [48]). The  main  contribution  of  Faster-\r\nRCNN  is  the  introduction  of  a  region  proposal  network\r\n(RPN) that enables nearly cost-free region proposals. From\r\nR-CNN to Faster RCNN, most individual blocks of an object\r\ndetection system, e.g., proposal detection, feature extrac-\r\ntion,  and  bounding  box  regression,  have  been  gradually\r\nintegrated into a unified, end-to-end learning framework. Although  Faster  RCNN  breaks  through  the  speed  bottle-\r\nneck of Fast RCNN, there is still computation redundancy\r\nat  the  subsequent  detection  stage.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"weFkIwc7aqQqrGiKuQ9Edtwc6jpEqOGHaFQlyQk5zbc="},"b1026068-cbc4-4792-aa26-85999e40b5bb":{"id_":"b1026068-cbc4-4792-aa26-85999e40b5bb","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"gf834UfOu4r2Es24X7Wq2vkb+6hOqapxAu3yCYsalCE="},"PREVIOUS":{"nodeId":"64442454-5aab-43b8-8594-767f34113e82","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"weFkIwc7aqQqrGiKuQ9Edtwc6jpEqOGHaFQlyQk5zbc="},"NEXT":{"nodeId":"ab24adb0-204c-4439-8fb0-5cd834fb367b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"3moNAojZzRQ9TeSYjeZDqelGFVQ7Pj0g2DbLRZbCYAc="}},"text":"Later  on,  a  variety\r\nof  improvements  have  been  proposed,  including  RFCN\r\n[49]  and  Light  head  RCNN  [50]  (see  more  details  in\r\nSection III). Feature  Pyramid  Networks  (FPNs):In  2017,  Lin  et  al. [24] proposed FPN. Before FPN, most of the deep learning-\r\nbased detectors run detection only on the feature maps of\r\nthe  networks’  top  layer. Although  the  features  in  deeper\r\nlayers of a CNN are beneficial for category recognition, it\r\nis not conducive to localizing objects. To this end, a top-\r\ndown architecture with lateral connections is developed in\r\nFPN for building high-level semantics at all scales. Since a\r\nCNN naturally forms a feature pyramid through its forward\r\npropagation, the FPN shows great advances for detecting\r\nobjects with a wide variety of scales. Using FPN in a basic\r\nFaster  R-CNN  system,  it  achieves  state-of-the-art  single\r\nmodel detection results on the COCO dataset without bells\r\nand  whistles  (COCO  mAP@.5=59.1%). FPN  has  now\r\nbecome a basic building block of many latest detectors. 3)  Milestones:  CNN-Based  One-Stage  Detectors:Most  of\r\nthe two-stage detectors follow a coarse-to-fine processing\r\nparadigm. The  coarse  strives  to  improve  recall  ability,\r\nwhile  the  fine  refines  the  localization  on  the  basis  of\r\nthe  coarse  detection  and  places  more  emphasis  on  the\r\ndiscriminate ability. They can easily attain high precision\r\nwithout any bells and whistles but are rarely employed in\r\nengineering due to their poor speed and enormous com-\r\nplexity. On the contrary, one-stage detectors can retrieve\r\nall  objects  in  one-step  inference. They  are  well-liked  by\r\nmobile devices with real-time and easy-deployed features,\r\nbut  their  performance  suffers  noticeably  when  detecting\r\ndense and small objects. You  Only  Look  Once  (YOLO):YOLO  was  proposed  by\r\nJoseph  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Vc84Nn0EqCHKx7IQm3x5k+LzLulbjXY4CWRsR87Sj3A="},"ab24adb0-204c-4439-8fb0-5cd834fb367b":{"id_":"ab24adb0-204c-4439-8fb0-5cd834fb367b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"gf834UfOu4r2Es24X7Wq2vkb+6hOqapxAu3yCYsalCE="},"PREVIOUS":{"nodeId":"b1026068-cbc4-4792-aa26-85999e40b5bb","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Vc84Nn0EqCHKx7IQm3x5k+LzLulbjXY4CWRsR87Sj3A="},"NEXT":{"nodeId":"5639dd39-99f3-4f9f-8891-e0616244e068","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"CL7NF0fPWMLfucZ5+x+MwmwvMNOzWVfp8mXX5/nDYlw="}},"text":"[20]  in  2015. It  was  the  first  one-stage\r\ndetector in the deep learning era [20]. YOLO is extremely\r\nfast: a fast version of YOLO runs at 155 fps with VOC07\r\nmAP=52.7%,  while  its  enhanced  version  runs  at  45\r\nfps  with  VOC07  mAP=63.4%. YOLO  follows  a  totally\r\ndifferent  paradigm  from  two-stage  detectors:  to  apply  a\r\nsingle  neural  network  to  the  full  image. This  network\r\ndivides the image into regions and predicts bounding boxes\r\nand probabilities for each region simultaneously. In spite\r\nof its great improvement in detection speed, YOLO suffers\r\nfrom a drop in localization accuracy compared with two-\r\nstage detectors, especially for some small objects. YOLO’s\r\nsubsequent  versions  [21],  [22],  [51]  and  the  latter  pro-\r\nposed SSD [23] have paid more attention to this problem. Recently, YOLOv7 [52], a follow-up work from the YOLOv4\r\nteam,  has  been  proposed. It  outperforms  most  existing\r\nobject  detectors  in  terms  of  speed  and  accuracy  (range\r\nfrom  5  to  160  fps)  by  introducing  optimized  structures,\r\nsuch  as  dynamic  label  assignment  and  model  structure\r\nreparameterization. Single-Shot Multibox Detector (SSD):SSD was proposed\r\nby Liu et al. [23] in 2015. The main contribution of SSD is\r\nthe introduction of the multireference and multiresolution\r\ndetection techniques (to be introduced in Section II-C1),\r\nwhich  significantly  improves  the  detection  accuracy  of\r\na  one-stage  detector,  especially  for  some  small  objects. SSD has advantages in terms of both detection speed and\r\naccuracy (COCO mAP@.5=46.5%, a fast version runs at\r\n59 fps).","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"3moNAojZzRQ9TeSYjeZDqelGFVQ7Pj0g2DbLRZbCYAc="},"5639dd39-99f3-4f9f-8891-e0616244e068":{"id_":"5639dd39-99f3-4f9f-8891-e0616244e068","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_4","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"gf834UfOu4r2Es24X7Wq2vkb+6hOqapxAu3yCYsalCE="},"PREVIOUS":{"nodeId":"ab24adb0-204c-4439-8fb0-5cd834fb367b","metadata":{"page_number":4,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"3moNAojZzRQ9TeSYjeZDqelGFVQ7Pj0g2DbLRZbCYAc="}},"text":"The main difference between SSD and previous\r\ndetectors is that SSD detects objects of different scales on\r\ndifferent  layers  of  the  network,  while  the  previous  ones\r\nonly run detection on their top layers. RetinaNet:Despite their high speed and simplicity, the\r\none-stage detectors have trailed the accuracy of two-stage\r\ndetectors for years. Lin et al. [25] have explored the rea-\r\nsons behind and proposed RetinaNet in 2017. They found\r\nthat the extreme foreground-background class imbalance\r\nencountered during the training of dense detectors is the\r\ncentral cause. To this end, a new loss function named “focal\r\nloss” has been introduced in RetinaNet by reshaping the\r\nstandard cross entropy loss so that detector will put more\r\nfocus  on  hard,  misclassified  examples  during  training. Focal loss enables the one-stage detectors to achieve com-\r\nparable accuracy to two-stage detectors while maintaining\r\na very high detection speed (COCO mAP@.5=59.1%). CornerNet:Previous  methods  primarily  used  anchor\r\nboxes to provide classification and regression references. Objects  frequently  exhibit  variation  in  terms  of  number,\r\nlocation, scale, ratio, and so on. They have to follow the\r\npath  of  setting  up  a  large  number  of  reference  boxes\r\nto  better  match  ground  truths  in  order  to  achieve  high\r\nperformance. However,  the  network  would  suffer  from\r\nfurther category imbalance, lots of hand-designed hyper-\r\nparameters, and a long convergence time. To address these\r\n260PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CL7NF0fPWMLfucZ5+x+MwmwvMNOzWVfp8mXX5/nDYlw="},"b7b106bf-8f34-4ab5-8007-380bc5f765da":{"id_":"b7b106bf-8f34-4ab5-8007-380bc5f765da","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"AHf+fFASoDGOdFnf71TqeCKUFmyh6gs/ZTBazyEuN3o="},"NEXT":{"nodeId":"11ad829e-1000-49c1-bba5-49c4713093a6","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"v+bQTs9ezEm5Mg0X25cgPa+6jpG/UJGSV8c9WA8LTJM="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nTable 1Some Well-Known Object Detection Datasets and Their Statistics\r\nproblems, Law and Deng [26] discard the previous detec-\r\ntion paradigm, and view the task as a keypoint (corners\r\nof  a  box)  prediction  problem. After  obtaining  the  key\r\npoints, it will decouple and regroup the corner points using\r\nextra embedding information to form the bounding boxes. CornerNet  outperforms  most  one-stage  detectors  at  that\r\ntime (COCO mAP@.5=57.8%). CenterNet:Zhou et al. [40] proposed CenterNet in 2019. It  also  follows  a  keypoint-based  detection  paradigm  but\r\neliminates costly postprocesses, such as group-based key-\r\npoint  assignment  (in  CornerNet  [26],  ExtremeNet  [53],\r\nand so on) and NMS, resulting in a fully end-to-end detec-\r\ntion network. CenterNet considers an object to be a single\r\npoint (the object’s center) and regresses all of its attributes\r\n(such as size, orientation, location, and pose) based on the\r\nreference center point. The model is simple and elegant,\r\nand  it  can  integrate  3-D  object  detection,  human  pose\r\nestimation,  optical  flow  learning,  depth  estimation,  and\r\nother  tasks  into  a  single  framework. Despite  using  such\r\na  concise  detection  concept,  CenterNet  can  also  achieve\r\ncomparative detection results (COCO mAP@.5=61.1%). DETR:In   recent   years,   Transformers   have   deeply\r\naffected the entire field of deep learning, particularly the\r\nfield  of  computer  vision. Transformers  discard  the  tra-\r\nditional  convolution  operator  in  favor  of  attention-alone\r\ncalculation   in   order   to   overcome   the   limitations   of\r\nCNNs  and  obtain  a  global-scale  receptive  field. In  2020,\r\nCarion  et  al.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Zkr0AFQH0YBOJvgKEpMrCwm6NYpNG9hMmFGYrXeZbQI="},"11ad829e-1000-49c1-bba5-49c4713093a6":{"id_":"11ad829e-1000-49c1-bba5-49c4713093a6","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_5","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"AHf+fFASoDGOdFnf71TqeCKUFmyh6gs/ZTBazyEuN3o="},"PREVIOUS":{"nodeId":"b7b106bf-8f34-4ab5-8007-380bc5f765da","metadata":{"page_number":5,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Zkr0AFQH0YBOJvgKEpMrCwm6NYpNG9hMmFGYrXeZbQI="}},"text":"In  2020,\r\nCarion  et  al. [28]  proposed  DETR,  where  they  viewed\r\nobject detection as a set prediction problem and proposed\r\nan end-to-end detection network with Transformers. So far,\r\nobject  detection  has  entered  a  new  era  in  which  objects\r\ncan be detected without the use of anchor boxes or anchor\r\npoints. Later, Zhu et al. [43] proposed Deformable DETR\r\nto address the DETR’s long convergence time and limited\r\nperformance in detecting small objects. It achieves state-\r\nof-the-art  performance  on  the  MSCOCO  dataset  (COCO\r\nmAP@.5=71.9%). B. Object Detection Datasets and Metrics\r\n1)  Datasets:Building  larger  datasets  with  less  bias  is\r\nessential  for  developing  advanced  detection  algorithms. A  number  of  well-known  detection  datasets  have  been\r\nreleased  in  the  past  ten  years,  including  the  datasets\r\nof  PASCAL  VOC  Challenges  [54],  [55]  (e.g.,  VOC2007,\r\nVOC2012), the ImageNet Large Scale Visual Recognition\r\nChallenge (e.g., ILSVRC2014) [56], the MS-COCO Detec-\r\ntion Challenge [57], the Open Images Dataset [58], [59],\r\nObjects365 [60], and so on. The statistics of these datasets\r\nare given in Table1. Fig.4shows some image examples of\r\nFig. 4.Some example images and annotations in (a) PASCAL-VOC07, (b) ILSVRC, (c) MS-COCO, and (d) Open images. Vol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE261Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"v+bQTs9ezEm5Mg0X25cgPa+6jpG/UJGSV8c9WA8LTJM="},"9143d630-ef5f-4551-97f6-258eab7674ad":{"id_":"9143d630-ef5f-4551-97f6-258eab7674ad","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"XjcfxotACWqu30lIWnUCEU1BQ9FfG+gOYilJNKX+LYI="},"NEXT":{"nodeId":"b50f9e2e-badf-4100-a432-d6e8ed3101ca","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"3OJCqR/mEiBQ1MExGTOeHY/URK6GFs+DYccwOntZ0cY="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nthese datasets. Fig.3shows the improvements in detection\r\naccuracy on VOC07, VOC12, and MS-COCO datasets from\r\n2008 to 2021. Pascal VOC:The PASCAL Visual Object Classes (VOCs)\r\nChallenge1(from 2005 to 2012) [54], [55] was one of the\r\nmost important competitions in the early computer vision\r\ncommunity. Two  versions  of  Pascal-VOC  are  mostly  used\r\nin object detection: VOC07 and VOC12, where the former\r\nconsists of 5k tr. images+12k annotated objects, and the\r\nlatter consists of 11k tr. images+27k annotated objects. 20 classes of objects that are common in life are annotated\r\nin these two datasets, e.g., “person,” “cat,” “bicycle,” and\r\n“sofa. ”\r\nILSVRC:The ILSVRC2[56] has pushed forward the state\r\nof the art in generic object detection. ILSVRC is organized\r\neach year from 2010 to 2017. It contains a detection chal-\r\nlenge using ImageNet images [61]. The ILSVRC detection\r\ndataset contains 200 classes of visual objects. The number\r\nof its images/object instances is two orders of magnitude\r\nlarger than VOC. MS-COCO:MS-COCO3[57] is one of the most challeng-\r\ning object detection datasets available today. The annual\r\ncompetition  based  on  the  MS-COCO  dataset  has  been\r\nheld  since  2015. It  has  less  number  of  object  categories\r\nthan  ILSVRC  but  more  object  instances. For  example,\r\nMS-COCO-17 contains 164k images and 897k annotated\r\nobjects  from  80  categories. Compared  with  VOC  and\r\nILSVRC,  the  biggest  progress  of  MS-COCO  is  that  apart\r\nfrom the bounding box annotations; each object is further\r\nlabeled using per-instance segmentation to aid in precise\r\nlocalization.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"y6rXsW1JwaJnoRZgVCucGBStHZpZxJmrtONH5oonn1Y="},"b50f9e2e-badf-4100-a432-d6e8ed3101ca":{"id_":"b50f9e2e-badf-4100-a432-d6e8ed3101ca","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"XjcfxotACWqu30lIWnUCEU1BQ9FfG+gOYilJNKX+LYI="},"PREVIOUS":{"nodeId":"9143d630-ef5f-4551-97f6-258eab7674ad","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"y6rXsW1JwaJnoRZgVCucGBStHZpZxJmrtONH5oonn1Y="},"NEXT":{"nodeId":"090ff9df-8745-440f-8e09-7008fbbaabbf","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"o6jgifL+hD3RcDhG91+dFRlm9FeDHM/wygkXnV2RqwE="}},"text":"In  addition,  MS-COCO  contains  more  small\r\nobjects (whose area is smaller than 1% of the image) and\r\nmore  densely  located  objects. Just  like  ImageNet  in  its\r\ntime, MS-COCO has become the de facto standard for the\r\nobject detection community. Open Images:The year 2018 sees the introduction of the\r\nopen  images  detection  (OID)  challenge4[62],  following\r\nMS-COCO but at an unprecedented scale. There are two\r\ntasks  in  open  images:  1)  standard  object  detection  and\r\n2) visual relationship detection that detects paired objects\r\nin particular relations. For the standard detection task, the\r\ndataset consists of 1910k images with 15 440k annotated\r\nbounding boxes on 600 object categories. 2)  Metrics:How  can  we  evaluate  the  accuracy  of  a\r\ndetector? This  question  may  have  different  answers  at\r\ndifferent times. In the early time’s detection research, there\r\nare  no  widely  accepted  evaluation  metrics  on  detection\r\naccuracy. For  example,  in  the  early  research  of  pedes-\r\ntrian detection [12], the “miss rate versus false positives\r\nper window (FPPW)” was commonly used as the metric. However, the per-window measurement can be flawed and\r\nfails to predict full image performance [63]. In 2009, the\r\nCaltech  pedestrian  detection  benchmark  was  introduced\r\n1http://host.robots.ox.ac.uk/pascal/VOC/\r\n2http://image-net.org/challenges/LSVRC/\r\n3http://cocodataset.org/\r\n4https://storage.googleapis.com/openimages/web/index.html\r\n[63],  [64],  and  since  then,  the  evaluation  metric  has\r\nchanged from FPPW to false positives per-image (FPPI). In recent years, the most frequently used evaluation for\r\ndetection is “average precision (AP),” which was originally\r\nintroduced in VOC2007. AP is defined as the average detec-\r\ntion precision under different recalls and is usually eval-\r\nuated  in  a  category-specific  manner.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"3OJCqR/mEiBQ1MExGTOeHY/URK6GFs+DYccwOntZ0cY="},"090ff9df-8745-440f-8e09-7008fbbaabbf":{"id_":"090ff9df-8745-440f-8e09-7008fbbaabbf","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"XjcfxotACWqu30lIWnUCEU1BQ9FfG+gOYilJNKX+LYI="},"PREVIOUS":{"nodeId":"b50f9e2e-badf-4100-a432-d6e8ed3101ca","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"3OJCqR/mEiBQ1MExGTOeHY/URK6GFs+DYccwOntZ0cY="},"NEXT":{"nodeId":"f0820303-b215-442d-b66f-514652d0ede4","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"2U1mudNSeSBW2/jPbEECyS6P+OObUW8+bV0m9Xz+Y6s="}},"text":"The  mAP  averaged\r\nover  all  categories  is  usually  used  as  the  final  metric  of\r\nperformance. To measure the object localization accuracy,\r\nthe  intersection  over  union  (IoU)  between  the  predicted\r\nbox and the ground truth is used to verify whether it is\r\ngreater  than  a  predefined  threshold,  say,  0.5. If  yes,  the\r\nobject will be identified as “detected,” otherwise, “missed. ”\r\nThe 0.5-IoU mAP has then become the de facto metric for\r\nobject detection. After   2014,   due   to   the   introduction   of   MS-COCO\r\ndatasets, researchers started to pay more attention to the\r\naccuracy  of  object  localization. Instead  of  using  a  fixed\r\nIoU  threshold,  MS-COCO  AP  is  averaged  over  multiple\r\nIoU thresholds between 0.5 and 0.95, which encourages\r\nmore  accurate  object  localization  and  may  be  of  great\r\nimportance for some real-world applications (e.g., imagine\r\nthere is a robot trying to grasp a spanner). C. Technical Evolution in Object Detection\r\nIn   this   section,   we   will   introduce   some   important\r\nbuilding  blocks  of  a  detection  system  and  its  technical\r\nevolutions. First, we describe the multiscale and context\r\npriming  on  model  designing,  followed  by  the  sample\r\nselection  strategy  and  the  design  of  the  loss  function  in\r\nthe  training  process  and,  finally,  the  nonmaximum  sup-\r\npression  in  the  inference. The  time  stamp  in  the  chart\r\nand  text  is  supplied  by  the  publication  time  of  papers. The evolution order  shown in the figures is  primarily to\r\nassist readers in understanding, and there may be temporal\r\noverlap.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"o6jgifL+hD3RcDhG91+dFRlm9FeDHM/wygkXnV2RqwE="},"f0820303-b215-442d-b66f-514652d0ede4":{"id_":"f0820303-b215-442d-b66f-514652d0ede4","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_6","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"XjcfxotACWqu30lIWnUCEU1BQ9FfG+gOYilJNKX+LYI="},"PREVIOUS":{"nodeId":"090ff9df-8745-440f-8e09-7008fbbaabbf","metadata":{"page_number":6,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"o6jgifL+hD3RcDhG91+dFRlm9FeDHM/wygkXnV2RqwE="}},"text":"1)  Technical Evolution of Multiscale Detection:Multiscale\r\ndetection  of  objects  with  “different  sizes”  and  “different\r\naspect  ratios”  is  one  of  the  main  technical  challenges  in\r\nobject detection. In the past 20 years, multiscale detection\r\nhas gone through multiple historical periods, as shown in\r\nFig.5. Feature Pyramids+Sliding Windows:After the VJ detec-\r\ntor,  researchers  started  to  pay  more  attention  to  a  more\r\nintuitive way of detection, i.e., by building “feature pyra-\r\nmid+sliding windows. ” From 2004, a number of milestone\r\ndetectors were built based on this paradigm, including the\r\nHOG  detector,  DPM,  and  so  on. They  frequently  glide  a\r\nfixed-size detection window over the image, paying little\r\nattention to “different aspect ratios. ” To detect objects with\r\na more complex appearance, Girshick et al. began to seek\r\nbetter solutions outside the feature pyramid. The “mixture\r\nmodel” [15] was a solution at that time, i.e., to train mul-\r\ntiple detectors for objects of different aspect ratios. Apart\r\nfrom  this,  exemplar-based  detection  [32],  [70]  provided\r\nanother  solution  by  training  individual  models  for  every\r\nobject instance (exemplar). 262PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"2U1mudNSeSBW2/jPbEECyS6P+OObUW8+bV0m9Xz+Y6s="},"5ea2bd14-99d8-4694-a61a-3d71431a32fe":{"id_":"5ea2bd14-99d8-4694-a61a-3d71431a32fe","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"hks9j0eM+r8P0Klbn5EAy7WF9mNDfeCLzH3cdeUsuAc="},"NEXT":{"nodeId":"760a9164-4280-42c9-a8b3-201a69a0b728","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"URCN2j2+XVYs6gZoJjeH5udzruV1TQrW5wPrCVCE5FM="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 5.Evolution of multiscale detection techniques in object detection. Detectors in this figure: VJ Det. [10], HOG Det. [12], DPM [13],\r\nExemplar SVM [32], Overfeat [65], RCNN [16], SPPNet [17], Fast RCNN [18], Faster RCNN [19], DNN Det. [66], YOLO [20], SSD [23], Unified Det. [67], FPN [24], RetinaNet [25], RefineDet [38], Cascade R-CNN [68], Swin Transformer [44], FCOS [41], YOLOv4 [22], CornerNet [26], CenterNet\r\n[40], Reppoints [69], and DETR [28]. Detection With Object proposals:Object proposals refer\r\nto a group of class-agnostic reference boxes that are likely\r\nto  contain  any  objects. Detection  with  object  proposals\r\nhelps to avoid the exhaustive sliding window search across\r\nan image. We refer readers to the following papers for a\r\ncomprehensive review on this topic [71], [72]. Early time’s\r\nproposal detection methods followed a bottom-up detec-\r\ntion philosophy [73], [74]. After 2014, with the popularity\r\nof deep CNN in visual recognition, the top-down, learning-\r\nbased approaches began to show more advantages in this\r\nproblem  [19],  [75],  [76]. Now,  the  proposal  detection\r\ngradually  slipped  out  of  sight  after  the  rise  of  one-stage\r\ndetectors. Deep  Regression  and  Anchor-Free  Detection:In  recent\r\nyears, with the increase of GPU’s computing power, mul-\r\ntiscale detection has become a more and more straightfor-\r\nward and brute force. The idea of using deep regression to\r\nsolve multiscale problems becomes simple, i.e., to directly\r\npredict  the  coordinates  of  a  bounding  box  based  on  the\r\ndeep learning features [20], [66]. After 2018, researchers\r\nbegan to think about the object detection problem from the\r\nperspective of keypoint detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"LE/JFSNnG8ChwSH65BWYoD/8lRSphUBal4DQRamqUvY="},"760a9164-4280-42c9-a8b3-201a69a0b728":{"id_":"760a9164-4280-42c9-a8b3-201a69a0b728","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"hks9j0eM+r8P0Klbn5EAy7WF9mNDfeCLzH3cdeUsuAc="},"PREVIOUS":{"nodeId":"5ea2bd14-99d8-4694-a61a-3d71431a32fe","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"LE/JFSNnG8ChwSH65BWYoD/8lRSphUBal4DQRamqUvY="},"NEXT":{"nodeId":"10b407ef-878a-4561-82f4-1ea0f554d1c8","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EOD7pLasTaQK80QBc/CY3WvAwtF03uTzi+UiS3SCLBk="}},"text":"These methods often fol-\r\nlow two ideas: one is the group-based method that detects\r\nkeypoints (corners, centers, or representative points) and\r\nthen conducts objectwise grouping [26], [53], [69], [77];\r\nthe other is the group-free method that regards an object as\r\none/many points and then regresses the object attributes\r\n(size, ratio, and so on) under the reference of the points\r\n[40], [41]. Multireference/Multiresolution Detection:Multireference\r\ndetection  is  now  the  most  used  method  for  multiscale\r\ndetection  [19],  [22],  [23],  [41],  [47],  [51]. The  main\r\nidea  of  multireference  detection  [19],  [22],  [23],  [41],\r\n[47],  [51]  is  to  first  define  a  set  of  references  (a.k.a. anchors, including boxes and points) at every location of\r\nan image and then predict the detection box based on these\r\nreferences. Another  popular  technique  is  multiresolution\r\ndetection  [23],  [24],  [44],  [67],  [68],  i.e.,  by  detecting\r\nobjects of different scales at different layers of the network. Multireference  and  multiresolution  detection  have  now\r\nbecome two basic building blocks in state-of-the-art object\r\ndetection systems. 2)  Technical Evolution of Context Priming:Visual objects\r\nare  usually  embedded  in  a  typical  context  with  the  sur-\r\nrounding environments. Our brain takes advantage of the\r\nassociations among objects and environments to facilitate\r\nvisual perception and cognition [96]. Context priming has\r\nlong  been  used  to  improve  detection. Fig.6shows  the\r\nevolution of context priming in object detection. Detection With Local Context:Local context refers to the\r\nvisual  information  in  the  area  that  surrounds  the  object\r\nto detect. It has long been acknowledged that local con-\r\ntext  helps  improve  object  detection.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"URCN2j2+XVYs6gZoJjeH5udzruV1TQrW5wPrCVCE5FM="},"10b407ef-878a-4561-82f4-1ea0f554d1c8":{"id_":"10b407ef-878a-4561-82f4-1ea0f554d1c8","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_7","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"hks9j0eM+r8P0Klbn5EAy7WF9mNDfeCLzH3cdeUsuAc="},"PREVIOUS":{"nodeId":"760a9164-4280-42c9-a8b3-201a69a0b728","metadata":{"page_number":7,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"URCN2j2+XVYs6gZoJjeH5udzruV1TQrW5wPrCVCE5FM="}},"text":"It has long been acknowledged that local con-\r\ntext  helps  improve  object  detection. In  the  early  2000s,\r\nSinha and Torralba [78] found that the inclusion of local\r\ncontextual  regions,  such  as  the  facial  bounding  contour,\r\nsubstantially improves face detection performance. Dalal\r\nand  Triggs  [12]  also  found  that  incorporating  a  small\r\namount of background information improves the accuracy\r\nof pedestrian detection. Recent deep learning-based detec-\r\ntors  can  also  be  improved  with  local  context  by  simply\r\nenlarging the networks’ receptive field or the size of object\r\nproposals [79], [80], [81], [82], [83], [84], [97]. Detection  With  Global  Context:Global  context  exploits\r\nscene configuration as an additional source of information\r\nfor object detection. For early time detectors, a common\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE263Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EOD7pLasTaQK80QBc/CY3WvAwtF03uTzi+UiS3SCLBk="},"31a80af6-14c0-4a12-8615-be85ff796ac6":{"id_":"31a80af6-14c0-4a12-8615-be85ff796ac6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FE7+iLlhlT1xOJggs4nIhermlgmyQHUo4fBLJt6u0Xc="},"NEXT":{"nodeId":"310b7a74-54a9-40bc-be0b-08635c0496f6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"QEj4g70fwFVuYEzOxHKo0ORn56m2gpVNp12XRMiHkR8="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 6.Evolution of context priming in object detection. Detectors in this figure: Face Det. [78], MultiPath [79], GBDNet [80], [81], CC-Net\r\n[82], MultiRegion-CNN [83], CoupleNet [84], DPM [14], [15], StructDet [85], ION [86], RFCN++[87], RBFNet [88], TridentNet [39], Non-Local\r\n[89], DETR [28], CtxSVM [90], PersonContext [91], SMN [92], RelationNet [93], SIN [94], and RescoringNet [95]. way of integrating global context is to integrate a statistical\r\nsummary  of  the  elements  that  comprise  the  scene,  such\r\nas Gist [96]. For recent detectors, there are two methods\r\nto  integrate  the  global  context. The  first  method  is  to\r\ntake advantage of deep convolution, dilated convolution,\r\ndeformable convolution, and pooling operation [39], [87],\r\n[88] to receive a large receptive field (even larger than the\r\ninput  image). However,  now,  researchers  have  explored\r\nthe potential to apply attention-based mechanisms (Non-\r\nLocal,  Transformers,  and  so  on)  to  achieve  a  full-image\r\nreceptive field and have obtained great success [28], [89]. The  second  method  is  to  think  of  the  global  context  as\r\na kind of sequential information and to learn it with the\r\nrecurrent neural networks [86], [98]. Context  Interactive:Context  interactive  refers  to  the\r\nconstraints and dependencies that convey between visual\r\nelements. Some  recent  studies  suggested  that  modern\r\ndetectors  can  be  improved  by  considering  context  inter-\r\nactives. Some  recent  improvements  can  be  grouped  into\r\ntwo categories, where the first one is to explore the rela-\r\ntionship between individual objects [15], [85], [90], [92],\r\n[93], [95], and the second one is to explore the dependen-\r\ncies between objects and scenes [91], [94].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"pcbezH8LaWl5/fmxwAnt609XjwMoH9BBESdg3xCT5e0="},"310b7a74-54a9-40bc-be0b-08635c0496f6":{"id_":"310b7a74-54a9-40bc-be0b-08635c0496f6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FE7+iLlhlT1xOJggs4nIhermlgmyQHUo4fBLJt6u0Xc="},"PREVIOUS":{"nodeId":"31a80af6-14c0-4a12-8615-be85ff796ac6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"pcbezH8LaWl5/fmxwAnt609XjwMoH9BBESdg3xCT5e0="},"NEXT":{"nodeId":"79aa8875-6e04-4a12-970d-4ce36103e973","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"n7HxO4QS1CVMB2vxKbzb1JV+cOySsVXp0Nv/QrbKMoE="}},"text":"3)  Technical  Evolution  of  Hard  Negative  Mining:The\r\ntraining of a detector is essentially an imbalanced learning\r\nproblem. In  the  case  of  sliding  window-based  detectors,\r\nthe imbalance between backgrounds and objects could be\r\nas  extreme  as  107:1  [71]. In  this  case,  using  all  back-\r\ngrounds will be harmful to training as the vast number of\r\neasy negatives will overwhelm the learning process. HNM\r\naims to overcome this problem. The technical evolution of\r\nHNM is shown in Fig.7. Bootstrap:Bootstrap in object detection refers to a group\r\nof  training  techniques  in  which  the  training  starts  with\r\na  small  part  of  background  samples  and  then  iteratively\r\nadds new misclassified samples. In early times detectors,\r\nbootstrap was commonly used with the purpose of reduc-\r\ning the training computations over millions of backgrounds\r\n[10], [99], [100]. Later, it became a standard technique in\r\nDPM and HOG detectors [12], [13] for solving the data\r\nimbalance problem. HNM  in  Deep  Learning-Based  Detectors:In  the  deep\r\nlearning  era,  due  to  the  increase  of  computing  power,\r\nbootstrap  was  shortly  discarded  in  object  detection  dur-\r\ning 2014–2016 [16], [17], [18], [19], [20]. To ease the\r\ndata-imbalance  problem  during  training,  detectors  such\r\nas  Faster  RCNN  and  YOLO  simply  balance  the  weights\r\nbetween  the  positive  and  negative  windows. However,\r\nresearchers later noticed that this cannot completely solve\r\nthe  imbalanced  problem  [25]. To  this  end,  the  boot-\r\nstrap  was  reintroduced  to  object  detection  after  2016\r\n[23], [38], [101], [102]. An alternative improvement is to\r\ndesign new loss functions [25] by reshaping the standard\r\ncross entropy loss so that it will put more focus on hard,\r\nmisclassified examples [25].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QEj4g70fwFVuYEzOxHKo0ORn56m2gpVNp12XRMiHkR8="},"79aa8875-6e04-4a12-970d-4ce36103e973":{"id_":"79aa8875-6e04-4a12-970d-4ce36103e973","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_8","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FE7+iLlhlT1xOJggs4nIhermlgmyQHUo4fBLJt6u0Xc="},"PREVIOUS":{"nodeId":"310b7a74-54a9-40bc-be0b-08635c0496f6","metadata":{"page_number":8,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"QEj4g70fwFVuYEzOxHKo0ORn56m2gpVNp12XRMiHkR8="}},"text":"4)  Technical Evolution of Loss Function:The loss function\r\nmeasures how well the model matches the data (i.e., the\r\ndeviation of the predictions from the true labels). Calcu-\r\nlating the loss yields the gradients of the model weights,\r\nwhich can subsequently be updated by backpropagation to\r\nbetter suit the data. Classification loss and localization loss\r\nmake up the supervision of the object detection problem\r\n[see (1)]. A general form of the loss function can be written\r\nas follows:\r\nL(p,p∗,t,t∗) =Lcls.(p,p∗) +βI(t)Lloc.(t,t∗)\r\nI(t) =\r\n(\r\n1,IoU{a,a∗}> η\r\n0,else(1)\r\n264PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"n7HxO4QS1CVMB2vxKbzb1JV+cOySsVXp0Nv/QrbKMoE="},"6da1d594-1db2-448f-8a56-44f3d42fcd8b":{"id_":"6da1d594-1db2-448f-8a56-44f3d42fcd8b","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"I8WEcxut+w2lgeopyenwj8YRmPy8q56ivtcKg76CMwk="},"NEXT":{"nodeId":"8408e751-f507-49f3-b6c5-f38a9589cbd6","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Tz205ntfUBZ5s/UwwNuXcMMpP64LyN0wIgSlnprPJ2E="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 7.Evolution of HNM techniques in object detection. Detectors in this figure: Face Det. [99], Haar Det. [100], VJ Det. [10], HOG Det. [12],\r\nDPM [13], [15], RCNN [16], SPPNet [17], Fast RCNN [18], Faster RCNN [19], YOLO [20], SSD [23], FasterPed [101], OHEM [102], RetinaNet [25],\r\nRefineDet [38], FCOS [41], and YOLOv4 [22]. wheretandt∗are the locations of predicted and ground-\r\ntruth  bounding  boxes,  andpandp∗are  their  category\r\nprobabilities. IoU{a,a∗}is the IoU between the reference\r\nbox/pointaand its ground trutha∗.ηis an IoU threshold,\r\nsay, 0.5. If an anchor box/point does not match any objects,\r\nits localization loss does not count in the final loss. Classification Loss:It is used to evaluate the divergence\r\nof the predicted category from the actual category, which\r\nwas not thoroughly investigated in prevIoUs work, such as\r\nYOLOv1 [20] and YOLOv2 [51] employing mean square\r\nerror (mse)/L2 loss. Later, the cross-entropy (CE) loss is\r\ntypically used [21], [23], [47]. L2 loss is a measure in the\r\nEuclidean space, whereas CE loss can measure distribution\r\ndifferences (termed a form of likelihood). The prediction\r\nof  classification  is  a  probability,  so  CE  loss  is  preferable\r\nto  L2  loss  with  greater  misclassification  cost  and  lower\r\ngradient  vanishing  effect. For  improving  categorization\r\nefficiency, label smooth has been proposed to enhance the\r\nmodel generalization ability and solve the overconfidence\r\nproblem  on  noise  labels  [103],  [104],  and  focal  loss  is\r\ndesigned to solve the problem of category imbalance and\r\ndifferences in classification difficulty [25].","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GwHRS9YAu9za/pnFig4vwqOi/3w0kMixKI8TB/Va0vI="},"8408e751-f507-49f3-b6c5-f38a9589cbd6":{"id_":"8408e751-f507-49f3-b6c5-f38a9589cbd6","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"I8WEcxut+w2lgeopyenwj8YRmPy8q56ivtcKg76CMwk="},"PREVIOUS":{"nodeId":"6da1d594-1db2-448f-8a56-44f3d42fcd8b","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"GwHRS9YAu9za/pnFig4vwqOi/3w0kMixKI8TB/Va0vI="},"NEXT":{"nodeId":"8386a417-d2ee-4210-891f-dd5cff0c8d30","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Vq45YrQeuXEsvNJRQZ31UcECYUtmw5REA5gji7Jw4a4="}},"text":"Localization  Loss:It  is  used  to  optimize  position  and\r\nsize  deviation. L2  loss  is  prevalent  in  early  research\r\n[16], [20], [51], but it is highly affected by outliers and\r\nprone to gradient explosion. Combining the benefits of L1\r\nloss and L2 loss, the researchers propose Smooth L1 loss\r\n[18], as illustrated in the following formula:\r\nSmoothL1(x) =\r\n(\r\n0.5x2,if|x|<1\r\n|x|−0.5,else(2)\r\nwherexdenotes the difference between the target and pre-\r\ndicted values. When calculating the error, the above losses\r\ntreat  four  numbers(x,y,w,h)representing  a  bounding\r\nbox as independent variables; however, a correlation exists\r\nbetween them. Moreover, IoU is utilized to determine if the\r\nprediction box corresponds to the actual ground-truth box\r\nin  evaluation. Equal  Smooth  L1  values  will  have  totally\r\ndifferent IoU values; hence, IoU loss [105] is introduced\r\nas follows:\r\nIoU loss=−log(IoU).(3)\r\nFollowing  that,  several  algorithms  improved  IoU  loss. Generalized  IoU  (G-IoU)  [106]  improved  the  case  when\r\nIoU loss could not optimize the nonoverlapping bounding\r\nboxes,  i.e.,  IoU=0. According  to  Distance-IoU  [107],\r\na  successful  detection  regression  loss  should  meet  three\r\ngeometric metrics: overlap area, center point distance, and\r\naspect ratio. Thus, based on IoU loss and G-IoU loss, the\r\ndistance IoU (DIoU) is defined as the distance between the\r\ncenter point of the prediction and the ground truth, and\r\nthe complete IoU (CIoU) [107] considered the aspect ratio\r\ndifference on the basis of DIoU. 5)  Technical  Evolution  of  Nonmaximum  Suppression:As\r\nthe  neighboring  windows  usually  have  similar  detection\r\nscores,  the  nonmaximum  suppression  is  used  as  a  post-\r\nprocessing step to remove the replicated bounding boxes\r\nand obtain the final detection result.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Tz205ntfUBZ5s/UwwNuXcMMpP64LyN0wIgSlnprPJ2E="},"8386a417-d2ee-4210-891f-dd5cff0c8d30":{"id_":"8386a417-d2ee-4210-891f-dd5cff0c8d30","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_9","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"I8WEcxut+w2lgeopyenwj8YRmPy8q56ivtcKg76CMwk="},"PREVIOUS":{"nodeId":"8408e751-f507-49f3-b6c5-f38a9589cbd6","metadata":{"page_number":9,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Tz205ntfUBZ5s/UwwNuXcMMpP64LyN0wIgSlnprPJ2E="}},"text":"In the early times of\r\nobject  detection,  NMS  was  not  always  integrated  [121]. This is because the desired output of an object detection\r\nsystem was not entirely clear at that time. Fig.8shows the\r\nevolution of NMS in the past 20 years. Greedy  Selection:It  is  an  old-fashioned  but  the  most\r\npopular way to perform NMS. The idea behind it is simple\r\nand intuitive: for a set of overlapped detections, the bound-\r\ning  box  with  the  maximum  detection  score  is  selected,\r\nwhile  its  neighboring  boxes  are  removed  according  to  a\r\npredefined  overlap  threshold. Although  greedy  selection\r\nhas now become the de facto method for NMS, it still has\r\nsome  space  for  improvement. First,  the  top-scoring  box\r\nmay not be the best fit. Second, it may suppress nearby\r\nobjects. Finally, it does not suppress false positives [116]. Many  works  have  been  proposed  to  solve  the  problems\r\nmentioned above [107], [112], [114], [115]. Bounding  Box  Aggregation:BB  aggregation  is  another\r\ngroup of techniques for NMS [10], [65], [116], [117] with\r\nthe  idea  of  combining  or  clustering  multiple  overlapped\r\nbounding boxes into one final detection. The advantage of\r\nthis  type  of  method  is  that  it  takes  full  consideration  of\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE265Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Vq45YrQeuXEsvNJRQZ31UcECYUtmw5REA5gji7Jw4a4="},"f03594bc-ebfa-4478-aa25-4d5a9b4e8055":{"id_":"f03594bc-ebfa-4478-aa25-4d5a9b4e8055","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"9TU9potGYCw5BPiyITEHJWGtfF6V5IUc1mV7VW4fano="},"NEXT":{"nodeId":"402cf23a-06d6-49b7-9c3b-802f95cdf740","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5FIaCJ0H2GfYHuYncktoOxthvUfUKwk80Keyb2hR4vM="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 8.Evolution of nonmax suppression (NMS) techniques in object detection from 1994 to 2021: 1) greedy selection; 2) bounding box\r\naggregation; 3) learning to NMS; and 4) NMS-free detection. Detectors in this figure: Face Det. [108], HOG Det. [12], DPM [13], [15], RCNN\r\n[16], SPPNet [17], Fast RCNN [18], Faster RCNN [19], YOLO [20], SSD [23], FPN [24], RetinaNet [25], FCOS [41], StrucDet [85], MAP-Det [109],\r\nLearnNMS [110], RelationNet [93], Learn2Rank [111], SoftNMS [112], FitnessNMS [113], SofterNMS [114], AdaptiveNMS [115], DIoUNMS [107],\r\nOverfeat [65], APC-NMS [116], MAPC [117], WBF [118], ClusterNMS [119], CenterNet [40], DETR [28], and POTO [120]. object relationships and their spatial layout [118], [119]. Some well-known detectors use this method, such as the\r\nVJ detector [10] and the Overfeat (winner of ILSVRC-13\r\nlocalization task) [65]. Learning-Based NMS:A recent group of NMS improve-\r\nments  that  have  recently  received  much  attention  is\r\nlearning-based  NMS  [85],  [93],  [109],  [110],  [111],\r\n[122]. The  main  idea  is  to  think  of  NMS  as  a  filter\r\nto  rescore  all  raw  detections  and  to  train  the  NMS  as\r\npart  of  a  network  in  an  end-to-end  fashion  or  train\r\na  net  to  imitate  NMS’s  behavior. These  methods  have\r\nshown   promising   results   in   improving   occlusion   and\r\ndense object detection over traditional handcrafted NMS\r\nmethods.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"lktsd8NegNxC+78aZiZ4DXq3emJVTFn7CD+SlNF6kdA="},"402cf23a-06d6-49b7-9c3b-802f95cdf740":{"id_":"402cf23a-06d6-49b7-9c3b-802f95cdf740","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"9TU9potGYCw5BPiyITEHJWGtfF6V5IUc1mV7VW4fano="},"PREVIOUS":{"nodeId":"f03594bc-ebfa-4478-aa25-4d5a9b4e8055","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"lktsd8NegNxC+78aZiZ4DXq3emJVTFn7CD+SlNF6kdA="},"NEXT":{"nodeId":"4197bdfd-549d-4010-b102-ea6fdf15d73d","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"KDqxDkagT27qmPXeyYtQHLm3kASH7cNEXFqwFV9AkYM="}},"text":"NMS-Free  Detector:To  release  from  NMS  and  achieve\r\na   fully   end-to-end   object   detection   training   network,\r\nresearchers  developed  a  series  of  methods  to  complete\r\none-to-one  label  assignment  (a.k.a. one  object  with  just\r\none  prediction  box)  [28],  [40],  [120]. These  methods\r\nfrequently  adhere  to  a  rule  that  calls  for  the  use  of  the\r\nhighest-quality  box  for  training  in  order  to  achieve  free\r\nNMS. NMS-free detectors are more similar to the human\r\nvisual perception system and are also a possible way to the\r\nfuture of object detection. III. S P E E D U P  O F  D E T E C T I O N\r\nThe acceleration of a detector has long been a challenging\r\nproblem. The speedup techniques in object detection can\r\nbe  divided  into  three  levels  of  groups:  the  speedup  of\r\n“detection pipeline,” “detector backbone,” and “numerical\r\ncomputation,” as shown in Fig. 9. Refer to [123] for a more\r\ndetailed version. A. Feature Map Shared Computation\r\nAmong  the  different  computational  stages  of  a  detec-\r\ntor,  feature  extraction  usually  dominates  the  amount\r\nof   computation. The   most   commonly   used   idea   to\r\nreduce the feature computational redundancy is to com-\r\npute  the  feature  map  of  the  whole  image  only  once\r\n[18], [19], [124], which have achieved tens or even hun-\r\ndreds of times of acceleration. B. Cascaded Detection\r\nCascaded detection is a commonly used technique [10],\r\n[125]. It  takes  a  coarse-to-fine  detection  philosophy:  to\r\nfilter out most of the simple background windows using\r\nsimple calculations and then to process those more difficult\r\nwindows  with  complex  ones.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5FIaCJ0H2GfYHuYncktoOxthvUfUKwk80Keyb2hR4vM="},"4197bdfd-549d-4010-b102-ea6fdf15d73d":{"id_":"4197bdfd-549d-4010-b102-ea6fdf15d73d","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_10","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"9TU9potGYCw5BPiyITEHJWGtfF6V5IUc1mV7VW4fano="},"PREVIOUS":{"nodeId":"402cf23a-06d6-49b7-9c3b-802f95cdf740","metadata":{"page_number":10,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5FIaCJ0H2GfYHuYncktoOxthvUfUKwk80Keyb2hR4vM="}},"text":"In  recent  years,  cascaded\r\ndetection  has  been  especially  applied  to  those  detection\r\ntasks of “small objects in large scenes,” e.g., face detection\r\n[126], [127] and pedestrian detection [101], [124], [128]. C. Network Pruning and Quantification\r\n“Network  pruning”  and  “network  quantification”  are\r\ntwo commonly used methods to speed up a CNN model. The  former  refers  to  pruning  the  network  structure  or\r\nweights, and the latter refers to reducing their code length. The research of “network pruning” can be traced back to\r\nas early as the 1980s [129]. The recent network pruning\r\nmethods  usually  take  an  iterative  training  and  pruning\r\nprocess, i.e., to remove only a small group of unimportant\r\nweights after each stage of training, and to repeat those\r\noperations [130]. The recent works on network quantifi-\r\ncation mainly focus on network binarization, which aims\r\n266PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KDqxDkagT27qmPXeyYtQHLm3kASH7cNEXFqwFV9AkYM="},"a2e6eb27-34c6-43ec-a061-d99f3bec15a5":{"id_":"a2e6eb27-34c6-43ec-a061-d99f3bec15a5","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"OcKonGx39KzXzEfIZxVi23pbE7kCdGo6S4Va0kk3xuA="},"NEXT":{"nodeId":"b4fc19a3-c744-4a1a-9b8e-9285187e3f0c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"TXbSD6LDGjSe6XMFcFCsd2r+3ueokj92Mbts5dMs9Oc="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 9.Overview of the speedup techniques in object detection. to  compress  a  network  by  quantifying  its  activations  or\r\nweights to binary variables (say, 0/1) so that the floating-\r\npoint operation is converted to logical operations. D. Lightweight Network Design\r\nThe  last  group  of  methods  to  speed  up  a  CNN-based\r\ndetector is to directly design lightweight networks. In addi-\r\ntion to some general designing principles, such as “fewer\r\nchannels  and  more  layers”  [131],  some  other  methods\r\nhave been proposed in recent years [132], [133], [134],\r\n[135], [136]. 1)  Factorizing Convolutions:Factorizing convolutions is\r\nthe most straightforward way to build a lightweight CNN\r\nmodel. There are two groups of factorizing methods. The\r\nfirst group is to factorize a large convolution filter into a\r\nset of small ones [50], [87], [137], as shown in Fig.10(b). For  example,  one  can  factorize  a  7×7  filter  into  three\r\n3×3 filters, where they share the same receptive field,\r\nbut  the  latter  one  is  more  efficient. The  second  group\r\nis  to  factorize  convolutions  in  their  channel  dimension\r\n[138], [139], as shown in Fig.10(c). 2)  Group   Convolution:Group   convolution   aims   to\r\nreduce the number of parameters in a convolution layer\r\nby dividing the feature channels into different groups and\r\nthen convolve on each group independently [140], [141],\r\nas shown in Fig.10(d). If we evenly divide the features\r\nintomgroups, without changing other configurations, the\r\ncomputation will be theoretically reduced to 1/mof that\r\nbefore. 3)  Depthwise Separable Convolution:Depthwise separa-\r\nble  convolution  [142],  as  shown  in  Fig.10(e),  can  be\r\nviewed as a special case of the group convolution when the\r\nnumber of groups is set equal to the number of channels.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"SCPmaMb4e1plLkGWNFtmsdlCtuLF2pZrNdsUTAPTrW8="},"b4fc19a3-c744-4a1a-9b8e-9285187e3f0c":{"id_":"b4fc19a3-c744-4a1a-9b8e-9285187e3f0c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"OcKonGx39KzXzEfIZxVi23pbE7kCdGo6S4Va0kk3xuA="},"PREVIOUS":{"nodeId":"a2e6eb27-34c6-43ec-a061-d99f3bec15a5","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"SCPmaMb4e1plLkGWNFtmsdlCtuLF2pZrNdsUTAPTrW8="},"NEXT":{"nodeId":"f40cbc5d-afce-417e-ae07-bd2f318f6c6c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"9VsF6u/2el+beHJqFgpEqwU/G0PzXvI5FnpbwriY/pg="}},"text":"Usually,  a  number  of  1×1  filters  are  used  to  make  a\r\ndimension  transform  so  that  the  final  output  will  have\r\nthe desired number of channels. By using depthwise sep-\r\narable convolution, the computation can be reduced from\r\nO(dk2c)toO(ck2) +O(dc). This  idea  has  been  recently\r\napplied  to  object  detection  and  fine-grain  classification\r\n[143], [144], [145]. 4)  Bottle-Neck  Design:A  bottleneck  layer  in  a  neu-\r\nral  network  contains  few  nodes  compared  to  the  previ-\r\nous  layers. In  recent  years,  the  bottle-neck  design  has\r\nbeen  widely  used  for  designing  lightweight  networks\r\n[50],  [133],  [146],  [147],  [148]. Among  these  meth-\r\nods,  the  input  layer  of  a  detector  can  be  compressed  to\r\nreduce the amount of computation from the very begin-\r\nning  of  the  detection  [133],  [146],  [147]. One  can  also\r\ncompress the feature map to make it thinner so as to speed\r\nup subsequent detection [50], [148]. 5)  Detection  With  NAS:Deep  learning-based  detectors\r\nare becoming increasingly sophisticated, relying heavily on\r\nhandcrafted network architecture and training parameters. Neural  architecture  search  (NAS)  is  primarily  concerned\r\nwith  defining  the  proper  space  of  candidate  networks,\r\nimproving strategies for searching quickly and accurately,\r\nand validating the searching results at a low cost. When\r\ndesigning  a  detection  model,  NAS  can  reduce  the  need\r\nfor human intervention in the design of the network back-\r\nbone and anchor boxes [149], [150], [151], [152], [153],\r\n[154], [155]. E. Numerical Acceleration\r\nNumerical acceleration aims to accelerate object detec-\r\ntors from the bottom of their implementations. 1)  Speedup With Integral Image:The integral image is an\r\nimportant method in image processing.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"TXbSD6LDGjSe6XMFcFCsd2r+3ueokj92Mbts5dMs9Oc="},"f40cbc5d-afce-417e-ae07-bd2f318f6c6c":{"id_":"f40cbc5d-afce-417e-ae07-bd2f318f6c6c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_11","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"OcKonGx39KzXzEfIZxVi23pbE7kCdGo6S4Va0kk3xuA="},"PREVIOUS":{"nodeId":"b4fc19a3-c744-4a1a-9b8e-9285187e3f0c","metadata":{"page_number":11,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"TXbSD6LDGjSe6XMFcFCsd2r+3ueokj92Mbts5dMs9Oc="}},"text":"1)  Speedup With Integral Image:The integral image is an\r\nimportant method in image processing. It helps to rapidly\r\ncalculate summations over image subregions. The essence\r\nof an integral image is the integral-differential separability\r\nof convolution in signal processing\r\nf(x)∗g(x) = (\r\nZ\r\nf(x)dx)∗\r\n\u0012dg(x)\r\ndx\r\n\u0013\r\n(4)\r\nwhere, ifdg(x)/dxis a sparse signal, then the convolution\r\ncan  be  accelerated  by  the  right  part  of  this  equation\r\n[10], [156]. The integral image can also be used to speed\r\nup  more  general  features  in  object  detection,  e.g.,  color\r\nhistogram  and  gradient  histogram  [124],  [157],  [158],\r\n[159]. A typical example is to speed up HOG by computing\r\nintegral  HOG  maps  [124],  [157],  as  shown  in  Fig. 11. Integral HOG map has been used in pedestrian detection\r\nand  has  achieved  dozens  of  times’  acceleration  without\r\nlosing any accuracy [124]. 2)  Speedup  in  Frequency  Domain:Convolution  is  an\r\nimportant type of numerical operation in object detection. As  the  detection  of  a  linear  detector  can  be  viewed  as\r\nthe windowwise inner product between the feature map\r\nand  detector’s  weights,  which  can  be  implemented  by\r\nconvolutions, the Fourier transform is a very practical way\r\nto  speed  up  convolutions,  where  the  theoretical  basis  is\r\nthe convolution theorem in signal processing, i.e., under\r\nsuitable conditions, the Fourier transformFof a convolu-\r\ntion of two signalsI∗Wis the pointwise product in their\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE267Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"9VsF6u/2el+beHJqFgpEqwU/G0PzXvI5FnpbwriY/pg="},"470e690a-1a52-461d-9f97-07013a9a3a96":{"id_":"470e690a-1a52-461d-9f97-07013a9a3a96","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"tI5UPltyKwgBXVnYuLxwbnXYsKegVVixTL+OVFTFz/k="},"NEXT":{"nodeId":"a2c7900b-9fb0-47c8-a157-804e9bd7a0f3","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"c+g1cDwe8hTAyCtJ12OEfrxCiMpD9qRnEyOlRuXwYOM="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 10.Overview of speedup methods of a CNN’s convolutional layer and the comparison of their computational complexity. (a) Standard\r\nconvolution:O(dk2c). (b) Factoring convolutional filters (k×k→(k′×k′)2or 1×k, k×1):O(dk′2c) orO(dkc). (c) Factoring convolutional\r\nchannels:O(d′k2c)+O(dk2d′). (d) Group convolution (#groups=m):O(dk2c/m). (e) Depthwise separable convolution:O(ck2)+O(dc). Fourier space\r\nI∗W=F−1(F(I)⊙F(W))(5)\r\nwhereFis  the  Fourier  transform,F−1is  the  inverse\r\nFourier  transform,  and⊙is  the  pointwise  product. The\r\nabove  calculation  can  be  accelerated  by  using  the  fast\r\nFourier   transform   (FFT)   and   the   inverse   FFT   (IFFT)\r\n[160], [161], [162], [163]. 3)  Vector  Quantization:Vector  quantization  (VQ)  is\r\na   classical   quantization   method   in   signal   processing\r\nthat  aims  to  approximate  the  distribution  of  a  large\r\ngroup   of   data   by   a   small   set   of   prototype   vectors. It   can   be   used   for   data   compression   and   accelerat-\r\ning   the   inner   product   operation   in   object   detection\r\n[164], [165]. IV. R E C E N T  A D V A N C E S  I N  O B J E C T\r\nD E T E C T I O N\r\nThe  continual  appearance  of  new  technologies  over  the\r\npast  two  decades  has  a  considerable  influence  on  object\r\ndetection, while its fundamental principles and underlying\r\nlogic have remained unchanged.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5xW42kIVnQdg74grYbV9io7Z62RyAyu4qjcSxdvuQv4="},"a2c7900b-9fb0-47c8-a157-804e9bd7a0f3":{"id_":"a2c7900b-9fb0-47c8-a157-804e9bd7a0f3","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_12","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"tI5UPltyKwgBXVnYuLxwbnXYsKegVVixTL+OVFTFz/k="},"PREVIOUS":{"nodeId":"470e690a-1a52-461d-9f97-07013a9a3a96","metadata":{"page_number":12,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5xW42kIVnQdg74grYbV9io7Z62RyAyu4qjcSxdvuQv4="}},"text":"In the above sections, we\r\nintroduced the evolution of technology over the past two\r\ndecades in a large-scale time range to help readers compre-\r\nhend object detection; in this section, we will focus more\r\non  state-of-the-art  algorithms  in  recent  years  in  a  short\r\ntime  range  to  help  readers  understand  object  detection. Some  are  expansions  of  previously  discussed  techniques\r\n(e.g., Sections IV-A–IV-E), while others are novel crossovers\r\nthat mix concepts (e.g., Sections IV-F–IV-H). A. Beyond Sliding Window Detection\r\nSince an object in an image can be uniquely determined\r\nby  its  upper  left  corner  and  lower  right  corner  of  the\r\nground-truth  box,  the  detection  task,  therefore,  can  be\r\nequivalently framed as a pairwise key points’ localization\r\nproblem. One  recent  implementation  of  this  idea  is  to\r\npredict a heat map for the corners [26]. Some other meth-\r\nods  follow  the  idea  and  utilize  more  key  points  (corner\r\nand  center  [77],  extreme  and  center  points  [53],  and\r\nrepresentative points [69]) to obtain better performance. Another  paradigm  views  an  object  as  point/points  and\r\nFig. 11.Illustration of how to compute the “Integral HOG Map” [124]. With integral image techniques, we can efficiently compute the\r\nhistogram feature of any location and any size with constant computational complexity. 268PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"c+g1cDwe8hTAyCtJ12OEfrxCiMpD9qRnEyOlRuXwYOM="},"f03ceaa4-211e-4249-84e4-aa0588d3f2ed":{"id_":"f03ceaa4-211e-4249-84e4-aa0588d3f2ed","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EK5ve4RfFo+DOiHvZ2fnCTIPQMBOkTy9jCI17jbs2Co="},"NEXT":{"nodeId":"92256402-5d5e-46e7-9fea-48a0d76ae8c3","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4Ykev6kIZf0flMuRHEQaPdErWY6n7mTAcaJSQsQn2c8="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 12.Different training strategies for multiscale object detection. (a) Training on a single resolution image, back propagate objects of all\r\nscales [17], [18], [19], [23]. (b) Training on multiresolution images (image pyramid), back propagate objects of the selected scale. If an object\r\nis too large or too small, its gradient will be discarded [39], [176], [177]. directly  predicts  the  object’s  attributes  (e.g.,  height  and\r\nwidth) without grouping. The advantage of this approach\r\nis that it can be implemented under a semantic segmenta-\r\ntion framework, and there is no need to design multiscale\r\nanchor boxes. Furthermore, by viewing object detection as\r\na set prediction, DETR [28], [43] completely liberates it in\r\na reference-based framework. B. Robust Detection of Rotation and Scale\r\nChanges\r\nIn recent years, efforts have been made to robust detec-\r\ntion of rotation and scale changes. 1)  Rotation Robust Detection:Object rotation is common\r\nto see in face detection, text detection, and remote sens-\r\ning  object  detection. The  most  straightforward  solution\r\nto this problem is to perform data augmentation so that\r\nan  object  in  any  orientation  can  be  well  covered  by  the\r\naugmented data distribution [166] or to train independent\r\ndetectors  separately  for  each  orientation  [167],  [168]. Designing  rotation  invariant  loss  functions  is  a  recent\r\npopular  solution,  where  a  constraint  on  the  detection\r\nloss is added so that the feature of rotated objects keeps\r\nunchanged [169], [170], [171]. Another recent solution is\r\nto learn geometric transformations of the object candidates\r\n[172],  [173],  [174],  [175]. In  two-stage  detectors,  ROI\r\npooling  aims  to  extract  a  fixed-length  feature  represen-\r\ntation for an object proposal with any location and size. Since  feature  pooling  usually  is  performed  in  Cartesian\r\ncoordinates,  it  is  not  invariant  to  rotation  transform.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"WJITHtm2767wD8qAyY9Cu3xxki5Qeu/vT4oXJY0Uho4="},"92256402-5d5e-46e7-9fea-48a0d76ae8c3":{"id_":"92256402-5d5e-46e7-9fea-48a0d76ae8c3","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EK5ve4RfFo+DOiHvZ2fnCTIPQMBOkTy9jCI17jbs2Co="},"PREVIOUS":{"nodeId":"f03ceaa4-211e-4249-84e4-aa0588d3f2ed","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"WJITHtm2767wD8qAyY9Cu3xxki5Qeu/vT4oXJY0Uho4="},"NEXT":{"nodeId":"26dd8798-45c2-41d5-b369-43ced4d14008","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Tofb4lz3pHadekdr7RgBMWb94/cceziJeLg0FU2m9xg="}},"text":"A\r\nrecent  improvement  is  to  perform  ROI  pooling  in  polar\r\ncoordinates  so  that  the  features  can  be  robust  to  the\r\nrotation changes [167]. 2)  Scale  Robust  Detection:Recent  studies  have  been\r\nmade for scale robust detection at both training and detec-\r\ntion stages. Scale   Adaptive   Training:Modern   detectors   usually\r\nrescale input images to a fixed size and back propagate the\r\nloss of the objects in all scales. A drawback of doing this is\r\nthat there will be a “scale imbalance” problem. Building an\r\nimage pyramid during detection could alleviate this prob-\r\nlem but not fundamentally [49], [178]. A recent improve-\r\nment  is  Scale  Normalization  for  Image  Pyramids  (SNIP)\r\n[176], which builds image pyramids at both training and\r\ndetection stages and only backpropagates the loss of some\r\nselected scales, as shown in Fig.12. Some researchers have\r\nfurther proposed a more efficient training strategy: SNIP\r\nwith Efficient Resampling (SNIPER) [177], i.e., to crop and\r\nrescale an image to a set of subregions so as to benefit from\r\nlarge batch training. Scale  Adaptive  Detection:In  CNN-based  detectors,  the\r\nsize and the aspect ratio of anchors are usually carefully\r\ndesigned. A  drawback  of  doing  this  is  that  the  configu-\r\nrations cannot be adaptive to unexpected scale changes. To  improve  the  detection  of  small  objects,  some  “adap-\r\ntive  zoom-in”  techniques  are  proposed  in  some  recent\r\ndetectors to adaptively enlarge the small objects into the\r\n“larger ones” [179], [180]. Another recent improvement\r\nis to predict the scale distribution of objects in an image\r\nand  then  adaptively  rescale  the  image  according  to  it\r\n[181], [182]. C. Detection With Better Backbones\r\nThe  accuracy/speed  of  a  detector  depends  heavily  on\r\nthe  feature  extraction  networks,  a.k.a.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4Ykev6kIZf0flMuRHEQaPdErWY6n7mTAcaJSQsQn2c8="},"26dd8798-45c2-41d5-b369-43ced4d14008":{"id_":"26dd8798-45c2-41d5-b369-43ced4d14008","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_13","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EK5ve4RfFo+DOiHvZ2fnCTIPQMBOkTy9jCI17jbs2Co="},"PREVIOUS":{"nodeId":"92256402-5d5e-46e7-9fea-48a0d76ae8c3","metadata":{"page_number":13,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4Ykev6kIZf0flMuRHEQaPdErWY6n7mTAcaJSQsQn2c8="}},"text":"backbones,  e.g.,\r\nthe  ResNet  [178],  CSPNet  [183],  Hourglass  [184],  and\r\nSwin Transformer [44]. For a detailed introduction to some\r\nimportant detection backbones in the deep learning era,\r\nwe  refer  readers  to  the  following  surveys  [185]. Fig.13\r\nshows the detection accuracy of three well-known detec-\r\ntion  systems:  Faster  RCNN  [19],  R-FCN  [49],  and  SSD\r\n[23] with different backbones [186]. Object detection has\r\nrecently  benefited  from  the  powerful  feature  extraction\r\ncapabilities  of  Transformers. On  the  COCO  dataset,  the\r\ntop-ten detection methods are all Transformer-based.5The\r\nperformance  gap  between  Transformers  and  CNNs  has\r\ngradually widened. 5https://paperswithcode.com/sota/object-detection-on-coco\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE269Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Tofb4lz3pHadekdr7RgBMWb94/cceziJeLg0FU2m9xg="},"505a5c5f-fa67-438e-b29f-89e8ceaa340c":{"id_":"505a5c5f-fa67-438e-b29f-89e8ceaa340c","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"geW2fqePw4kZVV2QG0E09vTLrORMZVBAwf2E9LliEXU="},"NEXT":{"nodeId":"6d16a8fb-2539-4151-a294-07f788f0728f","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"HeGF3spY6lijrQ2KOjg+t+maCyBg21pAVrHrVGjMzMo="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nFig. 13.Comparison of detection accuracy of three detectors: Faster\r\nRCNN [19], R-FCN [49], and SSD [23] on the MS-COCO dataset with\r\ndifferent detection backbones. Image from CVPR 2017 [186]. D. Improvements of Localization\r\nTo improve localization accuracy, there are two groups\r\nof  methods  in  recent  detectors:  1)  bounding  box  refine-\r\nment and 2) new loss functions for accurate localization. 1)  Bounding Box Refinement:The most intuitive way to\r\nimprove localization accuracy is bounding box refinement,\r\nwhich can be considered as postprocessing of the detec-\r\ntion results. One recent method is to iteratively feed the\r\ndetection results into a BB regressor until the prediction\r\nconverges  to  a  correct  location  and  size  [187],  [188],\r\n[189]. However, some researchers also claimed that this\r\nmethod does not guarantee the monotonicity of localiza-\r\ntion accuracy [187] and may degenerate the localization if\r\nthe refinement is applied multiple times. 2)  New Loss Functions for Accurate Localization:In most\r\nmodern detectors, object localization is considered a coor-\r\ndinate  regression  problem. However,  the  drawbacks  of\r\nthis paradigm are obvious. First, the regression loss does\r\nnot  correspond  to  the  final  evaluation  of  localization,\r\nespecially for some objects with very large aspect ratios. Second,  the  traditional  BB  regression  method  does  not\r\nprovide confidence in localization. When there is multiple\r\nBB’s overlapping with each other, this may lead to failure\r\nin nonmaximum suppression. The above problems can be\r\nalleviated by designing new loss functions. The most intu-\r\nitive improvement is to directly use IoU as the localization\r\nloss [105], [106], [107], [190]. Besides, some researchers\r\nalso  tried  to  improve  localization  under  a  probabilistic\r\ninference  framework  [191]. Different  from  the  previous\r\nmethods  that  directly  predict  the  box  coordinates,  this\r\nmethod predicts the probability distribution of a bounding\r\nbox location. E.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"QSy6bb4b8qlS75xrEuxaRqBjvXZEUYkVVyboAMvVGOg="},"6d16a8fb-2539-4151-a294-07f788f0728f":{"id_":"6d16a8fb-2539-4151-a294-07f788f0728f","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"geW2fqePw4kZVV2QG0E09vTLrORMZVBAwf2E9LliEXU="},"PREVIOUS":{"nodeId":"505a5c5f-fa67-438e-b29f-89e8ceaa340c","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"QSy6bb4b8qlS75xrEuxaRqBjvXZEUYkVVyboAMvVGOg="},"NEXT":{"nodeId":"25eec332-4acc-4f20-b35c-c8a04166a215","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"CFllFGvBio/Yk7UtRB3laabb5fVvVOBzivkdkZksHpw="}},"text":"E. Learning With Segmentation Loss\r\nObject  detection  and  semantic  segmentation  are  two\r\nfundamental tasks in computer vision. Recent studies sug-\r\ngest that object detection can be improved by learning with\r\nsemantic segmentation losses. To  improve  detection  with  segmentation,  the  simplest\r\nway  is  to  think  of  the  segmentation  network  as  a  fixed\r\nfeature extractor and integrate it into a detector as aux-\r\niliary features [83], [192], [193]. The advantage of this\r\napproach  is  that  it  is  easy  to  implement,  while  the  dis-\r\nadvantage  is  that  the  segmentation  network  may  bring\r\nadditional computation. Another way is to introduce an additional segmentation\r\nbranch on top of the original detector and train this model\r\nwith multitask loss functions (seg.+det. ) [4], [42], [192]. The advantage is that the seg. brunch will be removed at\r\nthe  inference  stage,  and  the  detection  speed  will  not  be\r\naffected. However,  the  disadvantage  is  that  the  training\r\nrequires pixel-level image annotations. F. Adversarial Training\r\nThe generative adversarial network (GAN), introduced\r\nby  Goodfellow  et  al. [194]  in  2014,  has  received  great\r\nattention   in   many   tasks,   such   as   image   generation\r\n[194], [195], image style transfer [196], and image super-\r\nresolution [197]. Recently, adversarial training has also been applied to\r\nobject  detection,  especially  for  improving  the  detection\r\nof small and occluded objects. For small object detection,\r\nGAN can be used to enhance the features of small objects\r\nby narrowing the representations between small and large\r\nones [198], [199]. To improve the detection of occluded\r\nobjects,  one  recent  idea  is  to  generate  occlusion  masks\r\nby using adversarial training [200]. Instead of generating\r\nexamples in pixel space, the adversarial network directly\r\nmodifies the features to mimic occlusion. G.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"HeGF3spY6lijrQ2KOjg+t+maCyBg21pAVrHrVGjMzMo="},"25eec332-4acc-4f20-b35c-c8a04166a215":{"id_":"25eec332-4acc-4f20-b35c-c8a04166a215","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_14","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"geW2fqePw4kZVV2QG0E09vTLrORMZVBAwf2E9LliEXU="},"PREVIOUS":{"nodeId":"6d16a8fb-2539-4151-a294-07f788f0728f","metadata":{"page_number":14,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"HeGF3spY6lijrQ2KOjg+t+maCyBg21pAVrHrVGjMzMo="}},"text":"G. Weakly Supervised Object Detection\r\nTraining  a  deep  learning-based  object  detector  usu-\r\nally  requires  a  large  amount  of  manually  labeled  data. Weakly supervised object detection (WSOD) aims at easing\r\nthe  reliance  on  data  annotation  by  training  a  detector\r\nwith  only  image-level  annotations  instead  of  bounding\r\nboxes [201]. Multi-instance learning is a group of supervised learning\r\nalgorithms that has seen widespread application in WSOD\r\n[202],  [203],  [204],  [205],  [206],  [207],  [208],  [209]. Instead of learning with a set of instances that are indi-\r\nvidually labeled, a multi-instance learning model receives\r\na set of labeled bags, each containing many instances. If\r\nwe consider object candidates in an image as a bag and\r\nimage-level annotation as the label, then the WSOD can be\r\nformulated as a multi-instance learning process. Class  activation  mapping  is  another  recent  group  of\r\nmethods  for  WSOD  [210],  [211]. The  research  on  CNN\r\nvisualization has shown that the convolutional layer of a\r\nCNN  behaves  as  an  object  detector,  despite  there  is  no\r\nsupervision  on  the  location  of  the  object. Class  activa-\r\ntion  mapping  shed  light  on  how  to  enable  a  CNN  with\r\nlocalization capability despite being trained on image-level\r\nlabels [212]. 270PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CFllFGvBio/Yk7UtRB3laabb5fVvVOBzivkdkZksHpw="},"00f8d90f-4f07-46b4-974e-32235bb127f5":{"id_":"00f8d90f-4f07-46b4-974e-32235bb127f5","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"UDs8avZKn54L26uDYfoArJu6rGy+pe6x8ClrKDWy420="},"NEXT":{"nodeId":"d2eec987-8365-411c-9e2b-1c60bb470930","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"wkp/jLB5rlA2Qpa4LdX4O7JsEjRcPPDtThMhYsAInKA="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nIn   addition   to   the   above   approaches,   some   other\r\nresearchers  considered  the  WSOD  as  a  proposal  ranking\r\nprocess by selecting the most informative regions and then\r\ntraining these regions with image-level annotation [213]. Some  other  researchers  proposed  to  mask  out  different\r\nparts  of  the  image. If  the  detection  score  drops  sharply,\r\nthen the masked region may contain an object with high\r\nprobability  [214]. More  recently,  generative  adversarial\r\ntraining has also been used for WSOD [215]. H. Detection With Domain Adaptation\r\nThe  training  process  of  most  object  detectors  can  be\r\nessentially viewed as a likelihood estimation process under\r\nthe  assumption  of  independent  identically  distributed\r\n(i.i.d. )  data. Object  detection  with  non-i.i.d. data,  espe-\r\ncially  for  some  real-world  applications,  still  remains  a\r\nchallenge. Aside  from  collecting  more  data  or  apply-\r\ning proper data augmentation, domain adaptation offers\r\nthe  possibility  of  narrowing  the  gap  between  domains. To  obtain  domain-invariant  feature  representation,  fea-\r\nture regularization and adversarial training-based methods\r\nhave been explored at the image, category, or object levels\r\n[216], [217], [218], [219], [220], [221]. Cycle-consistent\r\ntransformation [222] has also been applied to bridge the\r\ngap  between  source  and  target  domains  [223],  [224]. Some other methods also incorporate both ideas [225] to\r\nacquire better performance. V. C O N C L U S I O N  A N D  F U T U R E\r\nD I R E C T I O N S\r\nRemarkable   achievements   have   been   made   in   object\r\ndetection  over  the  past  20  years. This  article  exten-\r\nsively reviews some milestone detectors, key technologies,\r\nspeedup  methods,  datasets,  and  metrics  in  its  20  years\r\nof history.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"TD2TjHk9G40DcmqMWTn2m3n7F3lhR7UWZOZVVAMquvk="},"d2eec987-8365-411c-9e2b-1c60bb470930":{"id_":"d2eec987-8365-411c-9e2b-1c60bb470930","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"UDs8avZKn54L26uDYfoArJu6rGy+pe6x8ClrKDWy420="},"PREVIOUS":{"nodeId":"00f8d90f-4f07-46b4-974e-32235bb127f5","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"TD2TjHk9G40DcmqMWTn2m3n7F3lhR7UWZOZVVAMquvk="},"NEXT":{"nodeId":"76fa1f59-4a15-42dd-bb89-9daaab49b2a3","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"dwP0N7iod4TgN78tP2FnHsa6sWwYDha38nPs1yaKPOQ="}},"text":"Some promising future directions may include,\r\nbut  are  not  limited  to,  the  following  aspects  to  help\r\nreaders get more insights beyond the scheme mentioned\r\nabove. Lightweight  Object  Detection:It  aims  to  speed  up  the\r\ndetection  inference  to  run  on  low-power  edge  devices. Some  important  applications  include  mobile  augmented\r\nreality, automatic driving, smart city, smart cameras, face\r\nverification, and so on. Although a great effort has been\r\nmade in recent years, the speed gap between a machine\r\nand human eyes still remains large, especially for detecting\r\nsome small objects or detecting with multisource informa-\r\ntion [226], [227]. End-to-End  Object  Detection:Although  some  methods\r\nhave  been  developed  to  detect  objects  in  a  fully  end-\r\nto-end  manner  (image  to  box  in  a  network)  using  one-\r\nto-one label assignment training, the majority still use a\r\none-to-many label assignment method where the nonmax-\r\nimum suppression operation is separately designed. Future\r\nresearch on this topic may focus on designing end-to-end\r\npipelines that maintain both high detection accuracy and\r\nefficiency [228]. Small Object Detection:Detecting small objects in large\r\nscenes  has  long  been  a  challenge. Some  potential  appli-\r\ncation  of  this  research  direction  includes  counting  the\r\npopulation of people in crowd or animals in the open air\r\nand detecting military targets from satellite images. Some\r\nfurther directions may include the integration of the visual\r\nattention  mechanisms  and  the  design  of  high-resolution\r\nlightweight networks [229], [230]. 3-D  Object  Detection:Despite  recent  advances  in  2-D\r\nobject detection, applications such as autonomous driving\r\nrely on access to the objects’ location and pose in a 3-D\r\nworld.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"wkp/jLB5rlA2Qpa4LdX4O7JsEjRcPPDtThMhYsAInKA="},"76fa1f59-4a15-42dd-bb89-9daaab49b2a3":{"id_":"76fa1f59-4a15-42dd-bb89-9daaab49b2a3","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"UDs8avZKn54L26uDYfoArJu6rGy+pe6x8ClrKDWy420="},"PREVIOUS":{"nodeId":"d2eec987-8365-411c-9e2b-1c60bb470930","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"wkp/jLB5rlA2Qpa4LdX4O7JsEjRcPPDtThMhYsAInKA="},"NEXT":{"nodeId":"e33b8b3e-3f3e-422d-9f71-79291b43a698","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/6+KgeXtvk+hY84j0uoepZ+SMBiKzlQMT2+3QSOIyLQ="}},"text":"The  future  of  object  detection  will  receive  more\r\nattention  in  the  3-D  world  and  the  utilization  of  mul-\r\ntisource  and  multiview  data  (e.g.,  RGB  images  and  3-D\r\nLiDAR points from multiple sensors) [231], [232]. Detection in Videos:Real-time object detection/tracking\r\nin HD videos is of great importance for video surveillance\r\nand autonomous driving. Traditional object detectors are\r\nusually  designed  for  imagewise  detection  while  simply\r\nignoring the correlations between video frames. Improving\r\ndetection by exploring the spatial and temporal correlation\r\nunder the calculation limitation is an important research\r\ndirection [233], [234]. Cross-Modality Detection:Object detection with multiple\r\nsources/modalities  of  data,  e.g.,  RGB-D  image,  LiDAR,\r\nflow,  sound,  text,  and  video,  is  of  great  importance  for\r\na  more  accurate  detection  system,  which  performs  like\r\nhuman-being’s  perception. Some  open  questions  include\r\nhow   to   immigrate   well-trained   detectors   to   different\r\nmodalities  of  data,  how  to  make  information  fusion  to\r\nimprove detection, and so on [235], [236]. Toward  Open-World  Detection:Out-of-domain  general-\r\nization,  zero-shot  detection,  and  incremental  detection\r\nare  emerging  topics  in  object  detection. The  majority\r\nof  them  devised  ways  to  reduce  catastrophic  forgetting\r\nor  utilized  supplemental  information. Humans  have  the\r\ninstinct to discover objects of unknown categories in the\r\nenvironment. When the corresponding knowledge (label)\r\nis  given,  humans  will  learn  new  knowledge  from  it  and\r\nget  to  keep  the  patterns. However,  it  is  difficult  for  cur-\r\nrent  object  detection  algorithms  to  grasp  the  detection\r\nability  of  unknown  classes  of  objects.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"dwP0N7iod4TgN78tP2FnHsa6sWwYDha38nPs1yaKPOQ="},"e33b8b3e-3f3e-422d-9f71-79291b43a698":{"id_":"e33b8b3e-3f3e-422d-9f71-79291b43a698","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_15","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"UDs8avZKn54L26uDYfoArJu6rGy+pe6x8ClrKDWy420="},"PREVIOUS":{"nodeId":"76fa1f59-4a15-42dd-bb89-9daaab49b2a3","metadata":{"page_number":15,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"dwP0N7iod4TgN78tP2FnHsa6sWwYDha38nPs1yaKPOQ="}},"text":"Object  detection\r\nin  the  open  world  aims  at  discovering  unknown  cate-\r\ngories of objects when supervision signals are not explic-\r\nitly  given  or  partially  given,  which  holds  great  promise\r\nin applications such as robotics and autonomous driving\r\n[237], [238]. Standing  on  the  highway  of  technical  evolutions,  we\r\nbelieve that this article will help readers to build a com-\r\nplete road map of object detection and find future direc-\r\ntions of this fast-moving research field.■\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE271Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"/6+KgeXtvk+hY84j0uoepZ+SMBiKzlQMT2+3QSOIyLQ="},"e4f14fbe-7550-4caa-ada7-cb1e5a19129f":{"id_":"e4f14fbe-7550-4caa-ada7-cb1e5a19129f","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"NEXT":{"nodeId":"973b7c26-46ea-48f9-979f-dc5fd3aad38d","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"pEd5R04DTeT1w7g0ZJoj+4wXaGuTWuQevZUJ5lg5Mxo="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nR E F E R E N C E S\r\n[1]   B. Hariharan, P. Arbeláez, R. Girshick, andJ. Malik, “Simultaneous detection and\r\nsegmentation,” inProc. ECCV. Cham, Switzerland:Springer, 2014, pp. 297–312. [2]   B. Hariharan, P. Arbelaez, R. Girshick, andJ. Malik, “Hypercolumns for object segmentation\r\nand fine-grained localization,” inProc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015,\r\npp. 447–456.[3]   J. Dai, K. He, and J. Sun, “Instance-aware\r\nsemantic segmentation via multi-task networkcascades,” inProc. IEEE Conf. Comput. Vis. Pattern\r\nRecognit. (CVPR), Jun. 2016, pp. 3150–3158.[4]   K. He, G. Gkioxari, P. Dollár, and R. Girshick,\r\n“Mask R-CNN,” inProc. ICCV, Oct. 2017,pp. 2980–2988. [5]   A. Karpathy and L. Fei-Fei, “Deep visual-semanticalignments for generating image descriptions,” in\r\nProc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2015, pp. 3128–3137. [6]   K. Xu et al., “Show, attend and tell: Neural imagecaption generation with visual attention,” inProc. ICML, 2015, pp. 2048–2057.[7]   Q. Wu, C. Shen, P. Wang, A. Dick, and\r\nA. van den Hengel, “Image captioning and visualquestion answering based on attributes and\r\nexternal knowledge,”IEEE Trans. Pattern Anal.Mach. Intell., vol. 40, no.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"AJgVysdyKRdMFqnO0FgdRHYYewYiC4Jq7/3lpFZEDfw="},"973b7c26-46ea-48f9-979f-dc5fd3aad38d":{"id_":"973b7c26-46ea-48f9-979f-dc5fd3aad38d","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"e4f14fbe-7550-4caa-ada7-cb1e5a19129f","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"AJgVysdyKRdMFqnO0FgdRHYYewYiC4Jq7/3lpFZEDfw="},"NEXT":{"nodeId":"080620ef-5b87-4155-90e7-c42257441a53","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Lk180jga+cPO04gUTqWCG1/dxwtab2Moc+yBJGllt38="}},"text":"Pattern Anal.Mach. Intell., vol. 40, no. 6, pp. 1367–1381,\r\nJun. 2018.[8]   K. Kang et al., “T-CNN: Tubelets with\r\nconvolutional neural networks for object detectionfrom videos,”IEEE Trans. Circuits Syst. Video\r\nTechnol., vol. 28, no. 10, pp. 2896–2907,Oct. 2018. [9]   Y. LeCun, Y. Bengio, and G. Hinton, “Deeplearning,”Nature, vol. 521, no. 7553, p. 436,\r\nFeb. 2015.[10]   P. Viola and M. Jones, “Rapid object detection\r\nusing a boosted cascade of simple features,” inProc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern\r\nRecognit. (CVPR), Dec. 2001, pp. 1–9.[11]   P. Viola and M. J. Jones, “Robust real-time face\r\ndetection,”Int. J. Comput. Vis., vol. 57, no. 2,pp. 137–154, 2004. [12]   N. Dalal and B. Triggs, “Histograms of orientedgradients for human detection,” inProc. IEEE\r\nComput. Soc. Conf. Comput. Vis. Pattern Recognit.,vol. 1, no. 1, Jun. 2005, pp. 886–893. [13]   P. Felzenszwalb, D. McAllester, and D. Ramanan,“A discriminatively trained, multiscale, deformable\r\npart model,” inProc. IEEE Conf. Comput. Vis.Pattern Recognit., Jun. 2008, pp. 1–8. [14]   P. F. Felzenszwalb, R. B. Girshick, andD. McAllester, “Cascade object detection with\r\ndeformable part models,” inProc. IEEE Comput.Soc. Conf. Comput. Vis.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"pEd5R04DTeT1w7g0ZJoj+4wXaGuTWuQevZUJ5lg5Mxo="},"080620ef-5b87-4155-90e7-c42257441a53":{"id_":"080620ef-5b87-4155-90e7-c42257441a53","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"973b7c26-46ea-48f9-979f-dc5fd3aad38d","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"pEd5R04DTeT1w7g0ZJoj+4wXaGuTWuQevZUJ5lg5Mxo="},"NEXT":{"nodeId":"5af47763-0784-4255-9ece-f97d7eb19810","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FzOh8wkdkt0shYhNSrcN1Z5REK8XO8n/Ak1VXZDkEgk="}},"text":"IEEE Comput.Soc. Conf. Comput. Vis. Pattern Recognit.,\r\nJun. 2010, pp. 2241–2248.[15]   P. F. Felzenszwalb, R. B. Girshick, D. McAllester,\r\nand D. Ramanan, “Object detection withdiscriminatively trained part-based models,”IEEE\r\nTrans. Pattern Anal. Mach. Intell., vol. 32, no. 9,pp. 1627–1645, Sep. 2010. [16]   R. Girshick, J. Donahue, T. Darrell, and J. Malik,“Rich feature hierarchies for accurate object\r\ndetection and semantic segmentation,” inProc.IEEE Conf. Comput. Vis. Pattern Recognit.,\r\nJun. 2014, pp. 580–587.[17]   K. He, X. Zhang, S. Ren, and J. Sun, “Spatial\r\npyramid pooling in deep convolutional networksfor visual recognition,” inProc. ECCV. Cham,\r\nSwitzerland: Springer, 2014, pp. 346–361.[18]   R. Girshick, “Fast R-CNN,” inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1440–1448.[19]   S. Ren, K. He, R. Girshick, and J. Sun, “Faster\r\nR-CNN: Towards real-time object detection withregion proposal networks,” inProc. Adv. Neural\r\nInf. Process. Syst., 2015, pp. 91–99.[20]   J. Redmon, S. Divvala, R. Girshick, and A. Farhadi,\r\n“You only look once: Unified, real-time objectdetection,” inProc. IEEE Conf. Comput. Vis. Pattern\r\nRecognit. (CVPR), Jun. 2016, pp. 779–788.[21]   J. Redmon and A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"Lk180jga+cPO04gUTqWCG1/dxwtab2Moc+yBJGllt38="},"5af47763-0784-4255-9ece-f97d7eb19810":{"id_":"5af47763-0784-4255-9ece-f97d7eb19810","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"080620ef-5b87-4155-90e7-c42257441a53","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Lk180jga+cPO04gUTqWCG1/dxwtab2Moc+yBJGllt38="},"NEXT":{"nodeId":"de666b47-fa0d-4527-9b30-80a19aba56fb","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"CuVR9w5UtyEzJt++gwsoiToxje619xgG7J/+R3VYbBE="}},"text":"779–788.[21]   J. Redmon and A. Farhadi, “YOLOv3: An\r\nincremental improvement,” 2018,arXiv:1804.02767. [22]   A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao,“YOLOv4: Optimal speed and accuracy of object\r\ndetection,” 2020,arXiv:2004.10934.[23]   W. Liu et al., “SSD: Single shot multibox detector,”\r\ninProc. ECCV. Cham, Switzerland: Springer, 2016,pp. 21–37. [24]   T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan,and S. Belongie, “Feature pyramid networks for\r\nobject detection,” inProc. IEEE Conf. Comput. Vis.Pattern Recognit. (CVPR), Jul. 2017,\r\npp. 2117–2125.[25]   T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar,\r\n“Focal loss for dense object detection,”IEEE Trans.Pattern Anal. Mach. Intell., vol. 42, no. 2,\r\npp. 318–327, Feb. 2020.[26]   H. Law and J. Deng, “CornerNet: Detecting\r\nobjects as paired keypoints,” inProc. Eur. Conf.Comput. Vis. (ECCV), Sep. 2018, pp. 734–750. [27]   Z.-Q. Zhao, P. Zheng, S.-T. Xu, and X. Wu, “Objectdetection with deep learning: A review,”IEEE\r\nTrans. Pattern Anal. Mach. Intell., vol. 30, no. 11,pp. 3212–3232, Nov. 2019. [28]   N. Carion, F. Massa, G. Synnaeve, N. Usunier,A.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FzOh8wkdkt0shYhNSrcN1Z5REK8XO8n/Ak1VXZDkEgk="},"de666b47-fa0d-4527-9b30-80a19aba56fb":{"id_":"de666b47-fa0d-4527-9b30-80a19aba56fb","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"5af47763-0784-4255-9ece-f97d7eb19810","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FzOh8wkdkt0shYhNSrcN1Z5REK8XO8n/Ak1VXZDkEgk="},"NEXT":{"nodeId":"2eca8486-04ae-4fe1-ae96-0aa947512e96","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"qXZKLN/34tPkyXnki+XVfqpYQnFs4r08vzr0VIXGUzA="}},"text":"Massa, G. Synnaeve, N. Usunier,A. Kirillov, and S. Zagoruyko, “End-to-end object\r\ndetection with transformers,” inProc. Eur. Conf.Comput. Vis.Cham, Switzerland: Springer, 2020,\r\npp. 213–229.[29]   D. G. Lowe, “Object recognition from local\r\nscale-invariant features,” inProc. IEEE Int. Conf.Comput. Vis., vol. 2, Sep. 1999, pp. 1150–1157. [30]   D. G. Lowe, “Distinctive image features fromscale-invariant keypoints,”Int. J. Comput. Vis.,\r\nvol. 60, pp. 91–110, Dec. 2004.[31]   S. Belongie, J. Malik, and J. Puzicha, “Shape\r\nmatching and object recognition using shapecontexts,”IEEE Trans. Pattern Anal. Mach. Intell.,\r\nvol. 24, no. 4, pp. 509–522, Apr. 2002.[32]   T. Malisiewicz, A. Gupta, and A. A. Efros,\r\n“Ensemble of exemplar-SVMs for object detectionand beyond,” inProc. Int. Conf. Comput. Vis.,\r\nNov. 2011, pp. 89–96.[33]   R. B. Girshick, P. F. Felzenszwalb, and\r\nD. A. Mcallester, “Object detection with grammarmodels,” inProc. Adv. Neural Inf. Process. Syst.,\r\n2011, pp. 442–450.[34]   R. B. Girshick,From Rigid Templates to Grammars:\r\nObject Detection With Structured Models.Princeton, NJ, USA: Citeseer, 2012. [35]   A. Krizhevsky, I. Sutskever, and G. E. Hinton,“ImageNet classification with deep convolutional\r\nneural networks,” inProc. Adv. Neural Inf.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"CuVR9w5UtyEzJt++gwsoiToxje619xgG7J/+R3VYbBE="},"2eca8486-04ae-4fe1-ae96-0aa947512e96":{"id_":"2eca8486-04ae-4fe1-ae96-0aa947512e96","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"de666b47-fa0d-4527-9b30-80a19aba56fb","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"CuVR9w5UtyEzJt++gwsoiToxje619xgG7J/+R3VYbBE="},"NEXT":{"nodeId":"70d4b572-d6c1-4a49-a5ef-9c1066e1c1f2","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FNzXxKazrLHlcbmGZh7OHERKjOGvo8uFcoOsGpXchV8="}},"text":"Adv. Neural Inf. Process.Syst., 2012, pp. 1097–1105. [36]   R. Girshick, J. Donahue, T. Darrell, and J. Malik,“Region-based convolutional networks for\r\naccurate object detection and segmentation,”IEEETrans. Pattern Anal. Mach. Intell., vol. 38, no. 1,\r\npp. 142–158, Jan. 2016.[37]   M. A. Sadeghi and D. Forsyth, “30Hz object\r\ndetection with DPM V5,” inProc. ECCV. Cham,Switzerland: Springer, 2014, pp. 65–79. [38]   S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li,“Single-shot refinement neural network for object\r\ndetection,” inProc. IEEE/CVF Conf. Comput. Vis.Pattern Recognit., Jun. 2018, pp. 4203–4212. [39]   Y. Li, Y. Chen, N. Wang, and Z. Zhang,“Scale-aware trident networks for object\r\ndetection,” 2019,arXiv:1901.01892.[40]   X. Zhou, D. Wang, and P. Krähenbühl, “Objects as\r\npoints,” 2019,arXiv:1904.07850.[41]   Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully\r\nconvolutional one-stage object detection,” inProc.IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\r\nOct. 2019, pp. 9627–9636.[42]   K. Chen et al., “Hybrid task cascade for instance\r\nsegmentation,” inProc. IEEE/CVF Conf. Comput.Vis. Pattern Recognit. (CVPR), Jun. 2019,\r\npp. 4974–4983.[43]   X. Zhu, W. Su, L.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"qXZKLN/34tPkyXnki+XVfqpYQnFs4r08vzr0VIXGUzA="},"70d4b572-d6c1-4a49-a5ef-9c1066e1c1f2":{"id_":"70d4b572-d6c1-4a49-a5ef-9c1066e1c1f2","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"2eca8486-04ae-4fe1-ae96-0aa947512e96","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"qXZKLN/34tPkyXnki+XVfqpYQnFs4r08vzr0VIXGUzA="},"NEXT":{"nodeId":"efe6b93d-1cb2-40cb-9d75-75d74321822b","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"ONWdfzCR3xZj7+B2sEe+gbFTiX0eUVevMIqPlyDxbmk="}},"text":"Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai,\r\n“Deformable DETR: Deformable transformers forend-to-end object detection,” 2020,\r\narXiv:2010.04159.[44]   Z. Liu et al., “Swin transformer: Hierarchical\r\nvision transformer using shifted windows,” 2021,arXiv:2103.14030. [45]   J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers,and A. W. M. Smeulders, “Selective search for\r\nobject recognition,”Int. J. Comput. Vis., vol. 104,no. 2, pp. 154–171, Apr. 2013. [46]   R. B. Girshick, P. F. Felzenszwalb, andD. McAllester.Discriminatively Trained Deformable\r\nPart Models, Release 5. Accessed: Jan. 25, 2023.[Online]. Available: https://github.com/\r\nrbgirshick/voc-dpm[47]   S. Ren, K. He, R. Girshick, and J. Sun, “Faster\r\nR-CNN: Towards real-time object detection withregion proposal networks,”IEEE Trans. Pattern\r\nAnal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149,Jun. 2017. [48]   M. D. Zeiler and R. Fergus, “Visualizing andunderstanding convolutional networks,” inProc. ECCV. Cham, Switzerland: Springer, 2014,pp. 818–833. [49]   J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: Objectdetection via region-based fully convolutional\r\nnetworks,” inProc. Adv. Neural Inf. Process. Syst.,2016, pp. 379–387. [50]   Z. Li, C.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FNzXxKazrLHlcbmGZh7OHERKjOGvo8uFcoOsGpXchV8="},"efe6b93d-1cb2-40cb-9d75-75d74321822b":{"id_":"efe6b93d-1cb2-40cb-9d75-75d74321822b","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"70d4b572-d6c1-4a49-a5ef-9c1066e1c1f2","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FNzXxKazrLHlcbmGZh7OHERKjOGvo8uFcoOsGpXchV8="},"NEXT":{"nodeId":"86a0a5fa-339d-4a47-8d7b-4359a535b440","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"KWZqCZxBzhxZ9GO8WP2P0QsS/f1uOWyT//DipRHUCas="}},"text":"379–387. [50]   Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, andJ. Sun, “Light-head R-CNN: In defense of\r\ntwo-stage object detector,” 2017,arXiv:1711.07264. [51]   J. Redmon and A. Farhadi, “YOLO9000: Better,faster, stronger,” 2016,arXiv:1612.08242. [52]   C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao,“YOLOv7: Trainable bag-of-freebies sets new\r\nstate-of-the-art for real-time object detectors,”2022,arXiv:2207.02696. [53]   X. Zhou, J. Zhuo, and P. Krahenbuhl, “Bottom-upobject detection by grouping extreme and center\r\npoints,” inProc. IEEE/CVF Conf. Comput. Vis.Pattern Recognit. (CVPR), Jun. 2019, pp. 850–859. [54]   M. Everingham, L. Van Gool, C. K. I. Williams,J. Winn, and A. Zisserman, “The PASCAL visual\r\nobject classes (VOC) challenge,”Int. J. Comput.Vis., vol. 88, no. 2, pp. 303–338, Jun. 2010. [55]   M. Everingham, S. M. A. Eslami, L. Van Gool,C. K. I. Williams, J. Winn, and A. Zisserman, “The\r\nPASCAL visual object classes challenge:A retrospective,”Int. J. Comput. Vis., vol. 111,\r\nno. 1, pp. 98–136, Jan. 2014.[56]   O. Russakovsky et al., “ImageNet large scale visual\r\nrecognition challenge,”Int. J. Comput. Vis.,vol. 115, no. 3, pp. 211–252, Dec.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"ONWdfzCR3xZj7+B2sEe+gbFTiX0eUVevMIqPlyDxbmk="},"86a0a5fa-339d-4a47-8d7b-4359a535b440":{"id_":"86a0a5fa-339d-4a47-8d7b-4359a535b440","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_16","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"/JMH4vVeylqhuMuE8+ha5PhTNHtYHD+GO962LjUurco="},"PREVIOUS":{"nodeId":"efe6b93d-1cb2-40cb-9d75-75d74321822b","metadata":{"page_number":16,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"ONWdfzCR3xZj7+B2sEe+gbFTiX0eUVevMIqPlyDxbmk="}},"text":"115, no. 3, pp. 211–252, Dec. 2015. [57]   T.-Y. Lin et al., “Microsoft coco: Common objectsin context,” inProc. ECCV. Cham, Switzerland:\r\nSpringer, 2014, pp. 740–755.[58]   A. Kuznetsova et al., “The open images dataset\r\nV4: Unified image classification, object detection,and visual relationship detection at scale,”Int. J. Comput. Vis., vol. 128, pp. 1956–1981, Mar. 2020.[59]   R. Benenson, S. Popov, and V. Ferrari, “Large-scale\r\ninteractive object segmentation with humanannotators,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,pp. 11700–11709. [60]   S. Shao et al., “Objects365: A large-scale,high-quality dataset for object detection,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),Oct. 2019, pp. 8430–8439. [61]   J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, andL. Fei-Fei, “ImageNet: A large-scale hierarchical\r\nimage database,” inProc. IEEE Conf. Comput. Vis.Pattern Recognit., Jun. 2009, pp. 248–255. [62]   I. Krasin and T. E. A. Duerig. (2017).OpenImages:A Public Dataset for Large-Scale Multi-Label and\r\nMulti-Class Image Classification. Dataset. [Online].Available: https://storage.googleapis.com/\r\n272PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"KWZqCZxBzhxZ9GO8WP2P0QsS/f1uOWyT//DipRHUCas="},"6d03c586-046e-4d91-a523-aa72af72c693":{"id_":"6d03c586-046e-4d91-a523-aa72af72c693","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"NEXT":{"nodeId":"59a23678-0295-4104-8115-0ab558dbf9bd","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5oAYMYTAlNJ6rK+1kipbuVUB1pMbYYwUsDHdw0EVgqE="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nopenimages/web/index.html[63]   P. Dollar, C. Wojek, B. Schiele, and P. Perona,\r\n“Pedestrian detection: A benchmark,” inProc. IEEEConf. Comput. Vis. Pattern Recognit., Jun. 2009,\r\npp. 304–311.[64]   P. Dollár, C. Wojek, B. Schiele, and P. Perona,\r\n“Pedestrian detection: An evaluation of the stateof the art,”IEEE Trans. Pattern Anal. Mach. Intell.,\r\nvol. 34, no. 4, pp. 743–761, Apr. 2011.[65]   P. Sermanet, D. Eigen, X. Zhang, M. Mathieu,\r\nR. Fergus, and Y. LeCun, “OverFeat: Integratedrecognition, localization and detection using\r\nconvolutional networks,” 2013,arXiv:1312.6229.[66]   C. Szegedy, A. Toshev, and D. Erhan, “Deep neural\r\nnetworks for object detection,” inProc. Adv.Neural Inf. Process. Syst., 2013, pp. 2553–2561. [67]   Z. Cai, Q. Fan, R. S. Feris, and N. Vasconcelos,“A unified multi-scale deep convolutional neural\r\nnetwork for fast object detection,” inProc. ECCV.Cham, Switzerland: Springer, 2016, pp. 354–370. [68]   Z. Cai and N. Vasconcelos, “Cascade R-CNN:Delving into high quality object detection,” in\r\nProc. IEEE/CVF Conf. Comput. Vis. PatternRecognit., Jun. 2018, pp. 6154–6162. [69]   Z. Yang, S. Liu, H. Hu, L. Wang, and S. Lin,“RepPoints: Point set representation for object\r\ndetection,” inProc.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"sSXFtNmBqYzAdvkoChDA3vsfyOO4buuQFrLDFfyhVV4="},"59a23678-0295-4104-8115-0ab558dbf9bd":{"id_":"59a23678-0295-4104-8115-0ab558dbf9bd","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"6d03c586-046e-4d91-a523-aa72af72c693","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"sSXFtNmBqYzAdvkoChDA3vsfyOO4buuQFrLDFfyhVV4="},"NEXT":{"nodeId":"698661b8-c7b2-4a18-9e57-3483b9132819","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4HGsIMjEXHExYkWSL7ze7Gwtnx0ujSArimozxM/V/Ws="}},"text":"Lin,“RepPoints: Point set representation for object\r\ndetection,” inProc. IEEE/CVF Int. Conf. Comput.Vis. (ICCV), Oct. 2019, pp. 9657–9666. [70]   T. Malisiewicz,Exemplar-Based Representations forObject Detection, Association and Beyond. Pittsburgh, PA, USA: Carnegie Mellon Univ., 2011.[71]   J. Hosang, R. Benenson, P. Dollár, and B. Schiele,\r\n“What makes for effective detection proposals? ” inProc. IEEE Trans. Pattern Anal. Mach. Intell.,\r\nvol. 38, no. 4, pp. 814–830, Sep. 2016.[72]   J. Hosang, R. Benenson, and B. Schiele, “How\r\ngood are detection proposals, really? ” 2014,arXiv:1406.6962. [73]   B. Alexe, T. Deselaers, and V. Ferrari, “What is anobject,” inProc. CVPR, Jun. 2010, pp. 73–80. [74]   B. Alexe, T. Deselaers, and V. Ferrari, “Measuringthe objectness of image windows,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11,pp. 2189–2202, Nov. 2012. [75]   M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. Torr,“BING: Binarized normed gradients for objectness\r\nestimation at 300fps,” inProc. IEEE Conf. Comput.Vis. Pattern Recognit., Jun. 2014, pp. 3286–3293. [76]   D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov,“Scalable object detection using deep neural\r\nnetworks,” inProc. IEEE Conf. Comput. Vis. PatternRecognit., Jun.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5oAYMYTAlNJ6rK+1kipbuVUB1pMbYYwUsDHdw0EVgqE="},"698661b8-c7b2-4a18-9e57-3483b9132819":{"id_":"698661b8-c7b2-4a18-9e57-3483b9132819","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"59a23678-0295-4104-8115-0ab558dbf9bd","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5oAYMYTAlNJ6rK+1kipbuVUB1pMbYYwUsDHdw0EVgqE="},"NEXT":{"nodeId":"80714139-0dd0-4a14-b653-10fbe1ce1a53","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5QgTbOxk4z8SAqcXecwaxE15Wnk492PFMo6LSrPb8no="}},"text":"IEEE Conf. Comput. Vis. PatternRecognit., Jun. 2014, pp. 2147–2154. [77]   K. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, andQ. Tian, “CenterNet: Keypoint triplets for object\r\ndetection,” inProc. IEEE/CVF Int. Conf. Comput.Vis. (ICCV), Oct. 2019, pp. 6569–6578. [78]   A. Torralba and P. Sinha, “Detecting faces inimpoverished images,” Massachusetts Inst. Tech. Cambridge Artif. Intell. Lab, Cambridge, MA, USA,Tech. Rep. AIM-2001-028, 2001. [79]   S. Zagoruyko et al., “A MultiPath network forobject detection,” 2016,arXiv:1604.02135. [80]   X. Zeng, W. Ouyang, B. Yang, J. Yan, and X. Wang,“Gated bi-directional CNN for object detection,” in\r\nProc. ECCV. Cham, Switzerland: Springer, 2016,pp. 354–369. [81]   X. Zeng et al., “Crafting GBD-Net for objectdetection,”IEEE Trans. Pattern Anal. Mach. Intell.,\r\nvol. 40, no. 9, pp. 2109–2123, Sep. 2018.[82]   W. Ouyang, K. Wang, X. Zhu, and X. Wang,\r\n“Learning chained deep features and classifiers forcascade in object detection,” 2017,\r\narXiv:1702.07054.[83]   S. Gidaris and N. Komodakis, “Object detection via\r\na multi-region and semantic segmentation-awareCNN model,” inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Dec. 2015, pp. 1134–1142.[84]   Y. Zhu, C. Zhao, J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4HGsIMjEXHExYkWSL7ze7Gwtnx0ujSArimozxM/V/Ws="},"80714139-0dd0-4a14-b653-10fbe1ce1a53":{"id_":"80714139-0dd0-4a14-b653-10fbe1ce1a53","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"698661b8-c7b2-4a18-9e57-3483b9132819","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4HGsIMjEXHExYkWSL7ze7Gwtnx0ujSArimozxM/V/Ws="},"NEXT":{"nodeId":"418c0eba-bc10-42a8-b715-bf4324613685","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mP3Auq2zOzHiSD7hsjk87/E2QSDzwyuUhZBbWh4SjI8="}},"text":"Zhu, C. Zhao, J. Wang, X. Zhao, Y. Wu, and\r\nH. Lu, “CoupleNet: Coupling global structure withlocal parts for object detection,” inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,pp. 4126–4134. [85]   C. Desai, D. Ramanan, and C. C. Fowlkes,“Discriminative models for multi-class object\r\nlayout,”Int. J. Comput. Vis., vol. 95, no. 1,pp. 1–12, Oct. 2011. [86]   S. Bell, C. L. Zitnick, K. Bala, and R. Girshick,“Inside-outside net: Detecting objects in context\r\nwith skip pooling and recurrent neural networks,”inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2874–2883.[87]   Z. Li, Y. Chen, G. Yu, and Y. Deng, “R-FCN++:\r\nTowards accurate region-based fully convolutionalnetworks for object detection,” inProc. AAAI,\r\n2018, pp. 7073–7080.[88]   S. Liu et al., “Receptive field block net for accurate\r\nand fast object detection,” inProc. Eur. Conf.Comput. Vis. (ECCV), Sep. 2018, pp. 385–400. [89]   X. Wang, R. Girshick, A. Gupta, and K. He,“Non-local neural networks,” inProc. IEEE/CVF\r\nConf. Comput. Vis. Pattern Recognit., Jun. 2018,pp. 7794–7803. [90]   Q. Chen, Z. Song, J. Dong, Z. Huang, Y. Hua, andS. Yan, “Contextualizing object detection and\r\nclassification,”IEEE Trans. Pattern Anal.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5QgTbOxk4z8SAqcXecwaxE15Wnk492PFMo6LSrPb8no="},"418c0eba-bc10-42a8-b715-bf4324613685":{"id_":"418c0eba-bc10-42a8-b715-bf4324613685","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"80714139-0dd0-4a14-b653-10fbe1ce1a53","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5QgTbOxk4z8SAqcXecwaxE15Wnk492PFMo6LSrPb8no="},"NEXT":{"nodeId":"cc6a49cb-228a-42ca-9e20-c953a3f066cc","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"xU8wdXvy46BA00wa+k2n1lp7KuLdgiujYG78+hMx8dU="}},"text":"Yan, “Contextualizing object detection and\r\nclassification,”IEEE Trans. Pattern Anal. Mach.Intell., vol. 37, no. 1, pp. 13–27, Jan. 2015. [91]   S. Gupta, B. Hariharan, and J. Malik, “Exploringperson context and local scene context for object\r\ndetection,” 2015,arXiv:1511.08177.[92]   X. Chen and A. Gupta, “Spatial memory for\r\ncontext reasoning in object detection,” inProc.IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,\r\npp. 4086–4096.[93]   H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei,\r\n“Relation networks for object detection,” inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\r\nJun. 2018, pp. 3588–3597.[94]   Y. Liu, R. Wang, S. Shan, and X. Chen, “Structure\r\ninference net: Object detection using scene-levelcontext and instance-level relationships,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,Jun. 2018, pp. 6985–6994. [95]   L. V. Pato, R. Negrinho, and P. M. Q. Aguiar,“Seeing without looking: Contextual rescoring of\r\nobject detections for AP maximization,” inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020, pp. 14610–14618.[96]   S. K. Divvala, D. Hoiem, J. H. Hays, A. A. Efros,\r\nand M. Hebert, “An empirical study of context inobject detection,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2009, pp. 1271–1278.[97]   C.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"mP3Auq2zOzHiSD7hsjk87/E2QSDzwyuUhZBbWh4SjI8="},"cc6a49cb-228a-42ca-9e20-c953a3f066cc":{"id_":"cc6a49cb-228a-42ca-9e20-c953a3f066cc","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"418c0eba-bc10-42a8-b715-bf4324613685","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mP3Auq2zOzHiSD7hsjk87/E2QSDzwyuUhZBbWh4SjI8="},"NEXT":{"nodeId":"17c1a119-3b0c-41f5-bade-e4dd9bfe1c7f","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"OcUihBVBko4s8X9K+b91Ka5zuX3tIkS5vqY1vf4y+8M="}},"text":"2009, pp. 1271–1278.[97]   C. Chen, M.-Y. Liu, O. Tuzel, and J. Xiao, “R-CNN\r\nfor small object detection,” inProc. Asian Conf.Comput. Vis.Cham, Switzerland: Springer, 2016,\r\npp. 214–230.[98]   J. Li et al., “Attentive contexts for object\r\ndetection,”IEEE Trans. Multimedia, vol. 19, no. 5,pp. 944–954, May 2016. [99]   H. A. Rowley, S. Baluja, and T. Kanade, “Humanface detection in visual scenes,” inProc. Adv. Neural Inf. Process. Syst., 1996, pp. 875–881.[100]   C. P. Papageorgiou, M. Oren, and T. Poggio,\r\n“A general framework for object detection,” inProc. 6th Int. Conf. Comput. Vis., Jan. 1998,\r\npp. 555–562.[101]   L. Zhang, L. Lin, X. Liang, and K. He, “Is faster\r\nR-CNN doing well for pedestrian detection,” inProc. ECCV. Cham, Switzerland: Springer, 2016,\r\npp. 443–457.[102]   A. Shrivastava, A. Gupta, and R. Girshick,\r\n“Training region-based object detectors withonline hard example mining,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016,pp. 761–769. [103]   C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, andZ. Wojna, “Rethinking the inception architecture\r\nfor computer vision,” inProc. IEEE Conf. Comput.Vis. Pattern Recognit. (CVPR), Jun. 2016,\r\npp. 2818–2826.[104]   R. Müller, S.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"xU8wdXvy46BA00wa+k2n1lp7KuLdgiujYG78+hMx8dU="},"17c1a119-3b0c-41f5-bade-e4dd9bfe1c7f":{"id_":"17c1a119-3b0c-41f5-bade-e4dd9bfe1c7f","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"cc6a49cb-228a-42ca-9e20-c953a3f066cc","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"xU8wdXvy46BA00wa+k2n1lp7KuLdgiujYG78+hMx8dU="},"NEXT":{"nodeId":"9f46e2c1-9f85-4b53-83f7-a697724efe8d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"IPr0qarPVRb5SD7wYZwvy0XgJI5+bQSvebjzG4/6W5s="}},"text":"2818–2826.[104]   R. Müller, S. Kornblith, and G. E. Hinton, “When\r\ndoes label smoothing help,” inProc. Adv. NeuralInf. Process. Syst., vol. 32, 2019, pp. 1–13. [105]   J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang,“UnitBox: An advanced object detection network,”\r\ninProc. 24th ACM Int. Conf. Multimedia,Oct. 2016, pp. 516–520. [106]   H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian,I. Reid, and S. Savarese, “Generalized intersection\r\nover union: A metric and a loss for bounding boxregression,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 658–666.[107]   Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren,\r\n“Distance-IoU loss: Faster and better learning forbounding box regression,” inProc. AAAI Conf. Artif. Intell., vol. 34, no. 7, Apr. 2020,pp. 12993–13000. [108]   R. Vaillant, C. Monrocq, and Y. L. Cun, “Originalapproach for the localisation of objects in images,”\r\nIEE Proc.-Vis., Image Signal Process., vol. 141,no. 4, pp. 245–250, Aug. 1994. [109]   P. Henderson and V. Ferrari, “End-to-end trainingof object class detectors for mean average\r\nprecision,” inProc. Asian Conf. Comput. Vis.Cham,Switzerland: Springer, 2016, pp. 198–213. [110]   J. Hosang, R.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"OcUihBVBko4s8X9K+b91Ka5zuX3tIkS5vqY1vf4y+8M="},"9f46e2c1-9f85-4b53-83f7-a697724efe8d":{"id_":"9f46e2c1-9f85-4b53-83f7-a697724efe8d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"17c1a119-3b0c-41f5-bade-e4dd9bfe1c7f","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"OcUihBVBko4s8X9K+b91Ka5zuX3tIkS5vqY1vf4y+8M="},"NEXT":{"nodeId":"9ba5ff48-9814-4316-ae19-2db09ccfdd7b","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"D04c8cW2xrQeJk5FpPsnxSU7pmmHXBMuSTrwor+6EMs="}},"text":"198–213. [110]   J. Hosang, R. Benenson, and B. Schiele, “Learningnon-maximum suppression,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,pp. 6469–6477. [111]   Z. Tan, X. Nie, Q. Qian, N. Li, and H. Li, “Learningto rank proposals for object detection,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),Oct. 2019, pp. 8273–8281. [112]   N. Bodla, B. Singh, R. Chellappa, and L. S. Davis,“Soft-NMS—Improving object detection with one\r\nline of code,” inProc. IEEE Int. Conf. Comput. Vis.(ICCV), Oct. 2017, pp. 5562–5570. [113]   L. Tychsen-Smith and L. Petersson, “Improvingobject localization with fitness NMS and bounded\r\nIoU loss,” 2017,arXiv:1711.00164.[114]   Y. He, C. Zhu, J. Wang, M. Savvides, and\r\nX. Zhang, “Bounding box regression withuncertainty for accurate object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2019, pp. 2888–2897. [115]   S. Liu, D. Huang, and Y. Wang, “Adaptive NMS:Refining pedestrian detection in a crowd,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2019, pp. 6459–6468. [116]   R. Rothe, M. Guillaumin, and L. Van Gool,“Non-maximum suppression for object detection\r\nby passing messages between windows,” inProc.Asian Conf. Comput.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"IPr0qarPVRb5SD7wYZwvy0XgJI5+bQSvebjzG4/6W5s="},"9ba5ff48-9814-4316-ae19-2db09ccfdd7b":{"id_":"9ba5ff48-9814-4316-ae19-2db09ccfdd7b","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"9f46e2c1-9f85-4b53-83f7-a697724efe8d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"IPr0qarPVRb5SD7wYZwvy0XgJI5+bQSvebjzG4/6W5s="},"NEXT":{"nodeId":"14508b4d-aa56-4993-addd-de1b0fb12f0d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"oiEHV9JXFExV6D3IhFLh84IaaDn1q0OBopK/nUtglow="}},"text":"Comput. Vis.Cham, Switzerland:\r\nSpringer, 2014, pp. 290–306.[117]   D. Mrowca, M. Rohrbach, J. Hoffman, R. Hu,\r\nK. Saenko, and T. Darrell, “Spatial semanticregularisation for large scale object detection,” in\r\nProc. IEEE Int. Conf. Comput. Vis. (ICCV),Dec. 2015, pp. 2003–2011. [118]   R. Solovyev, W. Wang, and T. Gabruseva,“Weighted boxes fusion: Ensembling boxes from\r\ndifferent object detection models,”Image Vis.Comput., vol. 107, Mar. 2021, Art. no. 104117. [119]   Z. Zheng et al., “Enhancing geometric factors inmodel learning and inference for object detection\r\nand instance segmentation,”IEEE Trans. Cybern.,vol. 52, no. 8, pp. 8574–8586, Aug. 2022. [120]   J. Wang, L. Song, Z. Li, H. Sun, J. Sun, andN. Zheng, “End-to-end object detection with fully\r\nconvolutional network,” inProc. IEEE/CVF Conf.Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\r\npp. 15849–15858.[121]   C. Papageorgiou and T. Poggio, “A trainable\r\nsystem for object detection,”Int. J. Comput. Vis.,vol. 38, no. 1, pp. 15–33, 2000. [122]   L. Wan, D. Eigen, and R. Fergus, “End-to-endintegration of a convolutional network,\r\ndeformable parts model and non-maximumsuppression,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015, pp. 851–859.[123]   Z. Zou, K. Chen, Z. Shi, Y. Guo, and J.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"D04c8cW2xrQeJk5FpPsnxSU7pmmHXBMuSTrwor+6EMs="},"14508b4d-aa56-4993-addd-de1b0fb12f0d":{"id_":"14508b4d-aa56-4993-addd-de1b0fb12f0d","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_17","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"aSOlV7pKprcZ5NikXPwLbVA08jCBwdq0+qBI2w2rxQo="},"PREVIOUS":{"nodeId":"9ba5ff48-9814-4316-ae19-2db09ccfdd7b","metadata":{"page_number":17,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"D04c8cW2xrQeJk5FpPsnxSU7pmmHXBMuSTrwor+6EMs="}},"text":"Chen, Z. Shi, Y. Guo, and J. Ye, “Object\r\ndetection in 20 years: A survey,” 2019,arXiv:1905.05055. [124]   Q. Zhu, M.-C. Yeh, K.-T. Cheng, and S. Avidan,“Fast human detection using a cascade of\r\nhistograms of oriented gradients,” inProc. IEEEComput. Soc. Conf. Comput. Vis. Pattern Recognit.,\r\nVol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE273Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"oiEHV9JXFExV6D3IhFLh84IaaDn1q0OBopK/nUtglow="},"02b070e1-3f2f-4b2c-9ec5-65a48c8407e2":{"id_":"02b070e1-3f2f-4b2c-9ec5-65a48c8407e2","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"NEXT":{"nodeId":"ac0f1115-8168-4788-90cd-a4bd041453c5","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"arFYvuHZT1aFxx8UUHBx/Vu35/IIfapTb8opGLbH9Xw="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nvol. 2, Jun. 2006, pp. 1491–1498.[125]   F. Fleuret and D. Geman, “Coarse-to-fine face\r\ndetection,”Int. J. Comput. Vis., vol. 41, nos. 1–2,pp. 85–107, 2001. [126]   H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua,“A convolutional neural network cascade for face\r\ndetection,” inProc. IEEE Conf. Comput. Vis. PatternRecognit. (CVPR), Jun. 2015, pp. 5325–5334. [127]   K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint facedetection and alignment using multitask cascaded\r\nconvolutional networks,”IEEE Signal Process.Lett., vol. 23, no. 10, pp. 1499–1503, Oct. 2016. [128]   Z. Cai, M. Saberian, and N. Vasconcelos,“Learning complexity-aware cascades for deep\r\npedestrian detection,” inProc. IEEE Int. Conf.Comput. Vis. (ICCV), Dec. 2015, pp. 3361–3369. [129]   Y. LeCun, J. S. Denker, and S. A. Solla, “Optimalbrain damage,” inProc. Adv. Neural Inf. Process. Syst., 1990, pp. 598–605.[130]   S. Han, H. Mao, and W. J. Dally, “Deep\r\ncompression: Compressing deep neural networkswith pruning, trained quantization and Huffman\r\ncoding,” 2015,arXiv:1510.00149.[131]   K. He and J. Sun, “Convolutional neural networks\r\nat constrained time cost,” inProc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jun.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5Gx0tHyuQ3vciO7Zr7BgnOXTgIUeCALQsLUzhI9b1PI="},"ac0f1115-8168-4788-90cd-a4bd041453c5":{"id_":"ac0f1115-8168-4788-90cd-a4bd041453c5","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"02b070e1-3f2f-4b2c-9ec5-65a48c8407e2","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5Gx0tHyuQ3vciO7Zr7BgnOXTgIUeCALQsLUzhI9b1PI="},"NEXT":{"nodeId":"d809a9f0-ccfa-4e8c-a1bf-bf826f5b320b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"MXhqacxkl3X13GDZz8f6P6skqXbx/mchMTl+EUKwqjI="}},"text":"IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jun. 2015,\r\npp. 5353–5360.[132]   Z. Qin et al., “ThunderNet: Towards real-time\r\ngeneric object detection on mobile devices,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\r\nOct. 2019, pp. 6718–6727.[133]   R. J. Wang, X. Li, and C. X. Ling, “Pelee: A\r\nreal-time object detection system on mobiledevices,” inProc. Adv. Neural Inf. Process. Syst.,\r\nS. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett, Eds. Red Hook,\r\nNY, USA: Curran Associates, 2018,pp. 1967–1976. [134]   R. Huang, J. Pedoeem, and C. Chen, “YOLO-LITE:A real-time object detection algorithm optimized\r\nfor non-GPU computers,” inProc. IEEE Int. Conf.Big Data (Big Data), Dec. 2018, pp. 2503–2510. [135]   H. Law, Y. Teng, O. Russakovsky, and J. Deng,“CornerNet-lite: Efficient keypoint based object\r\ndetection,” 2019,arXiv:1904.08900.[136]   G. Yu et al., “PP-PicoDet: A better real-time object\r\ndetector on mobile devices,” 2021,arXiv:2111.00902. [137]   C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, andZ. Wojna, “Rethinking the inception architecture\r\nfor computer vision,” inProc. IEEE Conf. Comput.Vis. Pattern Recognit. (CVPR), Jun. 2016,\r\npp. 2818–2826.[138]   X. Zhang, J. Zou, X.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"arFYvuHZT1aFxx8UUHBx/Vu35/IIfapTb8opGLbH9Xw="},"d809a9f0-ccfa-4e8c-a1bf-bf826f5b320b":{"id_":"d809a9f0-ccfa-4e8c-a1bf-bf826f5b320b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"ac0f1115-8168-4788-90cd-a4bd041453c5","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"arFYvuHZT1aFxx8UUHBx/Vu35/IIfapTb8opGLbH9Xw="},"NEXT":{"nodeId":"6acf7010-d44c-40ef-8f3f-df93d47118f6","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"cK0oJ2UvGWgpE5V4BIIFQNwGqMyj/5+u2Evg7x4OgWk="}},"text":"Zhang, J. Zou, X. Ming, K. He, and J. Sun,\r\n“Efficient and accurate approximations ofnonlinear convolutional networks,” 2014,\r\narXiv:1411.4229.[139]   X. Zhang, J. Zou, K. He, and J. Sun, “Accelerating\r\nvery deep convolutional networks for classificationand detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 10, pp. 1943–1955, Oct. 2016.[140]   X. Zhang, X. Zhou, M. Lin, and J. Sun,\r\n“ShuffleNet: An extremely efficient convolutionalneural network for mobile devices,” 2017,\r\narXiv:1707.01083.[141]   G. Huang, S. Liu, L. van der Maaten, and\r\nK. Q. Weinberger, “CondenseNet: An efficientdensenet using learned group convolutions,”\r\nGroup, vol. 3, no. 12, p. 11, 2017.[142]   F. Chollet, “Xception: Deep learning with\r\ndepthwise separable convolutions,” 2016,arXiv:1610.02357. [143]   A. G. Howard et al., “MobileNets: Efficientconvolutional neural networks for mobile vision\r\napplications,” 2017,arXiv:1704.04861.[144]   M. Sandler, A. Howard, M. Zhu, A. Zhmoginov,\r\nand L.-C. Chen, “MobileNetV2: Inverted residualsand linear bottlenecks,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018,pp. 4510–4520. [145]   Y. Li, J. Li, W. Lin, and J. Li, “Tiny-DSOD:Lightweight object detection for\r\nresource-restricted usages,” 2018,arXiv:1807.11013. [146]   F. N. Iandola, S.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"MXhqacxkl3X13GDZz8f6P6skqXbx/mchMTl+EUKwqjI="},"6acf7010-d44c-40ef-8f3f-df93d47118f6":{"id_":"6acf7010-d44c-40ef-8f3f-df93d47118f6","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"d809a9f0-ccfa-4e8c-a1bf-bf826f5b320b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"MXhqacxkl3X13GDZz8f6P6skqXbx/mchMTl+EUKwqjI="},"NEXT":{"nodeId":"6807f3e5-d1ef-45c2-ae0e-434a492fb07c","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"A+XyPPEQe338cmcMDwmEmAbgZb4L1gSspUVg+RSsEHI="}},"text":"[146]   F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf,W. J. Dally, and K. Keutzer, “SqueezeNet:\r\nAlexNet-level accuracy with 50x fewer parametersand <0.5MB model size,” 2016,\r\narXiv:1602.07360.[147]   B. Wu, A. Wan, F. Iandola, P. H. Jin, and\r\nK. Keutzer, “SqueezeDet: Unified, small, lowpower fully convolutional neural networks for\r\nreal-time object detection for autonomousdriving,” inProc. IEEE Conf. Comput. Vis. Pattern\r\nRecognit. Workshops (CVPRW), Jul. 2017,pp. 446–454. [148]   T. Kong, A. Yao, Y. Chen, and F. Sun, “HyperNet:Towards accurate region proposal generation and\r\njoint object detection,” inProc. IEEE Conf. Comput.Vis. Pattern Recognit. (CVPR), Jun. 2016,\r\npp. 845–853.[149]   Y. Chen, T. Yang, X. Zhang, G. Meng, X. Xiao, and\r\nJ. Sun, “DetNAS: Backbone search for objectdetection,” 2019,arXiv:1903.10979. [150]   H. Xu, L. Yao, Z. Li, X. Liang, and W. Zhang,“Auto-FPN: Automatic network architecture\r\nadaptation for object detection beyondclassification,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019, pp. 6649–6658.[151]   G. Ghiasi, T.-Y. Lin, and Q. V. Le, “NAS-FPN:\r\nLearning scalable feature pyramid architecture forobject detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,pp. 7036–7045.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"cK0oJ2UvGWgpE5V4BIIFQNwGqMyj/5+u2Evg7x4OgWk="},"6807f3e5-d1ef-45c2-ae0e-434a492fb07c":{"id_":"6807f3e5-d1ef-45c2-ae0e-434a492fb07c","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"6acf7010-d44c-40ef-8f3f-df93d47118f6","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"cK0oJ2UvGWgpE5V4BIIFQNwGqMyj/5+u2Evg7x4OgWk="},"NEXT":{"nodeId":"da1a876b-cce9-4af2-a942-928ec93aac3b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EIfMoDjnJa3Yig/xS5e4NAy2laA4pB4SC70wZQcHEKg="}},"text":"(CVPR), Jun. 2019,pp. 7036–7045. [152]   J. Guo et al., “Hit-detector: Hierarchical trinityarchitecture search for object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2020, pp. 11405–11414. [153]   N. Wang et al., “NAS-FCOS: Fast neuralarchitecture search for object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2020, pp. 11943–11951. [154]   L. W. Yao, H. Xu, W. Zhang, X. D. Liang, andZ. G. Li, “SM-NAS: Structural-to-modular neural\r\narchitecture search for object detection,” inProc.AAAI Conf. Artif. Intell., vol. 34, no. 7, 2020,\r\npp. 12661–12668.[155]   C. Jiang, H. Xu, W. Zhang, X. Liang, and Z. Li,\r\n“SP-NAS: Serial-to-parallel backbone search forobject detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,pp. 11863–11872. [156]   P. Simard, L. Bottou, P. Haffner, and Y. LeCun,“Boxlets: A fast convolution algorithm for signal\r\nprocessing and neural networks,” inProc. Adv.Neural Inf. Process. Syst., 1999, pp. 571–577. [157]   X. Wang, T. X. Han, and S. Yan, “An HOG-LBPhuman detector with partial occlusion handling,”\r\ninProc. IEEE 12th Int. Conf. Comput. Vis.,Sep. 2009, pp. 32–39. [158]   F.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"A+XyPPEQe338cmcMDwmEmAbgZb4L1gSspUVg+RSsEHI="},"da1a876b-cce9-4af2-a942-928ec93aac3b":{"id_":"da1a876b-cce9-4af2-a942-928ec93aac3b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"6807f3e5-d1ef-45c2-ae0e-434a492fb07c","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"A+XyPPEQe338cmcMDwmEmAbgZb4L1gSspUVg+RSsEHI="},"NEXT":{"nodeId":"7f34fd6d-3e7c-435b-a564-231066c1c2ff","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4SLFYoPaW/2xI87BdGpmcwBmLpkHFJWDKHIxD5ytvCc="}},"text":"2009, pp. 32–39. [158]   F. Porikli, “Integral histogram: A fast way toextract histograms in Cartesian spaces,” inProc. IEEE Comput. Soc. Conf. Comput. Vis. PatternRecognit. (CVPR), vol. 1, Jun. 2005, pp. 829–836. [159]   P. Dollar, Z. Tu, P. Perona, and S. Belongie,“Integral channel features,” inProc. Brit. Mach. Vis. Conf., 2009, pp. 1–11.[160]   M. Mathieu, M. Henaff, and Y. LeCun, “Fast\r\ntraining of convolutional networks through FFTs,”2013,arXiv:1312.5851. [161]   H. Pratt, B. Williams, F. Coenen, and Y. Zheng,“FCNN: Fourier convolutional neural networks,”\r\ninProc. Joint Eur. Conf. Mach. Learn. Knowl.Discovery Databases. Cham, Switzerland: Springer,\r\n2017, pp. 786–798.[162]   N. Vasilache, J. Johnson, M. Mathieu, S. Chintala,\r\nS. Piantino, and Y. LeCun, “Fast convolutional netswith FBFFT: A GPU performance evaluation,”\r\n2014,arXiv:1412.7580.[163]   O. Rippel, J. Snoek, and R. P. Adams, “Spectral\r\nrepresentations for convolutional neuralnetworks,” inProc. Adv. Neural Inf. Process. Syst.,\r\n2015, pp. 2449–2457.[164]   M. A. Sadeghi and D. Forsyth, “Fast template\r\nevaluation with vector quantization,” inProc. Adv.Neural Inf. Process. Syst., 2013, pp. 2949–2957. [165]   I. Kokkinos, “Bounding part scores for rapiddetection with deformable part models,” inProc.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"EIfMoDjnJa3Yig/xS5e4NAy2laA4pB4SC70wZQcHEKg="},"7f34fd6d-3e7c-435b-a564-231066c1c2ff":{"id_":"7f34fd6d-3e7c-435b-a564-231066c1c2ff","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"da1a876b-cce9-4af2-a942-928ec93aac3b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"EIfMoDjnJa3Yig/xS5e4NAy2laA4pB4SC70wZQcHEKg="},"NEXT":{"nodeId":"693b4ba8-cac9-4646-8a1a-0c4b6877688b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"snCgQN9Ywx4FrwrA8ylbhbQr7+3TVaZoT1q2XoAK2xg="}},"text":"ECCV. Cham, Switzerland: Springer, 2012,pp. 41–50. [166]   H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao,“Orientation robust object detection in aerial\r\nimages using deep convolutional neural network,”inProc. IEEE Int. Conf. Image Process. (ICIP),\r\nSep. 2015, pp. 3735–3739.[167]   B. Cai, Z. Jiang, H. Zhang, Y. Yao, and S. Nie,\r\n“Online exemplar-based fully convolutionalnetwork for aircraft detection in remote sensing\r\nimages,”IEEE Geosci. Remote Sens. Lett., vol. 15,no. 7, pp. 1095–1099, Jul. 2018. [168]   G. Cheng, J. Han, P. Zhou, and L. Guo, “Multi-classgeospatial object detection and geographic image\r\nclassification based on collection of partdetectors,”ISPRS J. Photogramm. Remote Sens.,\r\nvol. 98, pp. 119–132, Dec. 2014.[169]   G. Cheng, P. Zhou, and J. Han, “RIFD-CNN:\r\nRotation-invariant and Fisher discriminativeconvolutional neural networks for object\r\ndetection,” inProc. IEEE Conf. Comput. Vis. PatternRecognit. (CVPR), Jun. 2016, pp. 2884–2893. [170]   G. Cheng, P. Zhou, and J. Han, “Learningrotation-invariant convolutional neural networks\r\nfor object detection in VHR optical remote sensingimages,”IEEE Trans. Geosci. Remote Sens., vol. 54,\r\nno. 12, pp. 7405–7415, Dec. 2016.[171]   G. Cheng, J. Han, P. Zhou, and D. Xu, “Learning\r\nrotation-invariant and Fisher discriminativeconvolutional neural networks for object\r\ndetection,”IEEE Trans. Image Process., vol. 28,no. 1, pp.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4SLFYoPaW/2xI87BdGpmcwBmLpkHFJWDKHIxD5ytvCc="},"693b4ba8-cac9-4646-8a1a-0c4b6877688b":{"id_":"693b4ba8-cac9-4646-8a1a-0c4b6877688b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"7f34fd6d-3e7c-435b-a564-231066c1c2ff","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4SLFYoPaW/2xI87BdGpmcwBmLpkHFJWDKHIxD5ytvCc="},"NEXT":{"nodeId":"17bcb273-17ef-49db-b7ac-43fc45f04b53","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"I1tGnJSVxrz5amSe1yizibHr6zgy+2LRSmunOixERu0="}},"text":"Image Process., vol. 28,no. 1, pp. 265–278, Jan. 2019. [172]   X. Shi, S. Shan, M. Kan, S. Wu, and X. Chen,“Real-time rotation-invariant face detection with\r\nprogressive calibration networks,” inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\r\nJun. 2018, pp. 2295–2303.[173]   M. Jaderberg et al., “Spatial transformer\r\nnetworks,” inProc. Adv. Neural Inf. Process. Syst.,2015, pp. 2017–2025. [174]   D. Chen, G. Hua, F. Wen, and J. Sun, “Supervisedtransformer network for efficient face detection,”\r\ninProc. ECCV. Cham, Switzerland: Springer, 2016,pp. 122–138. [175]   J. Ding, N. Xue, Y. Long, G.-S. Xia, and Q. Lu,“Learning RoI transformer for oriented object\r\ndetection in aerial images,” inProc. IEEE/CVFConf. Comput. Vis. Pattern Recognit. (CVPR),\r\nJun. 2019, pp. 2849–2858.[176]   B. Singh and L. S. Davis, “An analysis of scale\r\ninvariance in object detection–SNIP,” inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\r\nJun. 2018, pp. 3578–3587.[177]   B. Singh, M. Najibi, and L. S. Davis, “SNIPER:\r\nEfficient multi-scale training,” 2018,arXiv:1805.09300. [178]   K. He, X. Zhang, S. Ren, and J. Sun, “Deepresidual learning for image recognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),Jun. 2016, pp. 770–778.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"snCgQN9Ywx4FrwrA8ylbhbQr7+3TVaZoT1q2XoAK2xg="},"17bcb273-17ef-49db-b7ac-43fc45f04b53":{"id_":"17bcb273-17ef-49db-b7ac-43fc45f04b53","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_18","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"mINEmxEUFFh3HXA3rz0zrxPBnJdJkfD5Ef2y3nC89dg="},"PREVIOUS":{"nodeId":"693b4ba8-cac9-4646-8a1a-0c4b6877688b","metadata":{"page_number":18,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"snCgQN9Ywx4FrwrA8ylbhbQr7+3TVaZoT1q2XoAK2xg="}},"text":"(CVPR),Jun. 2016, pp. 770–778. [179]   M. Gao, R. Yu, A. Li, V. I. Morariu, and L. S. Davis,“Dynamic zoom-in network for fast object\r\ndetection in large images,” inProc. IEEE/CVF Conf.Comput. Vis. Pattern Recognit., Jun. 2018,\r\npp. 6926–6935.[180]   Y. Lu, T. Javidi, and S. Lazebnik, “Adaptive object\r\ndetection using adjacency and zoom prediction,”inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 2351–2359.[181]   S. Qiao, W. Shen, W. Qiu, C. Liu, and A. Yuille,\r\n“ScaleNet: Guiding object proposal generation insupermarkets and beyond,” inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 1809–1818.[182]   Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, and X. Hu,\r\n“Scale-aware face detection,” inProc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,\r\npp. 6186–6195.[183]   C.-Y. Wang, H.-Y. Mark Liao, Y.-H. Wu, P.-Y. Chen,\r\n274PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"I1tGnJSVxrz5amSe1yizibHr6zgy+2LRSmunOixERu0="},"aca1c779-b395-4887-8ed3-89615ceba116":{"id_":"aca1c779-b395-4887-8ed3-89615ceba116","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"NEXT":{"nodeId":"725dba10-47d7-4bc5-b943-baac8291786b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"j+qi2XC94Lkw2AgvoGLjzXh+bBYLtGTZ84Xz1Wxqge8="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nJ.-W. Hsieh, and I.-H. Yeh, “CSPNet: A newbackbone that can enhance learning capability of\r\nCNN,” inProc. IEEE/CVF Conf. Comput. Vis.Pattern Recognit. Workshops (CVPRW), Jun. 2020,\r\npp. 390–391.[184]   A. Newell, K. Yang, and J. Deng, “Stacked\r\nhourglass networks for human pose estimation,”inProc. Eur. Conf. Comput. Vis.Cham,\r\nSwitzerland: Springer, 2016, pp. 483–499.[185]   J. Gu et al., “Recent advances in convolutional\r\nneural networks,” 2015,arXiv:1512.07108.[186]   J. Huang et al., “Speed/accuracy trade-offs for\r\nmodern convolutional object detectors,” inProc.IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\r\nJul. 2017, pp. 7310–7311.[187]   Z. Cai and N. Vasconcelos, “Cascade R-CNN:\r\nDelving into high quality object detection,” 2017,arXiv:1712.00726. [188]   R. N. Rajaram, E. Ohn-Bar, and M. M. Trivedi,“RefineNet: Iterative refinement for accurate\r\nobject localization,” inProc. IEEE 19th Int. Conf.Intell. Transp. Syst. (ITSC), Nov. 2016,\r\npp. 1528–1533.[189]   M.-C. Roh and J.-Y. Lee, “Refining faster-RCNN for\r\naccurate object detection,” inProc. 15th IAPR Int.Conf. Mach. Vis. Appl. (MVA), May 2017,\r\npp. 514–517.[190]   B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang,\r\n“Acquisition of localization confidence for accurateobject detection,” inProc.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+B0IJ5OdQk/EvW4LkBV/nvGVYQYZ1iDzqSt1dKiPlSY="},"725dba10-47d7-4bc5-b943-baac8291786b":{"id_":"725dba10-47d7-4bc5-b943-baac8291786b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"aca1c779-b395-4887-8ed3-89615ceba116","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+B0IJ5OdQk/EvW4LkBV/nvGVYQYZ1iDzqSt1dKiPlSY="},"NEXT":{"nodeId":"17f86cd7-2110-4e3b-962c-175e432554aa","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"n6GTH42CIf6++K7Et3pEykhYyF+ZlmhoJeqerH5Eegw="}},"text":"Jiang,\r\n“Acquisition of localization confidence for accurateobject detection,” inProc. ECCV, Munich,\r\nGermany, 2018, pp. 8–14.[191]   S. Gidaris and N. Komodakis, “LocNet: Improving\r\nlocalization accuracy for object detection,” inProc.IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\r\nJun. 2016, pp. 789–798.[192]   S. Brahmbhatt, H. I. Christensen, and J. Hays,\r\n“StuffNet: Using ‘stuff’ to improve objectdetection,” inProc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2017, pp. 934–943.[193]   A. Shrivastava and A. Gupta, “Contextual priming\r\nand feedback for faster R-CNN,” inProc. ECCV.Cham, Switzerland: Springer, 2016, pp. 330–348. [194]   I. Goodfellow et al., “Generative adversarial nets,”inProc. Adv. Neural Inf. Process. Syst., 2014,\r\npp. 2672–2680.[195]   A. Radford, L. Metz, and S. Chintala,\r\n“Unsupervised representation learning with deepconvolutional generative adversarial networks,”\r\n2015,arXiv:1511.06434.[196]   J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros,\r\n“Unpaired image-to-image translation usingcycle-consistent adversarial networks,” 2017,\r\narXiv:1703.10593.[197]   C. Ledig et al., “Photo-realistic single image\r\nsuper-resolution using a generative adversarialnetwork,” inProc. IEEE Conf. Comput. Vis. Pattern\r\nRecognit. (CVPR), Jul. 2017, p. 4.[198]   J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"j+qi2XC94Lkw2AgvoGLjzXh+bBYLtGTZ84Xz1Wxqge8="},"17f86cd7-2110-4e3b-962c-175e432554aa":{"id_":"17f86cd7-2110-4e3b-962c-175e432554aa","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"725dba10-47d7-4bc5-b943-baac8291786b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"j+qi2XC94Lkw2AgvoGLjzXh+bBYLtGTZ84Xz1Wxqge8="},"NEXT":{"nodeId":"52ff03a5-8cc0-4001-9b0f-9fdef2fd0f05","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"iQtGZLgAfRTPWf+XUU/8hdzkT2BejqitAjvWLj4oLrQ="}},"text":"Wei, T. Xu, J. Feng, and S. Yan,\r\n“Perceptual generative adversarial networks forsmall object detection,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,pp. 1222–1230. [199]   Y. Bai, Y. Zhang, M. Ding, and B. Ghanem,“SOD-MTGAN: Small object detection via\r\nmulti-task generative adversarial network,” inProc. Comput. Vis. (ECCV), Sep. 2018, pp. 8–14. [200]   X. Wang, A. Shrivastava, and A. Gupta,“A-fast-RCNN: Hard positive generation via\r\nadversary for object detection,” inProc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,\r\npp. 2606–2615.[201]   D. Zhang, J. Han, G. Cheng, and M.-H. Yang,\r\n“Weakly supervised object localization anddetection: A survey,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 9, pp. 5866–5885,Sep. 2022. [202]   T. G. Dietterich, R. H. Lathrop, andT. Lozano-Pérez, “Solving the multiple instance\r\nproblem with axis-parallel rectangles,”Artif.Intell., vol. 89, nos. 1–2, pp. 31–71, 1997. [203]   S. Andrews, I. Tsochantaridis, and T. Hofmann,“Support vector machines for multiple-instance\r\nlearning,” inProc. Adv. Neural Inf. Process. Syst.,2003, pp. 577–584. [204]   R. G. Cinbis, J. Verbeek, and C. Schmid, “Weaklysupervised object localization with multi-fold\r\nmultiple instance learning,”IEEE Trans. PatternAnal.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"n6GTH42CIf6++K7Et3pEykhYyF+ZlmhoJeqerH5Eegw="},"52ff03a5-8cc0-4001-9b0f-9fdef2fd0f05":{"id_":"52ff03a5-8cc0-4001-9b0f-9fdef2fd0f05","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"17f86cd7-2110-4e3b-962c-175e432554aa","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"n6GTH42CIf6++K7Et3pEykhYyF+ZlmhoJeqerH5Eegw="},"NEXT":{"nodeId":"6a66e108-8703-489d-81ef-e5b7d8374068","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"L/X7i2e+q5aeSxXyKV8LfDZe+F6yCW5oxgQeexlSjSc="}},"text":"PatternAnal. Mach. Intell., vol. 39, no. 1, pp. 189–203,\r\nJan. 2017.[205]   D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and\r\nV. Ferrari, “We don’t need no bounding-boxes:Training object class detectors using only human\r\nverification,” inProc. IEEE Conf. Comput. Vis.Pattern Recognit. (CVPR), Jun. 2016, pp. 854–863. [206]   D. Zhang, W. Zeng, J. Yao, and J. Han, “Weaklysupervised object detection using proposal- and\r\nsemantic-level relationships,”IEEE Trans. PatternAnal. Mach. Intell., vol. 44, no. 6, pp. 3349–3363,\r\nJun. 2022.[207]   P. Tang et al., “PCL: Proposal cluster learning for\r\nweakly supervised object detection,”IEEE Trans.Pattern Anal. Mach. Intell., vol. 42, no. 1,\r\npp. 176–191, Jan. 2020.[208]   E. Sangineto, M. Nabi, D. Culibrk, and N. Sebe,\r\n“Self paced deep learning for weakly supervisedobject detection,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 3, pp. 712–725, Mar. 2016.[209]   D. Zhang, J. Han, L. Zhao, and D. Meng,\r\n“Leveraging prior-knowledge for weaklysupervised object detection under a collaborative\r\nself-paced curriculum learning framework,”Int. J.Comput. Vis., vol. 127, no. 4, pp. 363–380, 2018. [210]   Y. Zhu, Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao, “Softproposal networks for weakly supervised object\r\nlocalization,” inProc. IEEE Int. Conf. Comput. Vis.(ICCV), Oct.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"iQtGZLgAfRTPWf+XUU/8hdzkT2BejqitAjvWLj4oLrQ="},"6a66e108-8703-489d-81ef-e5b7d8374068":{"id_":"6a66e108-8703-489d-81ef-e5b7d8374068","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"52ff03a5-8cc0-4001-9b0f-9fdef2fd0f05","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"iQtGZLgAfRTPWf+XUU/8hdzkT2BejqitAjvWLj4oLrQ="},"NEXT":{"nodeId":"d838eee0-5869-4391-bffc-f80ca66fe319","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5elwyjClIFWXDMALa2YTl9VLW/7HBl65fDzMbPz5KRg="}},"text":"IEEE Int. Conf. Comput. Vis.(ICCV), Oct. 2017, pp. 1841–1850. [211]   A. Diba, V. Sharma, A. Pazandeh, H. Pirsiavash,and L. Van Gool, “Weakly supervised cascaded\r\nconvolutional networks,” inProc. IEEE Conf.Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,\r\npp. 914–922.[212]   B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and\r\nA. Torralba, “Learning deep features fordiscriminative localization,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016,pp. 2921–2929. [213]   H. Bilen and A. Vedaldi, “Weakly supervised deepdetection networks,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016,pp. 2846–2854. [214]   L. Bazzani, A. Bergamo, D. Anguelov, andL. Torresani, “Self-taught object localization with\r\ndeep networks,” inProc. IEEE Winter Conf. Appl.Comput. Vis. (WACV), Mar. 2016, pp. 1–9. [215]   Y. Shen, R. Ji, S. Zhang, W. Zuo, and Y. Wang,“Generative adversarial learning towards fast\r\nweakly supervised detection,” inProc. IEEE/CVFConf. Comput. Vis. Pattern Recognit., Jun. 2018,\r\npp. 5764–5773.[216]   Y. Chen, W. Li, C. Sakaridis, D. Dai, and\r\nL. Van Gool, “Domain adaptive faster R-CNN forobject detection in the wild,” inProc. IEEE/CVF\r\nConf. Comput. Vis. Pattern Recognit., Jun.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"L/X7i2e+q5aeSxXyKV8LfDZe+F6yCW5oxgQeexlSjSc="},"d838eee0-5869-4391-bffc-f80ca66fe319":{"id_":"d838eee0-5869-4391-bffc-f80ca66fe319","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"6a66e108-8703-489d-81ef-e5b7d8374068","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"L/X7i2e+q5aeSxXyKV8LfDZe+F6yCW5oxgQeexlSjSc="},"NEXT":{"nodeId":"39cf8919-1512-482a-952f-710ee6b9694b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FptLpae4PLhm6LZvo+0kvDXeO8vCaM+uhJTjM8Bgl6U="}},"text":"IEEE/CVF\r\nConf. Comput. Vis. Pattern Recognit., Jun. 2018,pp. 3339–3348. [217]   Y. Wang et al., “Domain-specific suppression foradaptive object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,pp. 9603–9612. [218]   L. Hou, Y. Zhang, K. Fu, and J. Li, “Informativeand consistent correspondence mining for\r\ncross-domain weakly supervised object detection,”inProc. IEEE/CVF Conf. Comput. Vis. Pattern\r\nRecognit. (CVPR), Jun. 2021, pp. 9929–9938.[219]   X. Zhu, J. Pang, C. Yang, J. Shi, and D. Lin,\r\n“Adapting object detectors via selectivecross-domain alignment,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,pp. 687–696. [220]   K. Saito, Y. Ushiku, T. Harada, and K. Saenko,“Strong-weak distribution alignment for adaptive\r\nobject detection,” inProc. IEEE/CVF Conf. Comput.Vis. Pattern Recognit. (CVPR), Jun. 2019,\r\npp. 6956–6965.[221]   C.-D. Xu, X.-R. Zhao, X. Jin, and X.-S. Wei,\r\n“Exploring categorical regularization for domainadaptive object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,pp. 11724–11733. [222]   J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros,“Unpaired image-to-image translation using\r\ncycle-consistent adversarial networks,” inProc.IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,\r\npp.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"5elwyjClIFWXDMALa2YTl9VLW/7HBl65fDzMbPz5KRg="},"39cf8919-1512-482a-952f-710ee6b9694b":{"id_":"39cf8919-1512-482a-952f-710ee6b9694b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"d838eee0-5869-4391-bffc-f80ca66fe319","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"5elwyjClIFWXDMALa2YTl9VLW/7HBl65fDzMbPz5KRg="},"NEXT":{"nodeId":"92d4764f-d69b-439e-a8e1-e93c206ac38f","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+1PCMvbg9BmhsnxQ7Qw0vmomPymaW5TIKTRUUHs5vhs="}},"text":"Conf. Comput. Vis. (ICCV), Oct. 2017,\r\npp. 2223–2232.[223]   T. Kim, M. Jeong, S. Kim, S. Choi, and C. Kim,\r\n“Diversify and match: A domain adaptiverepresentation learning paradigm for object\r\ndetection,” inProc. IEEE/CVF Conf. Comput. Vis.Pattern Recognit. (CVPR), Jun. 2019,\r\npp. 12456–12465.[224]   N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa,\r\n“Cross-domain weakly-supervised object detectionthrough progressive domain adaptation,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,Jun. 2018, pp. 5001–5009. [225]   H.-K. Hsu et al., “Progressive domain adaptationfor object detection,” inProc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), Mar. 2020,pp. 749–757. [226]   B. Bosquet, M. Mucientes, and V. M. Brea,“STDNet-ST: Spatio-temporal ConvNet for small\r\nobject detection,”Pattern Recognit., vol. 116,Aug. 2021, Art. no. 107929. [227]   C. Yang, Z. Huang, and N. Wang, “QueryDet:Cascaded sparse query for accelerating\r\nhigh-resolution small object detection,” inProc.IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 13668–13677.[228]   P. Sun et al., “What makes for end-to-end object\r\ndetection,” inProc. Int. Conf. Mach. Learn., 2021,pp. 9934–9944. [229]   X. Zhou et al., “Intelligent small object detectionfor digital twin in smart manufacturing with\r\nindustrial cyber-physical systems,”IEEE Trans. Ind.Informat., vol. 18, no. 2, pp.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"FptLpae4PLhm6LZvo+0kvDXeO8vCaM+uhJTjM8Bgl6U="},"92d4764f-d69b-439e-a8e1-e93c206ac38f":{"id_":"92d4764f-d69b-439e-a8e1-e93c206ac38f","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"39cf8919-1512-482a-952f-710ee6b9694b","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"FptLpae4PLhm6LZvo+0kvDXeO8vCaM+uhJTjM8Bgl6U="},"NEXT":{"nodeId":"549c26d1-8282-4499-a478-f0af5995b73a","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"NWg0vaYrxlMwkqDyPEh0anJg/ZlpeXH6BGncpyd1kkU="}},"text":"Ind.Informat., vol. 18, no. 2, pp. 1377–1386,\r\nFeb. 2022.[230]   G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, and\r\nJ. Han, “Towards large-scale small objectdetection: Survey and benchmarks,” 2022,\r\narXiv:2207.14096.[231]   Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang,\r\nH. Zhao, and J. Solomon, “DETR3D: 3D objectdetection from multi-view images via 3D-to-2D\r\nqueries,” inProc. Conf. Robot Learn., 2022,pp. 180–191. [232]   Y. Wang et al., “Bridged transformer for vision andpoint cloud 3D object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2022, pp. 12114–12123. [233]   X. Cheng et al., “Implicit motion handling forvideo camouflaged object detection,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2022, pp. 13864–13873. [234]   Q. Zhou et al., “TransVOD: End-to-end videoobject detection with spatial–temporal\r\ntransformers,” 2022,arXiv:2201.05047.[235]   R. Cong et al., “CIR-Net: Cross-modality\r\ninteraction and refinement for RGB-D salientobject detection,”IEEE Trans. Image Process.,\r\nvol. 31, pp. 6800–6815, 2022.[236]   Y. Wang et al., “Cross-modality domain adaptation\r\nfor freespace detection: A simple yet effectivebaseline,” inProc. 30th ACM Int. Conf. Multimedia,\r\nOct. 2022, pp. 4031–4042.[237]   C.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"+1PCMvbg9BmhsnxQ7Qw0vmomPymaW5TIKTRUUHs5vhs="},"549c26d1-8282-4499-a478-f0af5995b73a":{"id_":"549c26d1-8282-4499-a478-f0af5995b73a","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_19","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"JXNP6apytlBuQFizt5T4yGS43sy6P1C3Z4nA+qSK+kg="},"PREVIOUS":{"nodeId":"92d4764f-d69b-439e-a8e1-e93c206ac38f","metadata":{"page_number":19,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"+1PCMvbg9BmhsnxQ7Qw0vmomPymaW5TIKTRUUHs5vhs="}},"text":"2022, pp. 4031–4042.[237]   C. Feng et al., “PromptDet: Towards\r\nopen-vocabulary detection using uncuratedimages,” 2022,arXiv:2203.16513. [238]   Y. Zhong et al., “RegionCLIP: Region-basedlanguage-image pretraining,” inProc. IEEE/CVF\r\nConf. Comput. Vis. Pattern Recognit. (CVPR),Jun. 2022, pp. 16793–16803. Vol. 111, No. 3, March 2023| PROCEEDINGS OF THEIEEE275Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NWg0vaYrxlMwkqDyPEh0anJg/ZlpeXH6BGncpyd1kkU="},"77155bf5-3525-47af-9caa-8bc2f1326516":{"id_":"77155bf5-3525-47af-9caa-8bc2f1326516","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Dbr+7AyyXEXR3yuwXwey+Y+N+PbqXwYtG6Fbdx/X2eM="},"NEXT":{"nodeId":"d88fccbb-34a0-4ab8-84f8-dba69ecd758f","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"NPzULSLfV5qGiPHG0HsmVho83awXNDNmdQmMGPTTQT8="}},"text":"Zou et al.: Object Detection in 20 Years: A Survey\r\nA B O U T  T H E  A U T H O R S\r\nZhengxia Zoureceived the B.S. and Ph.D. degrees  from  Beihang  University,  Beijing,\r\nChina, in 2013 and 2018, respectively. From 2018 to 2021, he was a Postdoctoral\r\nResearch Fellow with the University of Michi-\r\ngan, Ann Arbor, MI, USA. He is currently an\r\nAssociate Professor with the School of Astro-\r\nnautics,  Beihang  University. He  has  pub-\r\nlished more than 20 peer-reviewed papers in\r\ntop-tier conferences and journals, including IEEE TRANSACTIONS ON\r\nPATTERNANALYSIS ANDMACHINEINTELLIGENCE(TPAMI),  the  Con-\r\nference on Computer Vision and Pattern Recognition (CVPR), the\r\nInternational  Conference  on  Computer  Vision  (ICCV),  and  the\r\nAssociation  for  the  Advancement  of  Artificial  Intelligence  (AAAI). His  research  has  been  covered  by  over  30  global  tech  media\r\nand  was  featured  in  a  number  of  apps  and  open-source  plat-\r\nforms  with  over  50M  users  worldwide. His  personal  website  is\r\nhttps://zhengxiazou.github.io/. His research interests include com-\r\nputer vision and related problems in remote sensing, autonomous\r\ndriving, and video games. Keyan Chenreceived the B.S. and master’s\r\ndegrees  from  Beihang  University,  Beijing,\r\nChina,   in   2019   and   2022,   respectively,\r\nwhere  he  is  currently  working  toward  the\r\nPh.D. degree at the Image Processing Cen-\r\nter, School of Astronautics. His  research  interests  include  computer\r\nvision,  pattern  recognition,  and  machine\r\nlearning. Zhenwei Shi(Member,   IEEE)   received\r\nthe Ph.D. degree in mathematics from the\r\nDalian   University   of   Technology,   Dalian,\r\nChina, in 2005.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"4zj55TeBJO0b9cD8r97vkNkWj1zwkOqC6Bcx+hzdOxk="},"d88fccbb-34a0-4ab8-84f8-dba69ecd758f":{"id_":"d88fccbb-34a0-4ab8-84f8-dba69ecd758f","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Dbr+7AyyXEXR3yuwXwey+Y+N+PbqXwYtG6Fbdx/X2eM="},"PREVIOUS":{"nodeId":"77155bf5-3525-47af-9caa-8bc2f1326516","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"4zj55TeBJO0b9cD8r97vkNkWj1zwkOqC6Bcx+hzdOxk="},"NEXT":{"nodeId":"6f6e2821-9614-413b-9914-8a0034786aec","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"GozXsmpRwbBo5Sla+lpd2p6+qpUwDmAXFJDhSk2uR9g="}},"text":"He  was  a  Postdoctoral  Researcher  with\r\nthe  Department  of  Automation,  Tsinghua\r\nUniversity,  Beijing,  China,  from  2005  to\r\n2007. He  was  a  Visiting  Scholar  with  the\r\nDepartment  of  Electrical  Engineering  and\r\nComputer  Sciences  (EECS),  Northwestern  University,  Evanston,\r\nIL,  USA,  from  2013  to  2014. He  is  currently  a  Professor  and  the\r\nDean  of  the  Image  Processing  Center,  School  of  Astronautics,\r\nBeihang University, Beijing. He has authored or coauthored over\r\n200  papers  in  refereed  journals  and  proceedings,  including  IEEE\r\nTRANSACTIONS ONPATTERNANALYSIS ANDMACHINEINTELLIGENCE\r\n(TPAMI), IEEE TRANSACTIONS ONNEURALNETWORKS ANDLEARNING\r\nSYSTEMS(TNNLS),    IEEE    TRANSACTIONS ONGEOSCIENCE AND\r\nREMOTESENSING(TGRS), and the Conference on Computer Vision\r\nand  Pattern  Recognition  (CVPR). His  current  research  interests\r\ninclude remote sensing image processing and analysis, computer\r\nvision, pattern recognition, and machine learning. Dr. Shi   is   an   Editorial   Board   Member   ofISPRS   Journal   of\r\nPhotogrammetry and Remote Sensing. He serves as the Associate\r\nEditor for IEEE TGRS,Infrared Physics and Technology, andPattern\r\nRecognition. His personal website is http://levir.buaa.edu.cn/. Yuhong Guoreceived  the  Ph.D. degree\r\nin   computing   science   from   the   Univer-\r\nsity  of  Alberta,  Edmonton,  AB,  Canada,  in\r\n2007. She has worked at The Australian National\r\nUniversity,  Canberra,  ACT,  Australia,  and\r\nTemple  University,  Philadelphia,  PA,  USA.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"NPzULSLfV5qGiPHG0HsmVho83awXNDNmdQmMGPTTQT8="},"6f6e2821-9614-413b-9914-8a0034786aec":{"id_":"6f6e2821-9614-413b-9914-8a0034786aec","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Dbr+7AyyXEXR3yuwXwey+Y+N+PbqXwYtG6Fbdx/X2eM="},"PREVIOUS":{"nodeId":"d88fccbb-34a0-4ab8-84f8-dba69ecd758f","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"NPzULSLfV5qGiPHG0HsmVho83awXNDNmdQmMGPTTQT8="},"NEXT":{"nodeId":"0a3f269c-6b39-4661-97e7-60d48ba21d1e","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"kF7bUuWi+4gZ1EniRJIE5nH5Vf5pTIAJ7V+lwMixJ70="}},"text":"She is currently a Professor with the School\r\nof  Computer  Science,  Carleton  University,\r\nOttawa,  ON,  Canada,  and  a  Canada  Research  Chair  in  machine\r\nlearning  and  a  Canada  CIFAR  AI  Chair  at  Amii,  Edmonton. Her\r\nresearch interests include machine learning, computer vision, and\r\nnatural language processing. She has published extensively in the\r\ntop venues in these areas. Dr. Guo  has  served  as  a  Senior  Program  Committee  Mem-\r\nber/Area  Chair  of  Association  for  the  Advancement  of  Artificial\r\nIntelligence (AAAI), International Joint Conference on Artificial Intel-\r\nligence (IJCAI), and Asian Conference on Machine Learning (ACML). She received paper awards from IJCAI, AAAI, and European Confer-\r\nence on Machine Learning (ECML). She is an Associate Editor of IEEE\r\nTRANSACTIONS ONPATTERNANALYSIS ANDMACHINEINTELLIGENCE. She is on the Editorial Board of theArtificial Intelligence Journal. She\r\nhas served as a Reviewer for many other international conferences,\r\nincluding  the  Conference  and  Workshop  on  Neural  Information\r\nProcessing  Systems  (NeurIPS),  the  International  Conference  on\r\nMachine Learning (ICML), the International Conference on Learning\r\nRepresentations (ICLR), the Conference on Uncertainty in Artificial\r\nIntelligence (UAI), the Annual Meeting of the Association for Compu-\r\ntational Linguistics (ACL), the Conference on Computer Vision and\r\nPattern Recognition (CVPR), and the International Conference on\r\nComputer Vision (ICCV). Jieping Ye(Fellow,  IEEE)  is  currently  the\r\nVP of the Alibaba Group, Hangzhou, China,\r\nwhere  he  is  the  Head  of  the  CityBrain\r\nLab, DAMO Academy. His research interests\r\ninclude big data, machine learning, and arti-\r\nficial intelligence with applications in trans-\r\nportation, smart city, and biomedicine. Dr. Ye was elevated to an IEEE Fellow in\r\n2019 and named an ACM Distinguished Sci-\r\nentist in 2020 for his contributions to the methodology and applica-\r\ntion of machine learning and data mining. He won the NSF CAREER\r\nAward in 2010.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"GozXsmpRwbBo5Sla+lpd2p6+qpUwDmAXFJDhSk2uR9g="},"0a3f269c-6b39-4661-97e7-60d48ba21d1e":{"id_":"0a3f269c-6b39-4661-97e7-60d48ba21d1e","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"excludedEmbedMetadataKeys":[],"excludedLlmMetadataKeys":[],"relationships":{"SOURCE":{"nodeId":"./RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf_20","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"Dbr+7AyyXEXR3yuwXwey+Y+N+PbqXwYtG6Fbdx/X2eM="},"PREVIOUS":{"nodeId":"6f6e2821-9614-413b-9914-8a0034786aec","metadata":{"page_number":20,"file_path":"/usr/src/app/RAG_storage/data/Object_Detection_in_20_Years_A_Survey.pdf","file_name":"Object_Detection_in_20_Years_A_Survey.pdf"},"hash":"GozXsmpRwbBo5Sla+lpd2p6+qpUwDmAXFJDhSk2uR9g="}},"text":"He won the NSF CAREER\r\nAward in 2010. His papers have been selected for the Outstanding\r\nStudent Paper at the International Conference on Machine Learning\r\n(ICML)  in  2004,  the  ACM  SIGKDD  Conference  on  Knowledge  Dis-\r\ncovery and Data Mining (KDD) Best Research Paper Runner Up in\r\n2013,  and  the  KDD  Best  Student  Paper  Award  in  2014. He  won\r\nthe First Place in the 2019 INFORMS Daniel H. Wagner Prize, one\r\nof the top awards in operation research practice. He has served as\r\na Senior Program Committee/Area Chair/Program Committee Vice-\r\nChair of many conferences, including the Conference and Workshop\r\non Neural Information Processing Systems (NeurIPS), ICML, KDD,\r\nInternational Joint Conference on Artificial Intelligence (IJCAI), the\r\nIEEE International Conference on Data Mining (ICDM), and the SIAM\r\nInternational Conference on Data Mining (SDM). He has served as\r\nan Associate Editor forData Mining and Knowledge Discovery, IEEE\r\nTRANSACTIONS ONKNOWLEDGE ANDDATAENGINEERING,  and  IEEE\r\nTRANSACTIONS ONPATTERNANALYSIS ANDMACHINEINTELLIGENCE. 276PROCEEDINGS OF THEIEEE  |Vol. 111, No. 3, March 2023Authorized licensed use limited to: National Tsing Hua Univ.. Downloaded on June 01,2024 at 06:14:27 UTC from IEEE Xplore. Restrictions apply.","textTemplate":"","metadataSeparator":"\n","type":"TEXT","hash":"kF7bUuWi+4gZ1EniRJIE5nH5Vf5pTIAJ7V+lwMixJ70="}},"type":"simple_dict"}}}